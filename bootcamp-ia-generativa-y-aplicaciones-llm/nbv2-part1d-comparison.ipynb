{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Side-by-Side Code Comparison - Part 1D\n",
    "\n",
    "This notebook provides a detailed side-by-side comparison between the original 2024 implementation and our modernized 2025 version. This is perfect for understanding exactly what changed and why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "project-structure",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Project Structure Comparison\n",
    "\n",
    "### Original 2024 Structure:\n",
    "```\n",
    "v1-162-part1/\n",
    "â”œâ”€â”€ app/\n",
    "â”‚   â”œâ”€â”€ server.py          # LangServe-based\n",
    "â”‚   â””â”€â”€ rag_chain.py       # Complex with TypedDict\n",
    "â”œâ”€â”€ rag-data-loader/\n",
    "â”‚   â””â”€â”€ rag_load_and_process.py  # Required flattening\n",
    "â”œâ”€â”€ pdf-documents/\n",
    "â”œâ”€â”€ pyproject.toml         # Python 3.11, LangServe dependency\n",
    "â””â”€â”€ .env\n",
    "```\n",
    "\n",
    "### Modern 2025 Structure:\n",
    "```\n",
    "v2-modern-step1/\n",
    "â”œâ”€â”€ app/\n",
    "â”‚   â”œâ”€â”€ server.py          # Direct FastAPI\n",
    "â”‚   â””â”€â”€ rag_chain.py       # Clean, simple\n",
    "â”œâ”€â”€ rag-data-loader/\n",
    "â”‚   â””â”€â”€ rag_load_and_process.py  # Direct processing\n",
    "â”œâ”€â”€ pdf-documents/\n",
    "â”œâ”€â”€ pyproject.toml         # Python 3.13, modern deps\n",
    "â”œâ”€â”€ .env.template          # Template for easy setup\n",
    "â””â”€â”€ .gitignore            # Better exclusions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependencies-comparison",
   "metadata": {},
   "source": [
    "## 1. Dependencies Comparison\n",
    "\n",
    "### ðŸ—“ï¸ Original 2024 `pyproject.toml`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_2024_deps = \"\"\"\n",
    "[tool.poetry]\n",
    "name = \"164-rag-for-pdfs-v2\"\n",
    "version = \"0.1.0\"\n",
    "\n",
    "[tool.poetry.dependencies]\n",
    "python = \">=3.11,<3.12\"           # Constrained by unstructured\n",
    "uvicorn = \"^0.23.2\"               # Older version\n",
    "langserve = {extras = [\"server\"], version = \">=0.0.30\"}  # DEPRECATED!\n",
    "pydantic = \"<2\"                   # LangServe constraint\n",
    "tqdm = \"^4.66.2\"\n",
    "unstructured = {extras = [\"all-docs\"], version = \"^0.12.6\"}\n",
    "langchain-experimental = \"^0.0.55\"\n",
    "python-dotenv = \"^1.0.1\"\n",
    "langchain-openai = \"^0.1.1\"       # Older version\n",
    "langchain-community = \"^0.0.31\"   # Older version\n",
    "tiktoken = \"^0.6.0\"\n",
    "psycopg = \"^3.1.18\"\n",
    "pgvector = \"^0.2.5\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ“œ Original 2024 Dependencies:\")\n",
    "print(original_2024_deps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_2025_deps = \"\"\"\n",
    "[tool.poetry]\n",
    "name = \"v2-modern-rag-step1\"\n",
    "version = \"0.1.0\"\n",
    "description = \"Modern RAG application for PDFs - Step 1 (2025 update)\"\n",
    "\n",
    "[tool.poetry.dependencies]\n",
    "python = \">=3.13,<3.14\"           # Latest Python!\n",
    "fastapi = \"^0.115.0\"              # Direct FastAPI (no LangServe)\n",
    "uvicorn = \"^0.32.0\"               # Latest version\n",
    "python-dotenv = \"^1.0.1\"\n",
    "langchain-core = \"^0.3.0\"         # Modular approach\n",
    "langchain-openai = \"^0.2.0\"       # Latest version\n",
    "langchain-community = \"^0.3.0\"    # Latest version\n",
    "langchain-experimental = \"^0.3.0\" # Latest version\n",
    "tqdm = \"^4.66.0\"\n",
    "unstructured = {extras = [\"all-docs\"], version = \"^0.16.0\"}\n",
    "tiktoken = \"^0.8.0\"               # Latest version\n",
    "psycopg = \"^3.2.0\"               # Latest version\n",
    "pgvector = \"^0.3.0\"              # Latest version\n",
    "# NO pydantic constraints!\n",
    "# NO langserve dependency!\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ†• Modern 2025 Dependencies:\")\n",
    "print(modern_2025_deps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-processing",
   "metadata": {},
   "source": [
    "## 2. Data Processing Comparison\n",
    "\n",
    "### ðŸ—“ï¸ Original 2024 `rag_load_and_process.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_2024_processing = '''\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import DirectoryLoader, UnstructuredPDFLoader\n",
    "from langchain_community.vectorstores.pgvector import PGVector\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    os.path.abspath(\"../pdf-documents\"),\n",
    "    glob=\"**/*.pdf\",\n",
    "    use_multithreading=True,\n",
    "    show_progress=True,\n",
    "    max_concurrency=50,\n",
    "    loader_cls=UnstructuredPDFLoader,\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# OLD EXPENSIVE MODEL!\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "\n",
    "text_splitter = SemanticChunker(embeddings=embeddings)\n",
    "\n",
    "# WEIRD FLATTENING STEP REQUIRED!\n",
    "flattened_docs = [doc[0] for doc in docs if doc]\n",
    "chunks = text_splitter.split_documents(flattened_docs)\n",
    "\n",
    "PGVector.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"collection164\",\n",
    "    connection_string=\"postgresql+psycopg://postgres@localhost:5432/database164\",\n",
    "    pre_delete_collection=True,\n",
    ")\n",
    "'''\n",
    "\n",
    "print(\"ðŸ“œ Original 2024 Data Processing:\")\n",
    "print(original_2024_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_2025_processing = '''\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import DirectoryLoader, UnstructuredPDFLoader\n",
    "from langchain_community.vectorstores.pgvector import PGVector\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    os.path.abspath(\"../pdf-documents\"),\n",
    "    glob=\"**/*.pdf\",\n",
    "    use_multithreading=True,\n",
    "    show_progress=True,\n",
    "    max_concurrency=50,\n",
    "    loader_cls=UnstructuredPDFLoader,\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# MODERN CHEAP MODEL! (5x cheaper)\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "text_splitter = SemanticChunker(embeddings=embeddings)\n",
    "\n",
    "# NO FLATTENING NEEDED! Direct processing\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks from {len(docs)} documents\")\n",
    "\n",
    "PGVector.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"modern_rag_collection\",\n",
    "    connection_string=\"postgresql+psycopg://postgres@localhost:5432/modern_rag_db\",\n",
    "    pre_delete_collection=True,\n",
    ")\n",
    "\n",
    "print(\"Vector database created successfully!\")\n",
    "'''\n",
    "\n",
    "print(\"ðŸ†• Modern 2025 Data Processing:\")\n",
    "print(modern_2025_processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-chain",
   "metadata": {},
   "source": [
    "## 3. RAG Chain Comparison\n",
    "\n",
    "### ðŸ—“ï¸ Original 2024 `rag_chain.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_2024_chain = '''\n",
    "import os\n",
    "from operator import itemgetter\n",
    "from typing import TypedDict\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores.pgvector import PGVector\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "vector_store = PGVector(\n",
    "    collection_name=\"collection164\",\n",
    "    connection_string=\"postgresql+psycopg://postgres@localhost:5432/database164\",\n",
    "    embedding_function=OpenAIEmbeddings()  # Default ada-002\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "Answer given the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# EXPENSIVE MODEL!\n",
    "llm = ChatOpenAI(temperature=0, model='gpt-4-1106-preview', streaming=True)\n",
    "\n",
    "# COMPLEX LANGSERVE-SPECIFIC SETUP!\n",
    "class RagInput(TypedDict):\n",
    "    question: str\n",
    "\n",
    "final_chain = (\n",
    "    {\n",
    "    \"context\": (itemgetter(\"question\") | vector_store.as_retriever()),\n",
    "    \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | ANSWER_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ").with_types(input_type=RagInput)  # LangServe requirement!\n",
    "'''\n",
    "\n",
    "print(\"ðŸ“œ Original 2024 RAG Chain:\")\n",
    "print(original_2024_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_2025_chain = '''\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores.pgvector import PGVector\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# MODERN EMBEDDINGS!\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "vector_store = PGVector(\n",
    "    collection_name=\"modern_rag_collection\",\n",
    "    connection_string=\"postgresql+psycopg://postgres@localhost:5432/modern_rag_db\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "template = \"\"\"Answer the question based on the following context:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# MUCH CHEAPER MODEL!\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0, \n",
    "    model='gpt-4o-mini',  # 67x cheaper!\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# CLEAN, SIMPLE APPROACH!\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# No TypedDict needed!\n",
    "# No itemgetter complexity!\n",
    "# No LangServe requirements!\n",
    "'''\n",
    "\n",
    "print(\"ðŸ†• Modern 2025 RAG Chain:\")\n",
    "print(modern_2025_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "server-comparison",
   "metadata": {},
   "source": [
    "## 4. Server Implementation Comparison\n",
    "\n",
    "### ðŸ—“ï¸ Original 2024 `server.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_2024_server = '''\n",
    "from fastapi import FastAPI\n",
    "from fastapi.responses import RedirectResponse\n",
    "from langserve import add_routes  # DEPRECATED!\n",
    "from app.rag_chain import final_chain\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def redirect_root_to_docs():\n",
    "    return RedirectResponse(\"/docs\")\n",
    "\n",
    "# BLACK BOX - NO CONTROL!\n",
    "add_routes(app, final_chain, path=\"/rag\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "    \n",
    "# Problems with this approach:\n",
    "# âŒ No custom error handling\n",
    "# âŒ No request/response validation\n",
    "# âŒ No streaming control\n",
    "# âŒ Deprecated LangServe dependency\n",
    "# âŒ Limited customization\n",
    "'''\n",
    "\n",
    "print(\"ðŸ“œ Original 2024 Server:\")\n",
    "print(original_2024_server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_2025_server = '''\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.responses import RedirectResponse\n",
    "from pydantic import BaseModel\n",
    "from typing import AsyncGenerator\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "from app.rag_chain import rag_chain\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Modern RAG API\",\n",
    "    description=\"A modern RAG application for querying PDF documents (2025 update)\",\n",
    "    version=\"2.0.0\"\n",
    ")\n",
    "\n",
    "# PROPER REQUEST/RESPONSE MODELS\n",
    "class QueryRequest(BaseModel):\n",
    "    question: str\n",
    "\n",
    "class QueryResponse(BaseModel):\n",
    "    answer: str\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def redirect_root_to_docs():\n",
    "    return RedirectResponse(\"/docs\")\n",
    "\n",
    "# FULL CONTROL WITH PROPER ERROR HANDLING\n",
    "@app.post(\"/query\", response_model=QueryResponse)\n",
    "async def query_documents(request: QueryRequest):\n",
    "    try:\n",
    "        answer = await rag_chain.ainvoke(request.question)\n",
    "        return QueryResponse(answer=answer)\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error: {str(e)}\")\n",
    "\n",
    "# STREAMING SUPPORT!\n",
    "@app.post(\"/stream\")\n",
    "async def stream_query(request: QueryRequest):\n",
    "    async def generate_response():\n",
    "        try:\n",
    "            async for chunk in rag_chain.astream(request.question):\n",
    "                yield f\"data: {json.dumps({'chunk': chunk})}\\n\\n\"\n",
    "        except Exception as e:\n",
    "            yield f\"data: {json.dumps({'error': str(e)})}\\n\\n\"\n",
    "    \n",
    "    from fastapi.responses import StreamingResponse\n",
    "    return StreamingResponse(generate_response(), media_type=\"text/plain\")\n",
    "\n",
    "# HEALTH CHECK FOR MONITORING\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"healthy\", \"version\": \"2.0.0\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "    \n",
    "# Benefits of this approach:\n",
    "# âœ… Custom error handling\n",
    "# âœ… Proper request/response validation\n",
    "# âœ… Streaming support\n",
    "# âœ… No deprecated dependencies\n",
    "# âœ… Full customization control\n",
    "# âœ… Health check endpoint\n",
    "# âœ… Better API documentation\n",
    "'''\n",
    "\n",
    "print(\"ðŸ†• Modern 2025 Server:\")\n",
    "print(modern_2025_server)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cost-analysis",
   "metadata": {},
   "source": [
    "## 5. Cost Analysis Comparison\n",
    "\n",
    "Let's calculate the real-world cost differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cost-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost comparison calculator\n",
    "print(\"ðŸ’° COST ANALYSIS: 2024 vs 2025\\n\")\n",
    "\n",
    "# Assumptions for a typical RAG query\n",
    "docs_to_embed = 100  # Number of document chunks\n",
    "tokens_per_chunk = 500  # Average tokens per chunk\n",
    "queries_per_day = 100  # Daily queries\n",
    "context_tokens = 2000  # Tokens in context per query\n",
    "response_tokens = 200  # Average response tokens\n",
    "\n",
    "# 2024 Pricing\n",
    "ada_002_price = 0.0001  # per 1k tokens\n",
    "gpt4_preview_input = 0.01  # per 1k tokens\n",
    "gpt4_preview_output = 0.03  # per 1k tokens\n",
    "\n",
    "# 2025 Pricing  \n",
    "embedding_3_small_price = 0.00002  # per 1k tokens\n",
    "gpt4o_mini_input = 0.00015  # per 1k tokens\n",
    "gpt4o_mini_output = 0.0006  # per 1k tokens\n",
    "\n",
    "# Calculate embedding costs (one-time setup)\n",
    "total_embedding_tokens = docs_to_embed * tokens_per_chunk\n",
    "embedding_2024 = total_embedding_tokens * ada_002_price / 1000\n",
    "embedding_2025 = total_embedding_tokens * embedding_3_small_price / 1000\n",
    "\n",
    "print(f\"ðŸ“Š ONE-TIME EMBEDDING COSTS ({docs_to_embed} chunks, {tokens_per_chunk} tokens each):\")\n",
    "print(f\"  2024 (ada-002): ${embedding_2024:.4f}\")\n",
    "print(f\"  2025 (3-small): ${embedding_2025:.4f}\")\n",
    "print(f\"  ðŸ’¡ Embedding savings: {((embedding_2024-embedding_2025)/embedding_2024)*100:.1f}%\\n\")\n",
    "\n",
    "# Calculate daily LLM costs\n",
    "daily_input_tokens = queries_per_day * context_tokens\n",
    "daily_output_tokens = queries_per_day * response_tokens\n",
    "\n",
    "llm_2024_daily = (daily_input_tokens * gpt4_preview_input / 1000) + (daily_output_tokens * gpt4_preview_output / 1000)\n",
    "llm_2025_daily = (daily_input_tokens * gpt4o_mini_input / 1000) + (daily_output_tokens * gpt4o_mini_output / 1000)\n",
    "\n",
    "print(f\"ðŸ“ˆ DAILY LLM COSTS ({queries_per_day} queries/day):\")\n",
    "print(f\"  2024 (gpt-4-preview): ${llm_2024_daily:.4f}/day\")\n",
    "print(f\"  2025 (gpt-4o-mini): ${llm_2025_daily:.4f}/day\")\n",
    "print(f\"  ðŸ’¡ Daily LLM savings: {((llm_2024_daily-llm_2025_daily)/llm_2024_daily)*100:.1f}%\\n\")\n",
    "\n",
    "# Monthly and yearly projections\n",
    "monthly_2024 = llm_2024_daily * 30\n",
    "monthly_2025 = llm_2025_daily * 30\n",
    "yearly_2024 = llm_2024_daily * 365\n",
    "yearly_2025 = llm_2025_daily * 365\n",
    "\n",
    "print(f\"ðŸ“… MONTHLY COSTS:\")\n",
    "print(f\"  2024: ${monthly_2024:.2f}/month\")\n",
    "print(f\"  2025: ${monthly_2025:.2f}/month\")\n",
    "print(f\"  ðŸ’° Monthly savings: ${monthly_2024-monthly_2025:.2f}\")\n",
    "\n",
    "print(f\"\\nðŸ—“ï¸ YEARLY COSTS:\")\n",
    "print(f\"  2024: ${yearly_2024:.2f}/year\")\n",
    "print(f\"  2025: ${yearly_2025:.2f}/year\")\n",
    "print(f\"  ðŸ’° Yearly savings: ${yearly_2024-yearly_2025:.2f}\")\n",
    "\n",
    "total_savings_percent = ((yearly_2024 - yearly_2025) / yearly_2024) * 100\n",
    "print(f\"\\nðŸŽ¯ TOTAL COST REDUCTION: {total_savings_percent:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-comparison",
   "metadata": {},
   "source": [
    "## 6. Feature Comparison Matrix\n",
    "\n",
    "### Complete Feature Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create comprehensive comparison\n",
    "comparison_data = {\n",
    "    'Feature': [\n",
    "        'Python Version',\n",
    "        'Dependency Management',\n",
    "        'Embedding Model',\n",
    "        'LLM Model',\n",
    "        'Deployment Framework',\n",
    "        'Error Handling',\n",
    "        'Request Validation',\n",
    "        'Streaming Support',\n",
    "        'API Documentation',\n",
    "        'Health Checks',\n",
    "        'Code Complexity',\n",
    "        'Maintainability',\n",
    "        'Future-Proof',\n",
    "        'Cost Efficiency',\n",
    "        'Performance'\n",
    "    ],\n",
    "    '2024 Approach': [\n",
    "        'Python 3.11.4',\n",
    "        'Poetry with constraints',\n",
    "        'text-embedding-ada-002',\n",
    "        'gpt-4-1106-preview',\n",
    "        'LangServe (deprecated)',\n",
    "        'Limited/default',\n",
    "        'Basic TypedDict',\n",
    "        'Basic via LangServe',\n",
    "        'Auto-generated',\n",
    "        'None',\n",
    "        'High (TypedDict, itemgetter)',\n",
    "        'Low (complex abstractions)',\n",
    "        'Poor (deprecated deps)',\n",
    "        'Expensive',\n",
    "        'Good'\n",
    "    ],\n",
    "    '2025 Approach': [\n",
    "        'Python 3.13.3',\n",
    "        'Clean Poetry setup',\n",
    "        'text-embedding-3-small',\n",
    "        'gpt-4o-mini',\n",
    "        'Direct FastAPI',\n",
    "        'Custom with details',\n",
    "        'Full Pydantic models',\n",
    "        'Custom streaming',\n",
    "        'Rich, customizable',\n",
    "        'Built-in endpoints',\n",
    "        'Low (clean patterns)',\n",
    "        'High (readable code)',\n",
    "        'Excellent (modern stack)',\n",
    "        'Very cost-effective',\n",
    "        'Excellent'\n",
    "    ],\n",
    "    'Improvement': [\n",
    "        'âš¡ Latest features & performance',\n",
    "        'ðŸ§¹ Fewer dependency conflicts',\n",
    "        'ðŸ’° 5x cost reduction',\n",
    "        'ðŸ’° 67x cost reduction',\n",
    "        'ðŸŽ¯ Full control, no deprecation',\n",
    "        'ðŸ› Better debugging experience',\n",
    "        'âœ… Type safety & validation',\n",
    "        'ðŸŒŠ Real-time user experience',\n",
    "        'ðŸ“– Better developer experience',\n",
    "        'ðŸ“Š Production monitoring',\n",
    "        'ðŸ§¹ Easier to understand',\n",
    "        'ðŸ”§ Easier to modify',\n",
    "        'ðŸš€ Long-term viability',\n",
    "        'ðŸ’¸ Significant cost savings',\n",
    "        'âš¡ Faster responses'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"ðŸ“Š COMPREHENSIVE FEATURE COMPARISON:\\n\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "migration-guide",
   "metadata": {},
   "source": [
    "## 7. Migration Guide\n",
    "\n",
    "### For Students Working with the Old Version:\n",
    "\n",
    "If you have the 2024 version and want to migrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "migration-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "migration_steps = \"\"\"\n",
    "ðŸ”„ MIGRATION STEPS FROM 2024 TO 2025:\n",
    "\n",
    "1. ðŸ UPDATE PYTHON:\n",
    "   - Install Python 3.13.3\n",
    "   - Update Poetry to 2.1.4\n",
    "\n",
    "2. ðŸ“¦ UPDATE DEPENDENCIES:\n",
    "   - Remove: langserve, pydantic constraints\n",
    "   - Add: fastapi (direct)\n",
    "   - Update: all langchain-* packages to latest\n",
    "\n",
    "3. ðŸ”§ UPDATE CODE:\n",
    "   - Replace LangServe with FastAPI endpoints\n",
    "   - Change embedding model to text-embedding-3-small\n",
    "   - Change LLM to gpt-4o-mini\n",
    "   - Remove document flattening step\n",
    "   - Simplify chain structure\n",
    "\n",
    "4. ðŸ—„ï¸ UPDATE DATABASE:\n",
    "   - Create new database (or clean existing)\n",
    "   - Re-run data loading with new models\n",
    "\n",
    "5. âœ… TEST:\n",
    "   - Verify all functionality works\n",
    "   - Test API endpoints\n",
    "   - Monitor costs and performance\n",
    "\n",
    "â±ï¸ ESTIMATED TIME: 2-4 hours\n",
    "ðŸ’° COST REDUCTION: ~95% for typical usage\n",
    "\"\"\"\n",
    "\n",
    "print(migration_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learning-outcomes",
   "metadata": {},
   "source": [
    "## 8. Learning Outcomes\n",
    "\n",
    "### What You Should Take Away:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "key-learnings",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_outcomes = \"\"\"\n",
    "ðŸŽ“ KEY LEARNING OUTCOMES:\n",
    "\n",
    "1. ðŸš€ AI MOVES FAST:\n",
    "   - Technologies can become deprecated in months\n",
    "   - Always check for newer, better alternatives\n",
    "   - Stay updated with the AI community\n",
    "\n",
    "2. ðŸ’° COST OPTIMIZATION MATTERS:\n",
    "   - Model choices dramatically affect costs\n",
    "   - Newer models often provide better value\n",
    "   - Always calculate total cost of ownership\n",
    "\n",
    "3. ðŸ§¹ SIMPLICITY WINS:\n",
    "   - Direct approaches often beat complex abstractions\n",
    "   - Fewer dependencies = fewer problems\n",
    "   - Readable code is maintainable code\n",
    "\n",
    "4. ðŸŽ¯ CONTROL IS VALUABLE:\n",
    "   - Black box solutions limit customization\n",
    "   - Direct implementations offer more flexibility\n",
    "   - Custom error handling improves debugging\n",
    "\n",
    "5. ðŸ”® FUTURE-PROOFING:\n",
    "   - Choose actively maintained technologies\n",
    "   - Avoid deprecated dependencies\n",
    "   - Design for change\n",
    "\n",
    "6. ðŸ“Š MONITORING MATTERS:\n",
    "   - Production systems need health checks\n",
    "   - Track performance and costs\n",
    "   - Plan for scale\n",
    "\n",
    "ðŸŽ¯ BOTTOM LINE:\n",
    "The 2025 approach is not just \"newer\" - it's fundamentally\n",
    "better in terms of cost, performance, maintainability, and\n",
    "future-proofing. This is how modern AI applications should\n",
    "be built.\n",
    "\"\"\"\n",
    "\n",
    "print(learning_outcomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "ðŸŽ‰ **You've now seen a complete side-by-side comparison!**\n",
    "\n",
    "### The Modernization Journey:\n",
    "- ðŸ“š **Started**: Understanding the original 2024 approach\n",
    "- ðŸ” **Analyzed**: What changed and why in the AI ecosystem\n",
    "- ðŸ› ï¸ **Implemented**: A modern, cost-effective solution\n",
    "- ðŸ“Š **Compared**: Old vs new approaches in detail\n",
    "\n",
    "### Key Improvements Achieved:\n",
    "- ðŸ’° **~95% cost reduction** through model optimization\n",
    "- ðŸ§¹ **Simplified codebase** with modern patterns\n",
    "- ðŸŽ¯ **Better control** with direct FastAPI implementation\n",
    "- ðŸš€ **Future-proof** architecture using current best practices\n",
    "\n",
    "### Next Steps:\n",
    "1. **Experiment** with the modern implementation\n",
    "2. **Try different models** and compare results\n",
    "3. **Apply these principles** to your own AI projects\n",
    "4. **Stay updated** with the rapidly evolving AI landscape\n",
    "\n",
    "The AI field moves incredibly fast, but by understanding the principles behind these modernizations, you'll be better equipped to adapt to future changes. Happy building! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}