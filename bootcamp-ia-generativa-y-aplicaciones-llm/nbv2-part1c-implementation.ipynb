{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Modern RAG Implementation Guide - Part 1C\n",
    "\n",
    "This notebook walks you through implementing the modern RAG application step by step. We'll build everything from scratch using 2025 best practices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisites",
   "metadata": {},
   "source": [
    "## Prerequisites Check\n",
    "\n",
    "Make sure you've completed the setup from **nbv2-part1a-setup.ipynb** before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify we're in the right directory and have the right dependencies\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Check if we can import the main dependencies\n",
    "try:\n",
    "    import fastapi\n",
    "    import langchain_openai\n",
    "    import langchain_community\n",
    "    import pgvector\n",
    "    print(\"‚úÖ All dependencies available!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Missing dependency: {e}\")\n",
    "    print(\"Make sure you've run 'poetry install' in the v2-modern-step1 directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-1",
   "metadata": {},
   "source": [
    "## Step 1: Add PDF Documents\n",
    "\n",
    "First, let's add some PDF documents to work with. Create the same documents as in the original project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-pdfs",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if PDF documents directory exists and has files\n",
    "pdf_dir = \"v2-modern-step1/pdf-documents\"\n",
    "\n",
    "if os.path.exists(pdf_dir):\n",
    "    pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith('.pdf')]\n",
    "    print(f\"PDF documents directory exists with {len(pdf_files)} files:\")\n",
    "    for file in pdf_files:\n",
    "        print(f\"  - {file}\")\n",
    "    \n",
    "    if len(pdf_files) == 0:\n",
    "        print(\"\\nüìÅ Please add your PDF documents to the pdf-documents folder.\")\n",
    "        print(\"For this demo, you can use:\")\n",
    "        print(\"  - John F. Kennedy biography (Wikipedia PDF)\")\n",
    "        print(\"  - Robert F. Kennedy biography (Wikipedia PDF)\")\n",
    "        print(\"  - Joseph P. Kennedy biography (Wikipedia PDF)\")\n",
    "else:\n",
    "    print(\"‚ùå PDF documents directory not found.\")\n",
    "    print(\"Make sure you're running this from the project root directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-2",
   "metadata": {},
   "source": [
    "## Step 2: Environment Configuration\n",
    "\n",
    "Let's set up our environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('v2-modern-step1/.env')\n",
    "\n",
    "# Check if API keys are configured\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "langsmith_key = os.getenv('LANGCHAIN_API_KEY')\n",
    "\n",
    "if openai_key and not openai_key.startswith('your_'):\n",
    "    print(\"‚úÖ OpenAI API key is configured\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è OpenAI API key needs to be configured in v2-modern-step1/.env\")\n",
    "    print(\"Copy .env.template to .env and add your API key\")\n",
    "\n",
    "if langsmith_key and not langsmith_key.startswith('your_'):\n",
    "    print(\"‚úÖ LangSmith API key is configured\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è LangSmith API key not configured (optional for monitoring)\")\n",
    "\n",
    "# Show the project name that will be used in LangSmith\n",
    "project_name = os.getenv('LANGCHAIN_PROJECT', 'ModernRAGStep1-2025')\n",
    "print(f\"üìä LangSmith project: {project_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-3",
   "metadata": {},
   "source": [
    "## Step 3: Test Database Connection\n",
    "\n",
    "Let's verify that PostgreSQL with PGVector is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg\n",
    "from psycopg import sql\n",
    "\n",
    "# Database connection string (same as in our code)\n",
    "connection_string = \"postgresql+psycopg://postgres@localhost:5432/modern_rag_db\"\n",
    "\n",
    "try:\n",
    "    # Extract connection parameters\n",
    "    # For testing, we'll use a simpler connection string\n",
    "    test_conn_str = \"host=localhost port=5432 dbname=modern_rag_db user=postgres\"\n",
    "    \n",
    "    with psycopg.connect(test_conn_str) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            # Check if pgvector extension exists\n",
    "            cur.execute(\"SELECT * FROM pg_extension WHERE extname = 'vector';\")\n",
    "            result = cur.fetchone()\n",
    "            \n",
    "            if result:\n",
    "                print(\"‚úÖ Database connection successful!\")\n",
    "                print(\"‚úÖ PGVector extension is installed!\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Database connected, but PGVector extension not found.\")\n",
    "                print(\"Run: psql -d modern_rag_db -c 'CREATE EXTENSION vector;'\")\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Database connection failed: {e}\")\n",
    "    print(\"\\nTroubleshooting steps:\")\n",
    "    print(\"1. Make sure PostgreSQL is running: brew services start postgresql\")\n",
    "    print(\"2. Create the database: psql -U postgres -c 'CREATE DATABASE modern_rag_db;'\")\n",
    "    print(\"3. Install pgvector: psql -d modern_rag_db -c 'CREATE EXTENSION vector;'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-4",
   "metadata": {},
   "source": [
    "## Step 4: Document Loading and Processing\n",
    "\n",
    "Now let's implement the modern document loading process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-docs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document loading with modern approach\n",
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader, UnstructuredPDFLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "print(\"üìö Loading documents...\")\n",
    "\n",
    "# Set up the document loader\n",
    "pdf_directory = \"v2-modern-step1/pdf-documents\"\n",
    "loader = DirectoryLoader(\n",
    "    pdf_directory,\n",
    "    glob=\"**/*.pdf\",\n",
    "    use_multithreading=True,\n",
    "    show_progress=True,\n",
    "    max_concurrency=50,\n",
    "    loader_cls=UnstructuredPDFLoader,\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Load documents\n",
    "    docs = loader.load()\n",
    "    print(f\"‚úÖ Loaded {len(docs)} documents\")\n",
    "    \n",
    "    # Show a sample of what we loaded\n",
    "    if docs:\n",
    "        sample_doc = docs[0]\n",
    "        print(f\"\\nSample document content (first 200 chars):\")\n",
    "        print(f\"Content: {sample_doc.page_content[:200]}...\")\n",
    "        print(f\"Metadata: {sample_doc.metadata}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading documents: {e}\")\n",
    "    print(\"Make sure you have PDF files in the pdf-documents directory\")\n",
    "    docs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modern embedding model setup\n",
    "print(\"üß† Setting up embeddings...\")\n",
    "\n",
    "# Using the modern, cost-effective embedding model\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "print(\"‚úÖ Using text-embedding-3-small (5x cheaper than ada-002!)\")\n",
    "\n",
    "# Test the embeddings\n",
    "try:\n",
    "    test_embedding = embeddings.embed_query(\"This is a test sentence.\")\n",
    "    print(f\"‚úÖ Embedding test successful! Vector dimension: {len(test_embedding)}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Embedding test failed: {e}\")\n",
    "    print(\"Check your OpenAI API key in the .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chunk-docs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document chunking with SemanticChunker\n",
    "if docs:  # Only proceed if we have documents\n",
    "    print(\"‚úÇÔ∏è Chunking documents...\")\n",
    "    \n",
    "    # Create the semantic text splitter\n",
    "    text_splitter = SemanticChunker(embeddings=embeddings)\n",
    "    \n",
    "    try:\n",
    "        # Modern approach: No flattening needed!\n",
    "        chunks = text_splitter.split_documents(docs)\n",
    "        \n",
    "        print(f\"‚úÖ Created {len(chunks)} chunks from {len(docs)} documents\")\n",
    "        \n",
    "        # Show sample chunk\n",
    "        if chunks:\n",
    "            sample_chunk = chunks[0]\n",
    "            print(f\"\\nSample chunk (first 300 chars):\")\n",
    "            print(f\"Content: {sample_chunk.page_content[:300]}...\")\n",
    "            print(f\"Metadata: {sample_chunk.metadata}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error chunking documents: {e}\")\n",
    "        chunks = []\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping chunking - no documents loaded\")\n",
    "    chunks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-5",
   "metadata": {},
   "source": [
    "## Step 5: Vector Database Setup\n",
    "\n",
    "Now let's create our vector database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-vectordb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector database\n",
    "if chunks:  # Only proceed if we have chunks\n",
    "    print(\"üóÑÔ∏è Creating vector database...\")\n",
    "    \n",
    "    from langchain_community.vectorstores.pgvector import PGVector\n",
    "    \n",
    "    try:\n",
    "        # Create the vector store\n",
    "        vector_store = PGVector.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=embeddings,\n",
    "            collection_name=\"modern_rag_collection\",\n",
    "            connection_string=\"postgresql+psycopg://postgres@localhost:5432/modern_rag_db\",\n",
    "            pre_delete_collection=True,  # Clean start\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Vector database created successfully!\")\n",
    "        print(f\"üìä Stored {len(chunks)} document chunks\")\n",
    "        \n",
    "        # Test similarity search\n",
    "        test_query = \"Who was John F. Kennedy?\"\n",
    "        similar_docs = vector_store.similarity_search(test_query, k=2)\n",
    "        \n",
    "        print(f\"\\nüîç Test search for '{test_query}':\")\n",
    "        for i, doc in enumerate(similar_docs):\n",
    "            print(f\"Result {i+1}: {doc.page_content[:100]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating vector database: {e}\")\n",
    "        print(\"Check your database connection and pgvector installation\")\n",
    "        vector_store = None\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping vector database creation - no chunks available\")\n",
    "    vector_store = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-6",
   "metadata": {},
   "source": [
    "## Step 6: RAG Chain Implementation\n",
    "\n",
    "Let's create our modern RAG chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-rag-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modern RAG chain setup\n",
    "if vector_store:\n",
    "    print(\"üîó Setting up RAG chain...\")\n",
    "    \n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "    from langchain_core.runnables import RunnablePassthrough\n",
    "    \n",
    "    # Create retriever\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 4})\n",
    "    \n",
    "    # Define prompt template\n",
    "    template = \"\"\"Answer the question based on the following context:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    \n",
    "    # Modern LLM setup\n",
    "    llm = ChatOpenAI(\n",
    "        temperature=0, \n",
    "        model='gpt-4o-mini',  # Cost-effective and great for RAG\n",
    "        streaming=True\n",
    "    )\n",
    "    \n",
    "    # Document formatting function\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    # Create the RAG chain - Modern, clean approach!\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": retriever | format_docs,\n",
    "            \"question\": RunnablePassthrough()\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ RAG chain created successfully!\")\n",
    "    print(\"ü§ñ Using gpt-4o-mini for cost-effective responses\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping RAG chain setup - no vector store available\")\n",
    "    rag_chain = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-7",
   "metadata": {},
   "source": [
    "## Step 7: Test the RAG Chain\n",
    "\n",
    "Let's test our RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-rag-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG chain\n",
    "if rag_chain:\n",
    "    print(\"üß™ Testing RAG chain...\")\n",
    "    \n",
    "    test_questions = [\n",
    "        \"Who was John F. Kennedy?\",\n",
    "        \"What was Robert Kennedy known for?\",\n",
    "        \"Tell me about the Kennedy family.\"\n",
    "    ]\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n‚ùì Question {i}: {question}\")\n",
    "        print(\"üí≠ Thinking...\")\n",
    "        \n",
    "        try:\n",
    "            # Use the chain to get an answer\n",
    "            answer = rag_chain.invoke(question)\n",
    "            print(f\"ü§ñ Answer: {answer}\\n\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            break\n",
    "            \n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Cannot test RAG chain - setup incomplete\")\n",
    "    print(\"Make sure you have:\")\n",
    "    print(\"1. PDF documents in the pdf-documents folder\")\n",
    "    print(\"2. Valid OpenAI API key\")\n",
    "    print(\"3. PostgreSQL with pgvector running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-8",
   "metadata": {},
   "source": [
    "## Step 8: FastAPI Server Implementation\n",
    "\n",
    "Now let's look at how the modern FastAPI server works (this runs separately from the notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-server-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine our modern FastAPI server code\n",
    "server_code = \"\"\"\n",
    "# v2-modern-step1/app/server.py\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class QueryRequest(BaseModel):\n",
    "    question: str\n",
    "\n",
    "class QueryResponse(BaseModel):\n",
    "    answer: str\n",
    "\n",
    "@app.post(\"/query\", response_model=QueryResponse)\n",
    "async def query_documents(request: QueryRequest):\n",
    "    try:\n",
    "        answer = await rag_chain.ainvoke(request.question)\n",
    "        return QueryResponse(answer=answer)\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error: {str(e)}\")\n",
    "\n",
    "@app.post(\"/stream\")\n",
    "async def stream_query(request: QueryRequest):\n",
    "    # Streaming support for real-time responses\n",
    "    ...\n",
    "\"\"\"\n",
    "\n",
    "print(\"üöÄ Modern FastAPI Server Features:\")\n",
    "print(\"‚úÖ Direct endpoint control (no LangServe)\")\n",
    "print(\"‚úÖ Custom error handling\")\n",
    "print(\"‚úÖ Proper request/response models\")\n",
    "print(\"‚úÖ Streaming support\")\n",
    "print(\"‚úÖ Health check endpoints\")\n",
    "print(\"‚úÖ Automatic API documentation\")\n",
    "\n",
    "print(\"\\nüìù To run the server:\")\n",
    "print(\"1. cd v2-modern-step1\")\n",
    "print(\"2. poetry shell\")\n",
    "print(\"3. uvicorn app.server:app --reload\")\n",
    "print(\"4. Visit http://localhost:8000/docs for API documentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-9",
   "metadata": {},
   "source": [
    "## Step 9: Performance and Cost Analysis\n",
    "\n",
    "Let's analyze the improvements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cost-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost analysis\n",
    "print(\"üí∞ Cost Analysis (2025 vs 2024):\")\n",
    "print(\"\\nüìä Embedding Costs:\")\n",
    "print(\"  2024: text-embedding-ada-002 = $0.0001/1k tokens\")\n",
    "print(\"  2025: text-embedding-3-small = $0.00002/1k tokens\")\n",
    "print(\"  üí° Savings: 5x cheaper!\")\n",
    "\n",
    "print(\"\\nü§ñ LLM Costs:\")\n",
    "print(\"  2024: gpt-4-1106-preview = $0.01/1k input tokens\")\n",
    "print(\"  2025: gpt-4o-mini = $0.00015/1k input tokens\")\n",
    "print(\"  üí° Savings: ~67x cheaper!\")\n",
    "\n",
    "# Estimate costs for a typical query\n",
    "if chunks:\n",
    "    avg_chunk_tokens = sum(len(chunk.page_content.split()) for chunk in chunks[:10]) // 10\n",
    "    context_tokens = avg_chunk_tokens * 4 * 1.3  # 4 chunks, ~1.3 words per token\n",
    "    \n",
    "    old_embedding_cost = len(chunks) * avg_chunk_tokens * 1.3 * 0.0001 / 1000\n",
    "    new_embedding_cost = len(chunks) * avg_chunk_tokens * 1.3 * 0.00002 / 1000\n",
    "    \n",
    "    old_llm_cost = context_tokens * 0.01 / 1000\n",
    "    new_llm_cost = context_tokens * 0.00015 / 1000\n",
    "    \n",
    "    print(f\"\\nüìà Example for {len(chunks)} chunks:\")\n",
    "    print(f\"  2024 embedding cost: ${old_embedding_cost:.4f}\")\n",
    "    print(f\"  2025 embedding cost: ${new_embedding_cost:.4f}\")\n",
    "    print(f\"  2024 LLM cost per query: ${old_llm_cost:.4f}\")\n",
    "    print(f\"  2025 LLM cost per query: ${new_llm_cost:.4f}\")\n",
    "    \n",
    "    total_old = old_embedding_cost + old_llm_cost\n",
    "    total_new = new_embedding_cost + new_llm_cost\n",
    "    savings = (total_old - total_new) / total_old * 100\n",
    "    \n",
    "    print(f\"\\nüí° Total cost reduction: {savings:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-10",
   "metadata": {},
   "source": [
    "## Step 10: Monitoring with LangSmith\n",
    "\n",
    "Check your LangSmith dashboard for monitoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "langsmith-info",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith monitoring info\n",
    "import os\n",
    "\n",
    "langsmith_configured = os.getenv('LANGCHAIN_TRACING_V2') == 'true'\n",
    "\n",
    "if langsmith_configured:\n",
    "    project_name = os.getenv('LANGCHAIN_PROJECT', 'ModernRAGStep1-2025')\n",
    "    print(f\"üìä LangSmith Monitoring Active!\")\n",
    "    print(f\"Project: {project_name}\")\n",
    "    print(f\"Dashboard: https://smith.langchain.com/\")\n",
    "    print(\"\\nüîç You can monitor:\")\n",
    "    print(\"  ‚Ä¢ Query performance\")\n",
    "    print(\"  ‚Ä¢ Token usage\")\n",
    "    print(\"  ‚Ä¢ Response quality\")\n",
    "    print(\"  ‚Ä¢ Error rates\")\n",
    "else:\n",
    "    print(\"üìä LangSmith monitoring not configured\")\n",
    "    print(\"To enable monitoring, set LANGCHAIN_TRACING_V2=true in .env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "üéâ **Congratulations!** You've successfully implemented a modern RAG application using 2025 best practices!\n",
    "\n",
    "### What We Built:\n",
    "‚úÖ **Modern Setup**: Python 3.13.3 + Poetry 2.1.4  \n",
    "‚úÖ **Cost-Effective Models**: text-embedding-3-small + gpt-4o-mini  \n",
    "‚úÖ **Direct FastAPI**: No deprecated LangServe dependency  \n",
    "‚úÖ **Clean Code**: Simplified, readable implementation  \n",
    "‚úÖ **Proper Error Handling**: Production-ready code  \n",
    "‚úÖ **Monitoring**: LangSmith integration  \n",
    "\n",
    "### Key Improvements Over 2024:\n",
    "- üí∞ **67x cheaper** LLM costs\n",
    "- üí∞ **5x cheaper** embedding costs\n",
    "- üöÄ **Better performance** with Python 3.13\n",
    "- üßπ **Cleaner code** without deprecated dependencies\n",
    "- üîß **More control** over API behavior\n",
    "\n",
    "### Next Steps:\n",
    "1. **Run the server**: `cd v2-modern-step1 && poetry shell && uvicorn app.server:app --reload`\n",
    "2. **Test the API**: Visit `http://localhost:8000/docs`\n",
    "3. **Compare with old approach**: Check out **nbv2-part1d-comparison.ipynb**\n",
    "\n",
    "### Production Considerations:\n",
    "- Add authentication and rate limiting\n",
    "- Implement proper logging\n",
    "- Use connection pooling for the database\n",
    "- Add caching for frequently asked questions\n",
    "- Monitor costs and performance in production\n",
    "\n",
    "Happy coding with modern RAG! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}