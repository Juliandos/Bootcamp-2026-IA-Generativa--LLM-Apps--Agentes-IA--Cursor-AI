{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Notebook 25: Backend RAG Implementation - Building the Document Processing Pipeline\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "Now that you understand the concepts behind RAG, it's time to build the real implementation! In this notebook, you'll transform your existing PDF CRUD backend into an intelligent document assistant by adding a complete RAG pipeline.\n",
    "\n",
    "You'll implement the `/qa-pdf/{id}` endpoint that can answer any question about any uploaded PDF file using the power of LangChain, OpenAI embeddings, and vector similarity search.\n",
    "\n",
    "## üõ†Ô∏è What We're Building\n",
    "\n",
    "**The RAG Endpoint Architecture:**\n",
    "```\n",
    "POST /pdfs/qa-pdf/{id}\n",
    "Body: {\"question\": \"What are the main conclusions?\"}\n",
    "Response: \"Based on the document, the main conclusions are...\"\n",
    "```\n",
    "\n",
    "**The Complete Processing Pipeline:**\n",
    "1. **Load PDF** from database/S3 storage\n",
    "2. **Extract text** using PyPDFLoader\n",
    "3. **Split into chunks** with RecursiveCharacterTextSplitter\n",
    "4. **Generate embeddings** with OpenAI\n",
    "5. **Create vector store** with FAISS\n",
    "6. **Set up RAG chain** with RetrievalQA\n",
    "7. **Process question** and return intelligent answer\n",
    "\n",
    "---\n",
    "\n",
    "**üí° Key Insight**: This single endpoint combines document processing, vector search, and AI generation into one seamless operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup and Dependencies\n",
    "\n",
    "### Required Packages Installation\n",
    "\n",
    "**üîß Core RAG Dependencies:**\n",
    "```bash\n",
    "# Navigate to your backend directory\n",
    "cd 001-langchain-pdf-fastapi-backend\n",
    "\n",
    "# Activate your virtual environment\n",
    "pyenv activate your-virtual-environment-name\n",
    "\n",
    "# Install the exact LangChain version we're using\n",
    "pip install langchain==0.1.1\n",
    "\n",
    "# Install additional required packages\n",
    "pip install boto3\n",
    "pip install python-multipart\n",
    "```\n",
    "\n",
    "**üîë Environment Configuration:**\n",
    "\n",
    "Add to your `backend/.env` file:\n",
    "```env\n",
    "# OpenAI API Key for embeddings and LLM\n",
    "OPENAI_API_KEY=sk-your-actual-openai-key-here\n",
    "\n",
    "# Existing environment variables\n",
    "DATABASE_URL=your-database-url\n",
    "AWS_ACCESS_KEY_ID=your-aws-key\n",
    "AWS_SECRET_ACCESS_KEY=your-aws-secret\n",
    "S3_BUCKET_NAME=your-bucket-name\n",
    "```\n",
    "\n",
    "### Understanding the Package Ecosystem\n",
    "\n",
    "**ü§ñ LangChain 0.1.1:**\n",
    "- **Why this version?** Stable API with all RAG components we need\n",
    "- **Core modules**: Document loaders, text splitters, embeddings, vector stores, chains\n",
    "- **Integration**: Works seamlessly with OpenAI and FAISS\n",
    "\n",
    "**‚òÅÔ∏è Boto3:**\n",
    "- **Purpose**: AWS S3 integration for PDF file access\n",
    "- **Usage**: Download PDF files from S3 for processing\n",
    "- **Alternative**: Direct file system access if not using S3\n",
    "\n",
    "**üì§ Python-multipart:**\n",
    "- **Purpose**: Handle file uploads and form data\n",
    "- **Usage**: Process the question request body in our endpoint\n",
    "- **FastAPI requirement**: Needed for request body parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Part 2: Enhanced Schema Definition\n",
    "\n",
    "### Adding Question Request Schema\n",
    "\n",
    "**üìù Update `backend/schemas.py`:**\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "\n",
    "# Existing PDF schemas\n",
    "class PDFRequest(BaseModel):\n",
    "    name: str\n",
    "    selected: bool\n",
    "    file: str\n",
    "\n",
    "class PDFResponse(BaseModel):\n",
    "    id: int\n",
    "    name: str\n",
    "    selected: bool\n",
    "    file: str\n",
    "\n",
    "    class Config:\n",
    "        from_attributes = True\n",
    "\n",
    "# NEW: Schema for PDF Q&A requests\n",
    "class QuestionRequest(BaseModel):\n",
    "    question: str\n",
    "    \n",
    "    class Config:\n",
    "        # Example for API documentation\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"question\": \"What are the main conclusions of this document?\"\n",
    "            }\n",
    "        }\n",
    "```\n",
    "\n",
    "### Why We Need This Schema\n",
    "\n",
    "**üéØ Type Safety:**\n",
    "- Ensures the request contains a valid question string\n",
    "- FastAPI automatically validates the request body\n",
    "- Provides clear error messages for malformed requests\n",
    "\n",
    "**üìö API Documentation:**\n",
    "- Automatically generates OpenAPI/Swagger documentation\n",
    "- Provides example requests in the FastAPI docs interface\n",
    "- Makes the API self-documenting for other developers\n",
    "\n",
    "**üîß Development Experience:**\n",
    "- IDE autocomplete and type checking\n",
    "- Clear contract between frontend and backend\n",
    "- Easy to extend with additional parameters later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Part 3: RAG Implementation in the PDF Router\n",
    "\n",
    "### Complete RAG-Enhanced Router Implementation\n",
    "\n",
    "**üìù Update `backend/routers/pdfs.py`:**\n",
    "\n",
    "```python\n",
    "from typing import List\n",
    "from sqlalchemy.orm import Session\n",
    "from fastapi import APIRouter, Depends, HTTPException, status, UploadFile, File\n",
    "import schemas\n",
    "import crud\n",
    "from database import SessionLocal\n",
    "from uuid import uuid4\n",
    "\n",
    "# Basic LangChain imports for text summarization\n",
    "from langchain import OpenAI, PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# RAG-specific imports for document Q&A\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from schemas import QuestionRequest\n",
    "\n",
    "# Initialize LLM instance for RAG\n",
    "llm = OpenAI(temperature=0)  # Temperature=0 for consistent, factual answers\n",
    "\n",
    "router = APIRouter(prefix=\"/pdfs\")\n",
    "\n",
    "def get_db():\n",
    "    db = SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "# ... (existing CRUD endpoints remain the same) ...\n",
    "\n",
    "# Basic LangChain text summarization (from previous tutorial)\n",
    "langchain_llm = OpenAI(temperature=0)\n",
    "\n",
    "summarize_template_string = \"\"\"\n",
    "        Provide a summary for the following text:\n",
    "        {text}\n",
    "\"\"\"\n",
    "\n",
    "summarize_prompt = PromptTemplate(\n",
    "    template=summarize_template_string,\n",
    "    input_variables=['text'],\n",
    ")\n",
    "\n",
    "summarize_chain = LLMChain(\n",
    "    llm=langchain_llm,\n",
    "    prompt=summarize_prompt,\n",
    ")\n",
    "\n",
    "@router.post('/summarize-text')\n",
    "async def summarize_text(text: str):\n",
    "    summary = summarize_chain.run(text=text)\n",
    "    return {'summary': summary}\n",
    "\n",
    "\n",
    "# NEW: Advanced RAG endpoint for PDF question answering\n",
    "@router.post(\"/qa-pdf/{id}\")\n",
    "def qa_pdf_by_id(id: int, question_request: QuestionRequest, db: Session = Depends(get_db)):\n",
    "    \"\"\"\n",
    "    Ask a question about a specific PDF document using RAG.\n",
    "    \n",
    "    This endpoint:\n",
    "    1. Retrieves the PDF from the database\n",
    "    2. Loads and processes the PDF content\n",
    "    3. Splits text into manageable chunks\n",
    "    4. Creates embeddings and vector store\n",
    "    5. Sets up retrieval-augmented generation\n",
    "    6. Answers the question based on document content\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Retrieve PDF from database\n",
    "    pdf = crud.read_pdf(db, id)\n",
    "    if pdf is None:\n",
    "        raise HTTPException(status_code=404, detail=\"PDF not found\")\n",
    "    \n",
    "    print(f\"Processing PDF: {pdf.file}\")  # Debug logging\n",
    "    \n",
    "    try:\n",
    "        # Step 2: Load PDF content using PyPDFLoader\n",
    "        loader = PyPDFLoader(pdf.file)\n",
    "        document = loader.load()\n",
    "        \n",
    "        # Step 3: Split document into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=3000,      # Size of each text chunk\n",
    "            chunk_overlap=400     # Overlap to maintain context\n",
    "        )\n",
    "        document_chunks = text_splitter.split_documents(document)\n",
    "        \n",
    "        print(f\"Created {len(document_chunks)} text chunks\")  # Debug logging\n",
    "        \n",
    "        # Step 4: Generate embeddings and create vector store\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        stored_embeddings = FAISS.from_documents(document_chunks, embeddings)\n",
    "        \n",
    "        # Step 5: Create RetrievalQA chain\n",
    "        QA_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",   # Stuff all retrieved chunks into one prompt\n",
    "            retriever=stored_embeddings.as_retriever()\n",
    "        )\n",
    "        \n",
    "        # Step 6: Process the question and generate answer\n",
    "        question = question_request.question\n",
    "        answer = QA_chain.run(question)\n",
    "        \n",
    "        return {\"answer\": answer}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF Q&A: {str(e)}\")  # Debug logging\n",
    "        raise HTTPException(\n",
    "            status_code=500, \n",
    "            detail=f\"Error processing PDF: {str(e)}\"\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Part 4: Understanding Each Component in Detail\n",
    "\n",
    "### Step-by-Step Breakdown of the RAG Pipeline\n",
    "\n",
    "**üîç Step 1: Database Retrieval**\n",
    "```python\n",
    "pdf = crud.read_pdf(db, id)\n",
    "if pdf is None:\n",
    "    raise HTTPException(status_code=404, detail=\"PDF not found\")\n",
    "```\n",
    "- **Purpose**: Validate that the PDF exists and get its file path\n",
    "- **Error handling**: Return 404 if PDF ID doesn't exist\n",
    "- **Security**: Ensures users can only query their accessible PDFs\n",
    "\n",
    "**üìÑ Step 2: Document Loading**\n",
    "```python\n",
    "loader = PyPDFLoader(pdf.file)\n",
    "document = loader.load()\n",
    "```\n",
    "- **PyPDFLoader**: Extracts text from PDF files, preserving page structure\n",
    "- **Document format**: Returns a list of Document objects with content and metadata\n",
    "- **File sources**: Works with local files, URLs, or S3 paths\n",
    "\n",
    "**‚úÇÔ∏è Step 3: Text Chunking**\n",
    "```python\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=3000,      # ~750 words per chunk\n",
    "    chunk_overlap=400     # 100 words overlap between chunks\n",
    ")\n",
    "document_chunks = text_splitter.split_documents(document)\n",
    "```\n",
    "\n",
    "**Why These Parameters?**\n",
    "- **chunk_size=3000**: Balances context vs. processing efficiency\n",
    "- **chunk_overlap=400**: Prevents important information from being split across chunks\n",
    "- **RecursiveCharacterTextSplitter**: Tries to split at natural boundaries (paragraphs, sentences)\n",
    "\n",
    "**üß† Step 4: Embedding Generation**\n",
    "```python\n",
    "embeddings = OpenAIEmbeddings()\n",
    "stored_embeddings = FAISS.from_documents(document_chunks, embeddings)\n",
    "```\n",
    "- **OpenAIEmbeddings**: Converts text to 1536-dimensional vectors\n",
    "- **FAISS**: Facebook AI Similarity Search - ultra-fast vector database\n",
    "- **Cost consideration**: Each chunk generates one embedding (~$0.0001 per chunk)\n",
    "\n",
    "**üîó Step 5: RAG Chain Setup**\n",
    "```python\n",
    "QA_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=stored_embeddings.as_retriever()\n",
    ")\n",
    "```\n",
    "- **RetrievalQA**: Orchestrates the entire retrieve-then-generate process\n",
    "- **\"stuff\" chain type**: Concatenates all retrieved chunks into one prompt\n",
    "- **Alternatives**: \"map_reduce\", \"refine\" for different use cases\n",
    "\n",
    "**‚ùì Step 6: Question Processing**\n",
    "```python\n",
    "question = question_request.question\n",
    "answer = QA_chain.run(question)\n",
    "```\n",
    "- **Semantic search**: Finds most relevant chunks based on question\n",
    "- **Context injection**: Combines question with relevant document excerpts\n",
    "- **LLM generation**: Produces answer based on provided context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Part 5: Error Handling and Production Considerations\n",
    "\n",
    "### Comprehensive Error Handling Strategy\n",
    "\n",
    "**üö® Common Error Scenarios:**\n",
    "\n",
    "**1. PDF Not Found:**\n",
    "```python\n",
    "if pdf is None:\n",
    "    raise HTTPException(status_code=404, detail=\"PDF not found\")\n",
    "```\n",
    "\n",
    "**2. File Access Issues:**\n",
    "```python\n",
    "try:\n",
    "    loader = PyPDFLoader(pdf.file)\n",
    "    document = loader.load()\n",
    "except FileNotFoundError:\n",
    "    raise HTTPException(status_code=404, detail=\"PDF file not accessible\")\n",
    "except Exception as e:\n",
    "    raise HTTPException(status_code=500, detail=f\"Error loading PDF: {str(e)}\")\n",
    "```\n",
    "\n",
    "**3. OpenAI API Issues:**\n",
    "```python\n",
    "try:\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    stored_embeddings = FAISS.from_documents(document_chunks, embeddings)\n",
    "except Exception as e:\n",
    "    if \"rate_limit\" in str(e).lower():\n",
    "        raise HTTPException(status_code=429, detail=\"OpenAI rate limit exceeded\")\n",
    "    elif \"api_key\" in str(e).lower():\n",
    "        raise HTTPException(status_code=500, detail=\"OpenAI API key configuration error\")\n",
    "    else:\n",
    "        raise HTTPException(status_code=500, detail=f\"AI processing error: {str(e)}\")\n",
    "```\n",
    "\n",
    "**4. Empty or Invalid PDFs:**\n",
    "```python\n",
    "if not document or len(document) == 0:\n",
    "    raise HTTPException(status_code=400, detail=\"PDF appears to be empty or unreadable\")\n",
    "    \n",
    "if len(document_chunks) == 0:\n",
    "    raise HTTPException(status_code=400, detail=\"No text content found in PDF\")\n",
    "```\n",
    "\n",
    "### Performance and Cost Optimization\n",
    "\n",
    "**‚è±Ô∏è Processing Time Considerations:**\n",
    "- **Small PDFs (1-10 pages)**: ~5-15 seconds\n",
    "- **Medium PDFs (10-50 pages)**: ~15-60 seconds\n",
    "- **Large PDFs (50+ pages)**: ~1-3 minutes\n",
    "\n",
    "**üí∞ Cost Breakdown (approximate):**\n",
    "- **Embeddings**: $0.0001 per text chunk\n",
    "- **10-page PDF**: ~20 chunks = $0.002 (0.2 cents)\n",
    "- **100-page PDF**: ~200 chunks = $0.02 (2 cents)\n",
    "- **Question answering**: $0.002 per question (GPT-3.5-turbo)\n",
    "\n",
    "**üéØ Optimization Strategies:**\n",
    "\n",
    "```python\n",
    "# Add caching for frequently accessed PDFs\n",
    "import hashlib\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def get_pdf_cache_key(pdf_id: int) -> str:\n",
    "    return f\"pdf_embeddings_{pdf_id}\"\n",
    "\n",
    "def cache_embeddings(pdf_id: int, embeddings):\n",
    "    cache_key = get_pdf_cache_key(pdf_id)\n",
    "    cache_path = f\"cache/{cache_key}.pkl\"\n",
    "    os.makedirs(\"cache\", exist_ok=True)\n",
    "    with open(cache_path, \"wb\") as f:\n",
    "        pickle.dump(embeddings, f)\n",
    "\n",
    "def load_cached_embeddings(pdf_id: int):\n",
    "    cache_key = get_pdf_cache_key(pdf_id)\n",
    "    cache_path = f\"cache/{cache_key}.pkl\"\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Part 6: Testing Your RAG Implementation\n",
    "\n",
    "### Starting the Backend Server\n",
    "\n",
    "**üöÄ Run the Development Server:**\n",
    "```bash\n",
    "# Navigate to backend directory\n",
    "cd 001-langchain-pdf-fastapi-backend\n",
    "\n",
    "# Activate virtual environment\n",
    "pyenv activate your-virtual-environment-name\n",
    "\n",
    "# Start the server\n",
    "uvicorn main:app --reload\n",
    "```\n",
    "\n",
    "**üìä Access the API Documentation:**\n",
    "Open your browser and go to: `http://127.0.0.1:8000/docs`\n",
    "\n",
    "### Testing the RAG Endpoint\n",
    "\n",
    "**üìã Step-by-Step Testing Process:**\n",
    "\n",
    "**1. Upload a Test PDF:**\n",
    "- Use the existing `/pdfs/upload` endpoint\n",
    "- Upload a PDF with readable text content\n",
    "- Note the returned PDF ID\n",
    "\n",
    "**2. Test Text Summarization First:**\n",
    "- Use the `/pdfs/summarize-text` endpoint\n",
    "- Input some sample text\n",
    "- Verify that basic LangChain integration works\n",
    "\n",
    "**3. Test RAG Q&A:**\n",
    "- Navigate to `/pdfs/qa-pdf/{id}` in the FastAPI docs\n",
    "- Enter the PDF ID from step 1\n",
    "- In the request body, enter:\n",
    "```json\n",
    "{\n",
    "  \"question\": \"What is this document about?\"\n",
    "}\n",
    "```\n",
    "- Execute the request\n",
    "\n",
    "### Example Test Scenarios\n",
    "\n",
    "**üìñ Test with Different Question Types:**\n",
    "\n",
    "**Factual Questions:**\n",
    "```json\n",
    "{\n",
    "  \"question\": \"What are the main topics covered in this document?\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Specific Detail Questions:**\n",
    "```json\n",
    "{\n",
    "  \"question\": \"What statistics or numbers are mentioned?\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Analytical Questions:**\n",
    "```json\n",
    "{\n",
    "  \"question\": \"What are the key conclusions or recommendations?\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Comparative Questions:**\n",
    "```json\n",
    "{\n",
    "  \"question\": \"What are the advantages and disadvantages mentioned?\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Interpreting Results\n",
    "\n",
    "**‚úÖ Good RAG Responses:**\n",
    "- Directly reference document content\n",
    "- Include specific details from the PDF\n",
    "- Stay grounded in the actual text\n",
    "- Acknowledge when information isn't in the document\n",
    "\n",
    "**‚ùå Poor RAG Responses:**\n",
    "- Generic answers not related to document\n",
    "- Hallucinated information not in the PDF\n",
    "- Very short or vague responses\n",
    "- Error messages or incomplete processing\n",
    "\n",
    "### Troubleshooting Common Issues\n",
    "\n",
    "**üîß Issue: \"PDF not found\" error**\n",
    "- Solution: Verify the PDF was uploaded successfully and use correct ID\n",
    "\n",
    "**üîß Issue: Long processing times**\n",
    "- Solution: Normal for large PDFs; consider implementing progress indicators\n",
    "\n",
    "**üîß Issue: OpenAI API key errors**\n",
    "- Solution: Verify API key in .env file and check OpenAI account balance\n",
    "\n",
    "**üîß Issue: Empty or generic responses**\n",
    "- Solution: Check if PDF has extractable text (not just images)\n",
    "\n",
    "**üîß Issue: Server timeout errors**\n",
    "- Solution: Increase timeout settings for very large documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Part 7: Advanced RAG Configurations and Optimizations\n",
    "\n",
    "### Fine-Tuning RAG Parameters\n",
    "\n",
    "**üéõÔ∏è Text Splitting Configuration:**\n",
    "```python\n",
    "# For technical documents with code\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,      # Smaller chunks for code\n",
    "    chunk_overlap=200,    # Less overlap needed\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"```\", \"###\"]  # Code-aware separators\n",
    ")\n",
    "\n",
    "# For narrative documents (reports, books)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=4000,      # Larger chunks for context\n",
    "    chunk_overlap=600,    # More overlap for narrative flow\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \", \"]  # Sentence-aware splitting\n",
    ")\n",
    "\n",
    "# For scientific papers\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=3000,      # Balanced approach\n",
    "    chunk_overlap=500,    # Preserve methodology connections\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"Abstract\", \"Introduction\", \"Methods\"]  # Section-aware\n",
    ")\n",
    "```\n",
    "\n",
    "**üîç Retrieval Configuration:**\n",
    "```python\n",
    "# Configure retrieval parameters\n",
    "retriever = stored_embeddings.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 4,              # Return top 4 most relevant chunks\n",
    "        \"score_threshold\": 0.5  # Minimum similarity threshold\n",
    "    }\n",
    ")\n",
    "\n",
    "# Alternative: MMR (Maximum Marginal Relevance) for diversity\n",
    "retriever = stored_embeddings.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 4,\n",
    "        \"fetch_k\": 10,       # Fetch 10, then diversify to 4\n",
    "        \"lambda_mult\": 0.7   # Balance relevance vs diversity\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "**üß† LLM Configuration:**\n",
    "```python\n",
    "# Factual, consistent answers\n",
    "llm = OpenAI(temperature=0, max_tokens=500)\n",
    "\n",
    "# More creative responses\n",
    "llm = OpenAI(temperature=0.3, max_tokens=800)\n",
    "\n",
    "# Use GPT-4 for better reasoning (more expensive)\n",
    "llm = OpenAI(model_name=\"gpt-4\", temperature=0, max_tokens=500)\n",
    "```\n",
    "\n",
    "### Alternative Chain Types\n",
    "\n",
    "**üìö \"stuff\" Chain (Current Implementation):**\n",
    "```python\n",
    "QA_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # Concatenate all chunks\n",
    "    retriever=retriever\n",
    ")\n",
    "```\n",
    "- **Pros**: Simple, fast for small chunks\n",
    "- **Cons**: Token limit issues with many/large chunks\n",
    "\n",
    "**üó∫Ô∏è \"map_reduce\" Chain:**\n",
    "```python\n",
    "QA_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"map_reduce\",  # Process chunks separately, then combine\n",
    "    retriever=retriever\n",
    ")\n",
    "```\n",
    "- **Pros**: Handles large documents better\n",
    "- **Cons**: More API calls, potentially less coherent\n",
    "\n",
    "**üîÑ \"refine\" Chain:**\n",
    "```python\n",
    "QA_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",  # Iteratively refine answer with each chunk\n",
    "    retriever=retriever\n",
    ")\n",
    "```\n",
    "- **Pros**: Progressive answer improvement\n",
    "- **Cons**: Slower, more expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Part 8: Production Deployment Considerations\n",
    "\n",
    "### Scalability and Performance\n",
    "\n",
    "**‚ö° Async Implementation for Better Performance:**\n",
    "```python\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "@router.post(\"/qa-pdf/{id}\")\n",
    "async def qa_pdf_by_id_async(id: int, question_request: QuestionRequest, db: Session = Depends(get_db)):\n",
    "    \"\"\"\n",
    "    Async version of RAG endpoint for better performance\n",
    "    \"\"\"\n",
    "    pdf = crud.read_pdf(db, id)\n",
    "    if pdf is None:\n",
    "        raise HTTPException(status_code=404, detail=\"PDF not found\")\n",
    "    \n",
    "    # Run RAG processing in thread pool to avoid blocking\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        answer = await asyncio.get_event_loop().run_in_executor(\n",
    "            executor, process_rag_question, pdf.file, question_request.question\n",
    "        )\n",
    "    \n",
    "    return {\"answer\": answer}\n",
    "\n",
    "def process_rag_question(pdf_file: str, question: str) -> str:\n",
    "    \"\"\"Separate function for RAG processing\"\"\"\n",
    "    # ... (RAG implementation here) ...\n",
    "    return answer\n",
    "```\n",
    "\n",
    "**üì¶ Caching Strategy:**\n",
    "```python\n",
    "import redis\n",
    "import json\n",
    "from datetime import timedelta\n",
    "\n",
    "# Initialize Redis client\n",
    "redis_client = redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "def cache_pdf_embeddings(pdf_id: int, embeddings_data: dict, ttl: int = 3600):\n",
    "    \"\"\"Cache embeddings for 1 hour by default\"\"\"\n",
    "    key = f\"pdf_embeddings:{pdf_id}\"\n",
    "    redis_client.setex(key, ttl, json.dumps(embeddings_data))\n",
    "\n",
    "def get_cached_embeddings(pdf_id: int) -> dict:\n",
    "    \"\"\"Retrieve cached embeddings\"\"\"\n",
    "    key = f\"pdf_embeddings:{pdf_id}\"\n",
    "    cached = redis_client.get(key)\n",
    "    return json.loads(cached) if cached else None\n",
    "```\n",
    "\n",
    "### Monitoring and Logging\n",
    "\n",
    "**üìä Comprehensive Logging:**\n",
    "```python\n",
    "import logging\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def log_performance(func):\n",
    "    \"\"\"Decorator to log RAG processing performance\"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            result = func(*args, **kwargs)\n",
    "            end_time = time.time()\n",
    "            logger.info(f\"RAG processing completed in {end_time - start_time:.2f} seconds\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            end_time = time.time()\n",
    "            logger.error(f\"RAG processing failed after {end_time - start_time:.2f} seconds: {str(e)}\")\n",
    "            raise\n",
    "    return wrapper\n",
    "\n",
    "@log_performance\n",
    "@router.post(\"/qa-pdf/{id}\")\n",
    "def qa_pdf_by_id(id: int, question_request: QuestionRequest, db: Session = Depends(get_db)):\n",
    "    # ... (implementation) ...\n",
    "```\n",
    "\n",
    "**üí∞ Cost Tracking:**\n",
    "```python\n",
    "import tiktoken\n",
    "\n",
    "def estimate_costs(text_chunks: list, question: str) -> dict:\n",
    "    \"\"\"Estimate OpenAI API costs for the operation\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    \n",
    "    # Embedding costs\n",
    "    total_embedding_tokens = sum(len(encoding.encode(chunk.page_content)) for chunk in text_chunks)\n",
    "    embedding_cost = (total_embedding_tokens / 1000) * 0.0001  # $0.0001 per 1K tokens\n",
    "    \n",
    "    # LLM costs (estimated)\n",
    "    question_tokens = len(encoding.encode(question))\n",
    "    context_tokens = sum(len(encoding.encode(chunk.page_content)) for chunk in text_chunks[:4])  # Top 4 chunks\n",
    "    llm_cost = ((question_tokens + context_tokens) / 1000) * 0.002  # $0.002 per 1K tokens\n",
    "    \n",
    "    return {\n",
    "        \"embedding_cost\": embedding_cost,\n",
    "        \"llm_cost\": llm_cost,\n",
    "        \"total_cost\": embedding_cost + llm_cost,\n",
    "        \"total_tokens\": total_embedding_tokens + question_tokens + context_tokens\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### What You've Built:\n",
    "\n",
    "1. **üîß Complete RAG Pipeline**: From PDF loading to intelligent question answering\n",
    "2. **üéØ Production-Ready Endpoint**: With proper error handling and validation\n",
    "3. **‚ö° Optimized Processing**: Efficient text chunking and embedding strategies\n",
    "4. **üîç Intelligent Retrieval**: Semantic search for finding relevant content\n",
    "5. **üß† Contextual Generation**: AI answers grounded in actual document content\n",
    "\n",
    "### Technical Skills Mastered:\n",
    "\n",
    "‚úÖ **Document Processing**: PyPDFLoader, text splitting, chunking strategies  \n",
    "‚úÖ **Vector Operations**: OpenAI embeddings, FAISS vector storage  \n",
    "‚úÖ **RAG Architecture**: RetrievalQA chains, different chain types  \n",
    "‚úÖ **API Design**: RESTful endpoints with proper schemas and error handling  \n",
    "‚úÖ **Performance Optimization**: Caching, async processing, cost management  \n",
    "\n",
    "### The RAG Implementation You've Created:\n",
    "\n",
    "```python\n",
    "# Your complete RAG endpoint\n",
    "@router.post(\"/qa-pdf/{id}\")\n",
    "def qa_pdf_by_id(id: int, question_request: QuestionRequest, db: Session = Depends(get_db)):\n",
    "    # 1. Validate PDF exists\n",
    "    # 2. Load and process document\n",
    "    # 3. Create embeddings and vector store\n",
    "    # 4. Set up RAG chain\n",
    "    # 5. Generate intelligent answer\n",
    "    return {\"answer\": answer}\n",
    "```\n",
    "\n",
    "### Next Steps in Your RAG Journey:\n",
    "\n",
    "**üìä Coming Up:**\n",
    "- **Notebook 26**: Deep dive into vector databases and embedding concepts\n",
    "- **Notebook 27**: Complete the frontend for a full RAG user experience\n",
    "\n",
    "**üöÄ Ready for Advanced Topics:**\n",
    "You now understand the core RAG implementation. In the next notebook, we'll explore the fascinating world of vector databases and embeddings - the mathematical foundation that makes semantic search possible.\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You've successfully implemented a production-ready RAG system that can intelligently answer questions about any PDF document. This is a significant achievement in modern AI development.\n",
    "\n",
    "**Your RAG backend is now ready.** Test it thoroughly with different PDFs and questions to see the power of document-grounded AI in action!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}