{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSM-005: Prompt Hub and Version Control\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Master the LangSmith Prompt Hub for collaborative prompt development\n",
    "- Implement prompt versioning and rollback strategies\n",
    "- Build systematic prompt optimization workflows\n",
    "- Create prompt templates with dynamic parameters\n",
    "- Set up team collaboration workflows for prompt engineering\n",
    "- Use A/B testing for prompt optimization\n",
    "- Implement prompt performance monitoring and analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup and Dependencies\n",
    "\n",
    "Let's start by setting up our prompt engineering environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for prompt engineering\n",
    "!pip install langsmith langchain langchain-openai langchain-hub\n",
    "!pip install python-dotenv pandas numpy matplotlib seaborn\n",
    "!pip install jinja2 pydantic typing-extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from jinja2 import Template\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import Client, traceable\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    PromptTemplate\n",
    ")\n",
    "from langchain.output_parsers import PydanticOutputParser, OutputFixingParser\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize clients\n",
    "client = Client()\n",
    "llm = ChatOpenAI(temperature=0.1, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "print(f\"‚úÖ Prompt Engineering environment ready\")\n",
    "print(f\"üìä Project: {os.getenv('LANGSMITH_PROJECT', 'Not set')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Advanced Prompt Design Patterns\n",
    "\n",
    "Let's start by exploring advanced prompt design patterns and templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced prompt design patterns\n",
    "\n",
    "class PromptPattern(BaseModel):\n",
    "    \"\"\"Structured prompt pattern definition\"\"\"\n",
    "    name: str = Field(description=\"Pattern name\")\n",
    "    description: str = Field(description=\"Pattern description\")\n",
    "    template: str = Field(description=\"Prompt template\")\n",
    "    variables: List[str] = Field(description=\"Required variables\")\n",
    "    use_cases: List[str] = Field(description=\"Common use cases\")\n",
    "    examples: List[Dict[str, str]] = Field(description=\"Example inputs/outputs\")\n",
    "\n",
    "class AdvancedPromptPatterns:\n",
    "    \"\"\"Collection of advanced prompt engineering patterns\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.patterns = self._initialize_patterns()\n",
    "    \n",
    "    def _initialize_patterns(self) -> Dict[str, PromptPattern]:\n",
    "        \"\"\"Initialize collection of prompt patterns\"\"\"\n",
    "        patterns = {}\n",
    "        \n",
    "        # Chain of Thought Pattern\n",
    "        patterns[\"chain_of_thought\"] = PromptPattern(\n",
    "            name=\"Chain of Thought\",\n",
    "            description=\"Step-by-step reasoning pattern for complex problems\",\n",
    "            template=\"\"\"Let's work through this step-by-step.\n",
    "\n",
    "Problem: {problem}\n",
    "\n",
    "Please provide your reasoning in clear steps:\n",
    "Step 1: [First step of reasoning]\n",
    "Step 2: [Second step of reasoning]\n",
    "...\n",
    "Final Answer: [Your conclusion]\n",
    "\n",
    "Remember to show your work and explain each step clearly.\"\"\",\n",
    "            variables=[\"problem\"],\n",
    "            use_cases=[\"Mathematical problems\", \"Logical reasoning\", \"Complex analysis\"],\n",
    "            examples=[\n",
    "                {\n",
    "                    \"problem\": \"If a train travels 60 mph for 2 hours, then 80 mph for 1.5 hours, what is the average speed?\",\n",
    "                    \"expected_steps\": \"Calculate total distance, total time, then average\"\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Few-Shot Learning Pattern\n",
    "        patterns[\"few_shot\"] = PromptPattern(\n",
    "            name=\"Few-Shot Learning\",\n",
    "            description=\"Learning pattern from examples\",\n",
    "            template=\"\"\"Here are some examples of {task_description}:\n",
    "\n",
    "{examples}\n",
    "\n",
    "Now, please {task_instruction} for the following:\n",
    "\n",
    "Input: {input}\n",
    "Output:\"\"\",\n",
    "            variables=[\"task_description\", \"examples\", \"task_instruction\", \"input\"],\n",
    "            use_cases=[\"Classification\", \"Text transformation\", \"Style adaptation\"],\n",
    "            examples=[\n",
    "                {\n",
    "                    \"task_description\": \"sentiment classification\",\n",
    "                    \"examples\": \"Input: I love this! Output: Positive\\nInput: This is terrible. Output: Negative\",\n",
    "                    \"input\": \"This product is amazing!\"\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Role-Playing Pattern\n",
    "        patterns[\"role_playing\"] = PromptPattern(\n",
    "            name=\"Role-Playing\",\n",
    "            description=\"AI assumes a specific role or persona\",\n",
    "            template=\"\"\"You are {role_description}. {role_context}\n",
    "\n",
    "Your characteristics:\n",
    "- {characteristic_1}\n",
    "- {characteristic_2}\n",
    "- {characteristic_3}\n",
    "\n",
    "User Request: {user_request}\n",
    "\n",
    "Please respond in character, maintaining your role throughout the conversation.\"\"\",\n",
    "            variables=[\"role_description\", \"role_context\", \"characteristic_1\", \"characteristic_2\", \"characteristic_3\", \"user_request\"],\n",
    "            use_cases=[\"Customer service\", \"Educational tutoring\", \"Creative writing\"],\n",
    "            examples=[\n",
    "                {\n",
    "                    \"role_description\": \"a helpful Python programming tutor\",\n",
    "                    \"characteristic_1\": \"Patient and encouraging\",\n",
    "                    \"user_request\": \"Help me understand loops\"\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Constraint-Based Pattern\n",
    "        patterns[\"constraint_based\"] = PromptPattern(\n",
    "            name=\"Constraint-Based\",\n",
    "            description=\"Output must satisfy specific constraints\",\n",
    "            template=\"\"\"Please {task} following these constraints:\n",
    "\n",
    "REQUIRED CONSTRAINTS:\n",
    "{constraints}\n",
    "\n",
    "ADDITIONAL GUIDELINES:\n",
    "{guidelines}\n",
    "\n",
    "Task Input: {input}\n",
    "\n",
    "Please ensure your response strictly adheres to all constraints listed above.\"\"\",\n",
    "            variables=[\"task\", \"constraints\", \"guidelines\", \"input\"],\n",
    "            use_cases=[\"Structured output\", \"Format compliance\", \"Content guidelines\"],\n",
    "            examples=[\n",
    "                {\n",
    "                    \"task\": \"write a summary\",\n",
    "                    \"constraints\": \"- Exactly 50 words\\n- Include 3 key points\\n- No technical jargon\",\n",
    "                    \"input\": \"Complex technical document\"\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def get_pattern(self, pattern_name: str) -> Optional[PromptPattern]:\n",
    "        \"\"\"Get a specific prompt pattern\"\"\"\n",
    "        return self.patterns.get(pattern_name)\n",
    "    \n",
    "    def list_patterns(self) -> List[str]:\n",
    "        \"\"\"List available patterns\"\"\"\n",
    "        return list(self.patterns.keys())\n",
    "    \n",
    "    def demonstrate_pattern(self, pattern_name: str, variables: Dict[str, str]):\n",
    "        \"\"\"Demonstrate a pattern with provided variables\"\"\"\n",
    "        pattern = self.get_pattern(pattern_name)\n",
    "        if not pattern:\n",
    "            print(f\"Pattern '{pattern_name}' not found.\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            template = Template(pattern.template)\n",
    "            filled_prompt = template.render(**variables)\n",
    "            \n",
    "            print(f\"üé® Pattern: {pattern.name}\")\n",
    "            print(f\"üìù Description: {pattern.description}\")\n",
    "            print(f\"üîß Use Cases: {', '.join(pattern.use_cases)}\")\n",
    "            print(f\"\\nüìã Generated Prompt:\")\n",
    "            print(\"=\" * 50)\n",
    "            print(filled_prompt)\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error demonstrating pattern: {e}\")\n",
    "\n",
    "# Initialize prompt patterns\n",
    "prompt_patterns = AdvancedPromptPatterns()\n",
    "\n",
    "print(\"üé® Advanced Prompt Patterns Library Initialized\")\n",
    "print(f\"üìö Available patterns: {', '.join(prompt_patterns.list_patterns())}\")\n",
    "\n",
    "# Demonstrate Chain of Thought pattern\n",
    "print(\"\\nüß† Demonstrating Chain of Thought Pattern:\")\n",
    "prompt_patterns.demonstrate_pattern(\n",
    "    \"chain_of_thought\",\n",
    "    {\"problem\": \"A company's revenue increased by 20% in Q1, then decreased by 15% in Q2. If the original revenue was $100,000, what is the revenue after Q2?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Prompt Hub Integration\n",
    "\n",
    "Now let's explore how to work with the LangSmith Prompt Hub for collaborative prompt development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Hub integration and management\n",
    "\n",
    "class PromptHubManager:\n",
    "    \"\"\"Manager for LangSmith Prompt Hub operations\"\"\"\n",
    "    \n",
    "    def __init__(self, client: Client):\n",
    "        self.client = client\n",
    "        self.llm = ChatOpenAI(temperature=0.3, model=\"gpt-3.5-turbo\")\n",
    "    \n",
    "    def create_prompt_template(self, name: str, template: str, description: str, \n",
    "                             variables: List[str], tags: List[str] = None) -> str:\n",
    "        \"\"\"Create a new prompt template in the hub\"\"\"\n",
    "        try:\n",
    "            # For demonstration purposes, we'll create local prompt templates\n",
    "            # In actual implementation, you'd use the LangSmith Hub API\n",
    "            \n",
    "            prompt_data = {\n",
    "                \"name\": name,\n",
    "                \"description\": description,\n",
    "                \"template\": template,\n",
    "                \"variables\": variables,\n",
    "                \"tags\": tags or [],\n",
    "                \"created_at\": datetime.now().isoformat(),\n",
    "                \"version\": \"1.0.0\"\n",
    "            }\n",
    "            \n",
    "            # Save locally for demo\n",
    "            filename = f\"prompt_{name.replace(' ', '_').lower()}.json\"\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(prompt_data, f, indent=2)\n",
    "            \n",
    "            print(f\"‚úÖ Prompt template '{name}' created: {filename}\")\n",
    "            return filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error creating prompt template: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def version_prompt(self, base_name: str, new_template: str, \n",
    "                      version_notes: str, variables: List[str]) -> str:\n",
    "        \"\"\"Create a new version of an existing prompt\"\"\"\n",
    "        try:\n",
    "            base_filename = f\"prompt_{base_name.replace(' ', '_').lower()}.json\"\n",
    "            \n",
    "            # Load base prompt\n",
    "            try:\n",
    "                with open(base_filename, 'r') as f:\n",
    "                    base_prompt = json.load(f)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"‚ùå Base prompt '{base_name}' not found\")\n",
    "                return None\n",
    "            \n",
    "            # Create new version\n",
    "            current_version = base_prompt.get(\"version\", \"1.0.0\")\n",
    "            version_parts = current_version.split(\".\")\n",
    "            new_minor = int(version_parts[1]) + 1\n",
    "            new_version = f\"{version_parts[0]}.{new_minor}.0\"\n",
    "            \n",
    "            new_prompt = {\n",
    "                **base_prompt,\n",
    "                \"template\": new_template,\n",
    "                \"variables\": variables,\n",
    "                \"version\": new_version,\n",
    "                \"updated_at\": datetime.now().isoformat(),\n",
    "                \"version_notes\": version_notes,\n",
    "                \"previous_version\": current_version\n",
    "            }\n",
    "            \n",
    "            # Save new version\n",
    "            new_filename = f\"prompt_{base_name.replace(' ', '_').lower()}_v{new_version.replace('.', '_')}.json\"\n",
    "            with open(new_filename, 'w') as f:\n",
    "                json.dump(new_prompt, f, indent=2)\n",
    "            \n",
    "            # Update base prompt\n",
    "            with open(base_filename, 'w') as f:\n",
    "                json.dump(new_prompt, f, indent=2)\n",
    "            \n",
    "            print(f\"‚úÖ New version {new_version} created: {new_filename}\")\n",
    "            return new_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error versioning prompt: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def load_prompt_template(self, name: str, version: str = None) -> Optional[Dict]:\n",
    "        \"\"\"Load a prompt template from the hub\"\"\"\n",
    "        try:\n",
    "            if version:\n",
    "                filename = f\"prompt_{name.replace(' ', '_').lower()}_v{version.replace('.', '_')}.json\"\n",
    "            else:\n",
    "                filename = f\"prompt_{name.replace(' ', '_').lower()}.json\"\n",
    "            \n",
    "            with open(filename, 'r') as f:\n",
    "                return json.load(f)\n",
    "                \n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ùå Prompt template '{name}' not found\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading prompt template: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def compare_prompt_versions(self, name: str, version1: str, version2: str):\n",
    "        \"\"\"Compare two versions of a prompt\"\"\"\n",
    "        prompt1 = self.load_prompt_template(name, version1)\n",
    "        prompt2 = self.load_prompt_template(name, version2)\n",
    "        \n",
    "        if not prompt1 or not prompt2:\n",
    "            print(\"‚ùå Could not load one or both prompt versions\")\n",
    "            return\n",
    "        \n",
    "        print(f\"üìä Comparing {name} v{version1} vs v{version2}:\\n\")\n",
    "        \n",
    "        print(f\"Version {version1}:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(prompt1[\"template\"][:200] + \"...\" if len(prompt1[\"template\"]) > 200 else prompt1[\"template\"])\n",
    "        \n",
    "        print(f\"\\nVersion {version2}:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(prompt2[\"template\"][:200] + \"...\" if len(prompt2[\"template\"]) > 200 else prompt2[\"template\"])\n",
    "        \n",
    "        print(f\"\\nChanges:\")\n",
    "        print(f\"- Variables: {prompt1.get('variables', [])} ‚Üí {prompt2.get('variables', [])}\")\n",
    "        print(f\"- Version notes: {prompt2.get('version_notes', 'No notes provided')}\")\n",
    "\n",
    "# Initialize Prompt Hub Manager\n",
    "prompt_hub = PromptHubManager(client)\n",
    "\n",
    "print(\"üèóÔ∏è Prompt Hub Manager initialized\")\n",
    "\n",
    "# Create sample prompt templates\n",
    "print(\"\\nüìù Creating Sample Prompt Templates...\")\n",
    "\n",
    "# Customer Service Prompt\n",
    "customer_service_template = \"\"\"\n",
    "You are a helpful customer service representative for {company_name}.\n",
    "\n",
    "Customer Context:\n",
    "- Customer Name: {customer_name}\n",
    "- Issue Type: {issue_type}\n",
    "- Priority: {priority}\n",
    "\n",
    "Customer Message: {customer_message}\n",
    "\n",
    "Please respond professionally and helpfully. Follow these guidelines:\n",
    "1. Acknowledge the customer's concern\n",
    "2. Provide a clear solution or next steps\n",
    "3. Offer additional assistance\n",
    "4. Maintain a friendly, professional tone\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt_hub.create_prompt_template(\n",
    "    name=\"Customer Service Response\",\n",
    "    template=customer_service_template,\n",
    "    description=\"Professional customer service response template\",\n",
    "    variables=[\"company_name\", \"customer_name\", \"issue_type\", \"priority\", \"customer_message\"],\n",
    "    tags=[\"customer_service\", \"support\", \"professional\"]\n",
    ")\n",
    "\n",
    "# Content Generation Prompt\n",
    "content_generation_template = \"\"\"\n",
    "Create engaging {content_type} content for {target_audience}.\n",
    "\n",
    "Topic: {topic}\n",
    "Tone: {tone}\n",
    "Length: {length}\n",
    "Key Points to Include:\n",
    "{key_points}\n",
    "\n",
    "Additional Requirements:\n",
    "{additional_requirements}\n",
    "\n",
    "Please ensure the content is:\n",
    "- Engaging and relevant to the target audience\n",
    "- Well-structured and easy to read\n",
    "- Optimized for the specified tone and length\n",
    "- Includes all key points naturally\n",
    "\n",
    "Content:\n",
    "\"\"\"\n",
    "\n",
    "prompt_hub.create_prompt_template(\n",
    "    name=\"Content Generation\",\n",
    "    template=content_generation_template,\n",
    "    description=\"Flexible content generation template for various formats\",\n",
    "    variables=[\"content_type\", \"target_audience\", \"topic\", \"tone\", \"length\", \"key_points\", \"additional_requirements\"],\n",
    "    tags=[\"content\", \"marketing\", \"creative\"]\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Sample prompt templates created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Prompt Version Control and A/B Testing\n",
    "\n",
    "Let's implement systematic prompt optimization through version control and A/B testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt optimization and A/B testing framework\n",
    "\n",
    "class PromptOptimizer:\n",
    "    \"\"\"Framework for systematic prompt optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, client: Client, prompt_hub: PromptHubManager):\n",
    "        self.client = client\n",
    "        self.prompt_hub = prompt_hub\n",
    "        self.llm = ChatOpenAI(temperature=0.1, model=\"gpt-3.5-turbo\")\n",
    "    \n",
    "    @traceable(run_type=\"prompt_test\", tags=[\"optimization\", \"testing\"])\n",
    "    def test_prompt_version(self, prompt_name: str, version: str, \n",
    "                           test_inputs: List[Dict], test_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Test a specific prompt version with given inputs\"\"\"\n",
    "        \n",
    "        prompt_data = self.prompt_hub.load_prompt_template(prompt_name, version)\n",
    "        if not prompt_data:\n",
    "            return {\"error\": \"Prompt not found\"}\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for i, test_input in enumerate(test_inputs):\n",
    "            try:\n",
    "                # Fill the prompt template\n",
    "                template = Template(prompt_data[\"template\"])\n",
    "                filled_prompt = template.render(**test_input)\n",
    "                \n",
    "                # Test with LLM\n",
    "                start_time = time.time()\n",
    "                response = self.llm.invoke([HumanMessage(content=filled_prompt)])\n",
    "                end_time = time.time()\n",
    "                \n",
    "                results.append({\n",
    "                    \"test_case\": i + 1,\n",
    "                    \"input\": test_input,\n",
    "                    \"output\": response.content,\n",
    "                    \"latency\": round(end_time - start_time, 3),\n",
    "                    \"success\": True\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    \"test_case\": i + 1,\n",
    "                    \"input\": test_input,\n",
    "                    \"error\": str(e),\n",
    "                    \"success\": False\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            \"prompt_name\": prompt_name,\n",
    "            \"version\": version,\n",
    "            \"test_name\": test_name,\n",
    "            \"total_tests\": len(test_inputs),\n",
    "            \"successful_tests\": sum(1 for r in results if r[\"success\"]),\n",
    "            \"average_latency\": np.mean([r[\"latency\"] for r in results if \"latency\" in r]),\n",
    "            \"results\": results\n",
    "        }\n",
    "    \n",
    "    def run_ab_test(self, prompt_name: str, version_a: str, version_b: str, \n",
    "                   test_inputs: List[Dict], test_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Run A/B test between two prompt versions\"\"\"\n",
    "        \n",
    "        print(f\"üß™ Running A/B test: {prompt_name} v{version_a} vs v{version_b}\")\n",
    "        \n",
    "        # Test version A\n",
    "        results_a = self.test_prompt_version(\n",
    "            prompt_name, version_a, test_inputs, f\"{test_name}_version_a\"\n",
    "        )\n",
    "        \n",
    "        # Test version B\n",
    "        results_b = self.test_prompt_version(\n",
    "            prompt_name, version_b, test_inputs, f\"{test_name}_version_b\"\n",
    "        )\n",
    "        \n",
    "        # Compare results\n",
    "        comparison = {\n",
    "            \"test_name\": test_name,\n",
    "            \"prompt_name\": prompt_name,\n",
    "            \"version_a\": {\n",
    "                \"version\": version_a,\n",
    "                \"success_rate\": results_a[\"successful_tests\"] / results_a[\"total_tests\"],\n",
    "                \"avg_latency\": results_a.get(\"average_latency\", 0),\n",
    "                \"results\": results_a\n",
    "            },\n",
    "            \"version_b\": {\n",
    "                \"version\": version_b,\n",
    "                \"success_rate\": results_b[\"successful_tests\"] / results_b[\"total_tests\"],\n",
    "                \"avg_latency\": results_b.get(\"average_latency\", 0),\n",
    "                \"results\": results_b\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Determine winner\n",
    "        if comparison[\"version_a\"][\"success_rate\"] > comparison[\"version_b\"][\"success_rate\"]:\n",
    "            winner = \"version_a\"\n",
    "        elif comparison[\"version_b\"][\"success_rate\"] > comparison[\"version_a\"][\"success_rate\"]:\n",
    "            winner = \"version_b\"\n",
    "        else:\n",
    "            # Tie-breaker: lower latency wins\n",
    "            winner = \"version_a\" if comparison[\"version_a\"][\"avg_latency\"] < comparison[\"version_b\"][\"avg_latency\"] else \"version_b\"\n",
    "        \n",
    "        comparison[\"winner\"] = winner\n",
    "        comparison[\"recommendation\"] = f\"Version {comparison[winner]['version']} performs better\"\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    def generate_prompt_variations(self, base_prompt: str, variation_types: List[str]) -> List[Dict]:\n",
    "        \"\"\"Generate prompt variations for testing\"\"\"\n",
    "        variations = [{\"type\": \"original\", \"prompt\": base_prompt}]\n",
    "        \n",
    "        for variation_type in variation_types:\n",
    "            try:\n",
    "                if variation_type == \"more_specific\":\n",
    "                    # Add more specific instructions\n",
    "                    variation = base_prompt + \"\\n\\nPlease be specific and provide detailed examples in your response.\"\n",
    "                \n",
    "                elif variation_type == \"more_concise\":\n",
    "                    # Request more concise output\n",
    "                    variation = base_prompt + \"\\n\\nPlease provide a concise, to-the-point response.\"\n",
    "                \n",
    "                elif variation_type == \"step_by_step\":\n",
    "                    # Add step-by-step instruction\n",
    "                    variation = \"Let's approach this step-by-step.\\n\\n\" + base_prompt + \"\\n\\nBreak down your response into clear steps.\"\n",
    "                \n",
    "                elif variation_type == \"with_examples\":\n",
    "                    # Request examples\n",
    "                    variation = base_prompt + \"\\n\\nPlease include relevant examples to illustrate your points.\"\n",
    "                \n",
    "                elif variation_type == \"creative\":\n",
    "                    # Encourage creativity\n",
    "                    variation = base_prompt + \"\\n\\nFeel free to be creative and think outside the box in your response.\"\n",
    "                \n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                variations.append({\"type\": variation_type, \"prompt\": variation})\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not generate {variation_type} variation: {e}\")\n",
    "        \n",
    "        return variations\n",
    "    \n",
    "    def optimize_prompt_iteratively(self, base_prompt: str, test_inputs: List[Dict], \n",
    "                                  iterations: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"Iteratively optimize a prompt through multiple rounds of testing\"\"\"\n",
    "        \n",
    "        print(f\"üîÑ Starting iterative prompt optimization ({iterations} iterations)\")\n",
    "        \n",
    "        current_best = base_prompt\n",
    "        optimization_history = []\n",
    "        \n",
    "        for iteration in range(iterations):\n",
    "            print(f\"\\nüìä Iteration {iteration + 1}/{iterations}\")\n",
    "            \n",
    "            # Generate variations\n",
    "            variations = self.generate_prompt_variations(\n",
    "                current_best, \n",
    "                [\"more_specific\", \"more_concise\", \"step_by_step\", \"with_examples\"]\n",
    "            )\n",
    "            \n",
    "            # Test each variation\n",
    "            best_score = 0\n",
    "            best_variation = None\n",
    "            \n",
    "            for var in variations:\n",
    "                try:\n",
    "                    # Simulate testing (in real scenario, you'd use actual evaluation metrics)\n",
    "                    test_score = np.random.uniform(0.6, 0.95)  # Simulated score\n",
    "                    \n",
    "                    if test_score > best_score:\n",
    "                        best_score = test_score\n",
    "                        best_variation = var\n",
    "                    \n",
    "                    print(f\"  - {var['type']}: {test_score:.3f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  - {var['type']}: Error - {e}\")\n",
    "            \n",
    "            if best_variation:\n",
    "                current_best = best_variation[\"prompt\"]\n",
    "                optimization_history.append({\n",
    "                    \"iteration\": iteration + 1,\n",
    "                    \"best_type\": best_variation[\"type\"],\n",
    "                    \"score\": best_score,\n",
    "                    \"prompt\": current_best\n",
    "                })\n",
    "                print(f\"  ‚úÖ Best: {best_variation['type']} (score: {best_score:.3f})\")\n",
    "        \n",
    "        return {\n",
    "            \"original_prompt\": base_prompt,\n",
    "            \"optimized_prompt\": current_best,\n",
    "            \"optimization_history\": optimization_history,\n",
    "            \"total_iterations\": iterations,\n",
    "            \"improvement_achieved\": len(optimization_history) > 0\n",
    "        }\n",
    "\n",
    "# Initialize Prompt Optimizer\n",
    "optimizer = PromptOptimizer(client, prompt_hub)\n",
    "\n",
    "print(\"üîÑ Prompt Optimizer initialized\")\n",
    "\n",
    "# Create an improved version of the customer service prompt\n",
    "print(\"\\nüìù Creating improved customer service prompt version...\")\n",
    "\n",
    "improved_customer_service_template = \"\"\"\n",
    "You are an expert customer service representative for {company_name}.\n",
    "\n",
    "Customer Profile:\n",
    "- Name: {customer_name}\n",
    "- Issue Category: {issue_type}\n",
    "- Priority Level: {priority}\n",
    "- Previous Interactions: [Check if customer has contacted before]\n",
    "\n",
    "Customer's Message: \"{customer_message}\"\n",
    "\n",
    "Response Framework:\n",
    "1. ACKNOWLEDGE: Personally acknowledge their specific concern\n",
    "2. EMPATHIZE: Show understanding of their situation\n",
    "3. SOLVE: Provide clear, actionable solution steps\n",
    "4. FOLLOW-UP: Offer additional help and next steps\n",
    "5. PERSONALIZE: Use customer's name and reference their specific situation\n",
    "\n",
    "Tone Guidelines:\n",
    "- Warm and professional\n",
    "- Confident in solutions\n",
    "- Proactive in offering help\n",
    "\n",
    "Your Response:\n",
    "\"\"\"\n",
    "\n",
    "# Create new version\n",
    "prompt_hub.version_prompt(\n",
    "    base_name=\"Customer Service Response\",\n",
    "    new_template=improved_customer_service_template,\n",
    "    version_notes=\"Added customer profile section, structured response framework, and enhanced tone guidelines\",\n",
    "    variables=[\"company_name\", \"customer_name\", \"issue_type\", \"priority\", \"customer_message\"]\n",
    ")\n",
    "\n",
    "# Run A/B test between versions\n",
    "print(\"\\nüß™ Running A/B test between prompt versions...\")\n",
    "\n",
    "test_inputs = [\n",
    "    {\n",
    "        \"company_name\": \"TechCorp\",\n",
    "        \"customer_name\": \"Sarah Johnson\",\n",
    "        \"issue_type\": \"Product Defect\",\n",
    "        \"priority\": \"High\",\n",
    "        \"customer_message\": \"I received my order yesterday and the screen is cracked. This is unacceptable for a premium product.\"\n",
    "    },\n",
    "    {\n",
    "        \"company_name\": \"TechCorp\",\n",
    "        \"customer_name\": \"Mike Chen\",\n",
    "        \"issue_type\": \"Billing Question\",\n",
    "        \"priority\": \"Medium\",\n",
    "        \"customer_message\": \"I was charged twice for my subscription this month. Can you help me understand why?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    ab_test_results = optimizer.run_ab_test(\n",
    "        prompt_name=\"Customer Service Response\",\n",
    "        version_a=\"1.0.0\",\n",
    "        version_b=\"1.1.0\",\n",
    "        test_inputs=test_inputs,\n",
    "        test_name=\"customer_service_optimization\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä A/B Test Results:\")\n",
    "    print(f\"üèÜ Winner: {ab_test_results['winner']}\")\n",
    "    print(f\"üí° Recommendation: {ab_test_results['recommendation']}\")\n",
    "    \n",
    "    # Display metrics comparison\n",
    "    print(f\"\\nüìà Performance Metrics:\")\n",
    "    print(f\"Version A (1.0.0): {ab_test_results['version_a']['success_rate']:.2%} success, {ab_test_results['version_a']['avg_latency']:.3f}s avg latency\")\n",
    "    print(f\"Version B (1.1.0): {ab_test_results['version_b']['success_rate']:.2%} success, {ab_test_results['version_b']['avg_latency']:.3f}s avg latency\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå A/B test failed: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Prompt optimization workflow completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ù Team Collaboration Workflows\n",
    "\n",
    "Let's implement collaborative workflows for prompt engineering teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Team collaboration tools for prompt engineering\n",
    "\n",
    "class PromptCollaboration:\n",
    "    \"\"\"Tools for collaborative prompt engineering\"\"\"\n",
    "    \n",
    "    def __init__(self, client: Client):\n",
    "        self.client = client\n",
    "        self.reviews = []  # In-memory storage for demo\n",
    "        self.approval_workflow = []\n",
    "    \n",
    "    def submit_prompt_for_review(self, prompt_name: str, version: str, \n",
    "                                author: str, reviewers: List[str], \n",
    "                                description: str) -> str:\n",
    "        \"\"\"Submit a prompt for peer review\"\"\"\n",
    "        \n",
    "        review_id = f\"review_{len(self.reviews) + 1}\"\n",
    "        \n",
    "        review_request = {\n",
    "            \"id\": review_id,\n",
    "            \"prompt_name\": prompt_name,\n",
    "            \"version\": version,\n",
    "            \"author\": author,\n",
    "            \"reviewers\": reviewers,\n",
    "            \"description\": description,\n",
    "            \"status\": \"pending\",\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"reviews\": [],\n",
    "            \"comments\": []\n",
    "        }\n",
    "        \n",
    "        self.reviews.append(review_request)\n",
    "        \n",
    "        print(f\"üìù Review request {review_id} created for '{prompt_name}' v{version}\")\n",
    "        print(f\"üë• Reviewers assigned: {', '.join(reviewers)}\")\n",
    "        \n",
    "        return review_id\n",
    "    \n",
    "    def add_review_comment(self, review_id: str, reviewer: str, \n",
    "                          comment: str, rating: int = None, \n",
    "                          suggestions: List[str] = None) -> bool:\n",
    "        \"\"\"Add a review comment to a prompt\"\"\"\n",
    "        \n",
    "        review = next((r for r in self.reviews if r[\"id\"] == review_id), None)\n",
    "        if not review:\n",
    "            print(f\"‚ùå Review {review_id} not found\")\n",
    "            return False\n",
    "        \n",
    "        if reviewer not in review[\"reviewers\"]:\n",
    "            print(f\"‚ùå {reviewer} is not assigned as a reviewer for this prompt\")\n",
    "            return False\n",
    "        \n",
    "        review_comment = {\n",
    "            \"reviewer\": reviewer,\n",
    "            \"comment\": comment,\n",
    "            \"rating\": rating,  # 1-5 scale\n",
    "            \"suggestions\": suggestions or [],\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        review[\"reviews\"].append(review_comment)\n",
    "        \n",
    "        print(f\"‚úÖ Review added by {reviewer} for {review_id}\")\n",
    "        if rating:\n",
    "            print(f\"‚≠ê Rating: {rating}/5\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def approve_prompt(self, review_id: str, approver: str, \n",
    "                      approval_notes: str = \"\") -> bool:\n",
    "        \"\"\"Approve a prompt for production use\"\"\"\n",
    "        \n",
    "        review = next((r for r in self.reviews if r[\"id\"] == review_id), None)\n",
    "        if not review:\n",
    "            print(f\"‚ùå Review {review_id} not found\")\n",
    "            return False\n",
    "        \n",
    "        # Check if all reviewers have provided feedback\n",
    "        reviewers_who_reviewed = {r[\"reviewer\"] for r in review[\"reviews\"]}\n",
    "        missing_reviewers = set(review[\"reviewers\"]) - reviewers_who_reviewed\n",
    "        \n",
    "        if missing_reviewers:\n",
    "            print(f\"‚ö†Ô∏è  Warning: Still waiting for reviews from: {', '.join(missing_reviewers)}\")\n",
    "        \n",
    "        # Calculate average rating\n",
    "        ratings = [r[\"rating\"] for r in review[\"reviews\"] if r[\"rating\"]]\n",
    "        avg_rating = np.mean(ratings) if ratings else None\n",
    "        \n",
    "        approval = {\n",
    "            \"review_id\": review_id,\n",
    "            \"approver\": approver,\n",
    "            \"approval_notes\": approval_notes,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"average_rating\": avg_rating,\n",
    "            \"total_reviews\": len(review[\"reviews\"])\n",
    "        }\n",
    "        \n",
    "        review[\"status\"] = \"approved\"\n",
    "        review[\"approval\"] = approval\n",
    "        self.approval_workflow.append(approval)\n",
    "        \n",
    "        print(f\"‚úÖ Prompt approved by {approver}\")\n",
    "        if avg_rating:\n",
    "            print(f\"üìä Average rating: {avg_rating:.1f}/5.0\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def generate_review_report(self, review_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate a comprehensive review report\"\"\"\n",
    "        \n",
    "        review = next((r for r in self.reviews if r[\"id\"] == review_id), None)\n",
    "        if not review:\n",
    "            return {\"error\": f\"Review {review_id} not found\"}\n",
    "        \n",
    "        # Compile suggestions\n",
    "        all_suggestions = []\n",
    "        for r in review[\"reviews\"]:\n",
    "            all_suggestions.extend(r[\"suggestions\"])\n",
    "        \n",
    "        # Calculate metrics\n",
    "        ratings = [r[\"rating\"] for r in review[\"reviews\"] if r[\"rating\"]]\n",
    "        \n",
    "        report = {\n",
    "            \"review_summary\": {\n",
    "                \"id\": review_id,\n",
    "                \"prompt_name\": review[\"prompt_name\"],\n",
    "                \"version\": review[\"version\"],\n",
    "                \"author\": review[\"author\"],\n",
    "                \"status\": review[\"status\"],\n",
    "                \"created_at\": review[\"created_at\"]\n",
    "            },\n",
    "            \"review_metrics\": {\n",
    "                \"total_reviewers\": len(review[\"reviewers\"]),\n",
    "                \"reviews_completed\": len(review[\"reviews\"]),\n",
    "                \"average_rating\": np.mean(ratings) if ratings else None,\n",
    "                \"rating_distribution\": {i: ratings.count(i) for i in range(1, 6)} if ratings else {},\n",
    "                \"completion_rate\": len(review[\"reviews\"]) / len(review[\"reviewers\"]) if review[\"reviewers\"] else 0\n",
    "            },\n",
    "            \"feedback_summary\": {\n",
    "                \"total_suggestions\": len(all_suggestions),\n",
    "                \"common_themes\": self._extract_common_themes(all_suggestions),\n",
    "                \"individual_reviews\": review[\"reviews\"]\n",
    "            },\n",
    "            \"recommendations\": self._generate_recommendations(review)\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def _extract_common_themes(self, suggestions: List[str]) -> List[str]:\n",
    "        \"\"\"Extract common themes from review suggestions\"\"\"\n",
    "        # Simple keyword-based theme extraction\n",
    "        themes = []\n",
    "        keywords = [\"clarity\", \"specificity\", \"examples\", \"tone\", \"structure\", \"length\"]\n",
    "        \n",
    "        for keyword in keywords:\n",
    "            if any(keyword.lower() in suggestion.lower() for suggestion in suggestions):\n",
    "                themes.append(keyword.title())\n",
    "        \n",
    "        return themes\n",
    "    \n",
    "    def _generate_recommendations(self, review: Dict) -> List[str]:\n",
    "        \"\"\"Generate actionable recommendations based on review feedback\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        ratings = [r[\"rating\"] for r in review[\"reviews\"] if r[\"rating\"]]\n",
    "        if ratings:\n",
    "            avg_rating = np.mean(ratings)\n",
    "            if avg_rating < 3.0:\n",
    "                recommendations.append(\"Consider significant revisions before production deployment\")\n",
    "            elif avg_rating < 4.0:\n",
    "                recommendations.append(\"Address reviewer feedback and consider minor revisions\")\n",
    "            else:\n",
    "                recommendations.append(\"Prompt is ready for production with excellent review scores\")\n",
    "        \n",
    "        # Check completion rate\n",
    "        completion_rate = len(review[\"reviews\"]) / len(review[\"reviewers\"]) if review[\"reviewers\"] else 0\n",
    "        if completion_rate < 0.8:\n",
    "            recommendations.append(\"Consider getting additional reviews before final approval\")\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def create_collaboration_dashboard(self) -> Dict[str, Any]:\n",
    "        \"\"\"Create a dashboard view of all collaboration activities\"\"\"\n",
    "        \n",
    "        dashboard = {\n",
    "            \"overview\": {\n",
    "                \"total_reviews\": len(self.reviews),\n",
    "                \"pending_reviews\": len([r for r in self.reviews if r[\"status\"] == \"pending\"]),\n",
    "                \"approved_prompts\": len([r for r in self.reviews if r[\"status\"] == \"approved\"]),\n",
    "                \"total_approvals\": len(self.approval_workflow)\n",
    "            },\n",
    "            \"active_reviews\": [\n",
    "                {\n",
    "                    \"id\": r[\"id\"],\n",
    "                    \"prompt_name\": r[\"prompt_name\"],\n",
    "                    \"author\": r[\"author\"],\n",
    "                    \"reviewers\": r[\"reviewers\"],\n",
    "                    \"reviews_completed\": len(r[\"reviews\"]),\n",
    "                    \"status\": r[\"status\"]\n",
    "                }\n",
    "                for r in self.reviews if r[\"status\"] == \"pending\"\n",
    "            ],\n",
    "            \"recent_approvals\": self.approval_workflow[-5:],  # Last 5 approvals\n",
    "            \"team_metrics\": self._calculate_team_metrics()\n",
    "        }\n",
    "        \n",
    "        return dashboard\n",
    "    \n",
    "    def _calculate_team_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate team collaboration metrics\"\"\"\n",
    "        all_reviewers = set()\n",
    "        all_authors = set()\n",
    "        review_times = []\n",
    "        \n",
    "        for review in self.reviews:\n",
    "            all_authors.add(review[\"author\"])\n",
    "            all_reviewers.update(review[\"reviewers\"])\n",
    "            \n",
    "            # Calculate review completion time (simplified)\n",
    "            if review[\"reviews\"]:\n",
    "                created_time = datetime.fromisoformat(review[\"created_at\"])\n",
    "                last_review_time = max(\n",
    "                    datetime.fromisoformat(r[\"timestamp\"]) \n",
    "                    for r in review[\"reviews\"]\n",
    "                )\n",
    "                review_times.append((last_review_time - created_time).total_seconds() / 3600)  # Hours\n",
    "        \n",
    "        return {\n",
    "            \"active_team_members\": len(all_reviewers | all_authors),\n",
    "            \"active_reviewers\": len(all_reviewers),\n",
    "            \"active_authors\": len(all_authors),\n",
    "            \"avg_review_time_hours\": np.mean(review_times) if review_times else 0\n",
    "        }\n",
    "\n",
    "# Initialize collaboration tools\n",
    "collaboration = PromptCollaboration(client)\n",
    "\n",
    "print(\"ü§ù Prompt Collaboration Tools initialized\")\n",
    "\n",
    "# Demo collaboration workflow\n",
    "print(\"\\nüìã Demonstrating Collaboration Workflow...\")\n",
    "\n",
    "# Submit prompt for review\n",
    "review_id = collaboration.submit_prompt_for_review(\n",
    "    prompt_name=\"Customer Service Response\",\n",
    "    version=\"1.1.0\",\n",
    "    author=\"Alice Johnson\",\n",
    "    reviewers=[\"Bob Smith\", \"Carol Davis\", \"David Wilson\"],\n",
    "    description=\"Improved customer service prompt with structured response framework\"\n",
    ")\n",
    "\n",
    "# Add review comments\n",
    "print(\"\\nüí¨ Adding review comments...\")\n",
    "\n",
    "collaboration.add_review_comment(\n",
    "    review_id=review_id,\n",
    "    reviewer=\"Bob Smith\",\n",
    "    comment=\"Great improvement! The structured framework makes responses more consistent. Consider adding examples for complex scenarios.\",\n",
    "    rating=4,\n",
    "    suggestions=[\"Add examples for edge cases\", \"Consider tone guidance for different priority levels\"]\n",
    ")\n",
    "\n",
    "collaboration.add_review_comment(\n",
    "    review_id=review_id,\n",
    "    reviewer=\"Carol Davis\",\n",
    "    comment=\"The response framework is excellent. The ACKNOWLEDGE-EMPATHIZE-SOLVE-FOLLOW-UP structure is very clear.\",\n",
    "    rating=5,\n",
    "    suggestions=[\"Perfect as is\", \"Maybe add personalization tips\"]\n",
    ")\n",
    "\n",
    "collaboration.add_review_comment(\n",
    "    review_id=review_id,\n",
    "    reviewer=\"David Wilson\",\n",
    "    comment=\"Good structure but might be too lengthy for simple issues. Consider a simplified version for low-priority cases.\",\n",
    "    rating=3,\n",
    "    suggestions=[\"Create simplified version\", \"Add conditional logic for issue complexity\"]\n",
    ")\n",
    "\n",
    "# Generate review report\n",
    "print(\"\\nüìä Generating Review Report...\")\n",
    "report = collaboration.generate_review_report(review_id)\n",
    "\n",
    "print(f\"\\nüìã Review Report for {report['review_summary']['prompt_name']} v{report['review_summary']['version']}:\")\n",
    "print(f\"üìà Completion Rate: {report['review_metrics']['completion_rate']:.1%}\")\n",
    "print(f\"‚≠ê Average Rating: {report['review_metrics']['average_rating']:.1f}/5.0\")\n",
    "print(f\"üí° Common Themes: {', '.join(report['feedback_summary']['common_themes'])}\")\n",
    "print(f\"üéØ Recommendations:\")\n",
    "for rec in report['recommendations']:\n",
    "    print(f\"  - {rec}\")\n",
    "\n",
    "# Approve the prompt\n",
    "print(\"\\n‚úÖ Approving prompt...\")\n",
    "collaboration.approve_prompt(\n",
    "    review_id=review_id,\n",
    "    approver=\"Emma Thompson\",\n",
    "    approval_notes=\"Approved for production with minor suggestions for future iterations\"\n",
    ")\n",
    "\n",
    "# Show collaboration dashboard\n",
    "print(\"\\nüìä Collaboration Dashboard:\")\n",
    "dashboard = collaboration.create_collaboration_dashboard()\n",
    "print(f\"üìà Overview: {dashboard['overview']['total_reviews']} total reviews, {dashboard['overview']['approved_prompts']} approved\")\n",
    "print(f\"üë• Team: {dashboard['team_metrics']['active_team_members']} active members\")\n",
    "\n",
    "print(\"\\n‚úÖ Collaboration workflow demonstration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Prompt Performance Analytics\n",
    "\n",
    "Let's implement comprehensive analytics for prompt performance monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt performance analytics and monitoring\n",
    "\n",
    "class PromptAnalytics:\n",
    "    \"\"\"Advanced analytics for prompt performance monitoring\"\"\"\n",
    "    \n",
    "    def __init__(self, client: Client):\n",
    "        self.client = client\n",
    "        self.performance_data = []  # In-memory storage for demo\n",
    "    \n",
    "    @traceable(run_type=\"analytics\", tags=[\"performance-tracking\"])\n",
    "    def track_prompt_performance(self, prompt_name: str, version: str, \n",
    "                                metrics: Dict[str, Any]) -> None:\n",
    "        \"\"\"Track performance metrics for a prompt\"\"\"\n",
    "        \n",
    "        performance_record = {\n",
    "            \"prompt_name\": prompt_name,\n",
    "            \"version\": version,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"metrics\": metrics\n",
    "        }\n",
    "        \n",
    "        self.performance_data.append(performance_record)\n",
    "        print(f\"üìä Performance metrics tracked for {prompt_name} v{version}\")\n",
    "    \n",
    "    def analyze_prompt_trends(self, prompt_name: str, days_back: int = 30) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze performance trends for a prompt over time\"\"\"\n",
    "        \n",
    "        # Filter data for the specified prompt and time period\n",
    "        cutoff_date = datetime.now().timestamp() - (days_back * 24 * 3600)\n",
    "        \n",
    "        relevant_data = [\n",
    "            record for record in self.performance_data\n",
    "            if (record[\"prompt_name\"] == prompt_name and \n",
    "                datetime.fromisoformat(record[\"timestamp\"]).timestamp() >= cutoff_date)\n",
    "        ]\n",
    "        \n",
    "        if not relevant_data:\n",
    "            return {\"error\": f\"No data found for {prompt_name} in the last {days_back} days\"}\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        all_metrics = [record[\"metrics\"] for record in relevant_data]\n",
    "        \n",
    "        # Calculate trends for common metrics\n",
    "        trend_analysis = {\n",
    "            \"prompt_name\": prompt_name,\n",
    "            \"analysis_period_days\": days_back,\n",
    "            \"total_data_points\": len(relevant_data),\n",
    "            \"date_range\": {\n",
    "                \"start\": min(record[\"timestamp\"] for record in relevant_data),\n",
    "                \"end\": max(record[\"timestamp\"] for record in relevant_data)\n",
    "            },\n",
    "            \"metric_trends\": {}\n",
    "        }\n",
    "        \n",
    "        # Analyze each metric\n",
    "        metric_names = set()\n",
    "        for metrics in all_metrics:\n",
    "            metric_names.update(metrics.keys())\n",
    "        \n",
    "        for metric_name in metric_names:\n",
    "            values = [metrics.get(metric_name) for metrics in all_metrics if metric_name in metrics]\n",
    "            numeric_values = [v for v in values if isinstance(v, (int, float))]\n",
    "            \n",
    "            if numeric_values:\n",
    "                trend_analysis[\"metric_trends\"][metric_name] = {\n",
    "                    \"average\": np.mean(numeric_values),\n",
    "                    \"median\": np.median(numeric_values),\n",
    "                    \"std_deviation\": np.std(numeric_values),\n",
    "                    \"min\": min(numeric_values),\n",
    "                    \"max\": max(numeric_values),\n",
    "                    \"trend\": self._calculate_trend(numeric_values),\n",
    "                    \"data_points\": len(numeric_values)\n",
    "                }\n",
    "        \n",
    "        return trend_analysis\n",
    "    \n",
    "    def _calculate_trend(self, values: List[float]) -> str:\n",
    "        \"\"\"Calculate trend direction (improving, declining, stable)\"\"\"\n",
    "        if len(values) < 2:\n",
    "            return \"insufficient_data\"\n",
    "        \n",
    "        # Simple linear trend calculation\n",
    "        x = list(range(len(values)))\n",
    "        slope = np.polyfit(x, values, 1)[0]\n",
    "        \n",
    "        if slope > 0.01:  # Threshold for significant positive trend\n",
    "            return \"improving\"\n",
    "        elif slope < -0.01:  # Threshold for significant negative trend\n",
    "            return \"declining\"\n",
    "        else:\n",
    "            return \"stable\"\n",
    "    \n",
    "    def compare_prompt_versions_performance(self, prompt_name: str, \n",
    "                                          versions: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Compare performance between different prompt versions\"\"\"\n",
    "        \n",
    "        version_data = {}\n",
    "        \n",
    "        for version in versions:\n",
    "            version_records = [\n",
    "                record for record in self.performance_data\n",
    "                if (record[\"prompt_name\"] == prompt_name and \n",
    "                    record[\"version\"] == version)\n",
    "            ]\n",
    "            \n",
    "            if version_records:\n",
    "                all_metrics = [record[\"metrics\"] for record in version_records]\n",
    "                version_data[version] = self._aggregate_metrics(all_metrics)\n",
    "        \n",
    "        if not version_data:\n",
    "            return {\"error\": \"No performance data found for specified versions\"}\n",
    "        \n",
    "        # Generate comparison\n",
    "        comparison = {\n",
    "            \"prompt_name\": prompt_name,\n",
    "            \"versions_compared\": list(version_data.keys()),\n",
    "            \"version_metrics\": version_data,\n",
    "            \"recommendations\": self._generate_version_recommendations(version_data)\n",
    "        }\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    def _aggregate_metrics(self, all_metrics: List[Dict]) -> Dict[str, Any]:\n",
    "        \"\"\"Aggregate metrics from multiple records\"\"\"\n",
    "        aggregated = {}\n",
    "        \n",
    "        # Get all metric names\n",
    "        metric_names = set()\n",
    "        for metrics in all_metrics:\n",
    "            metric_names.update(metrics.keys())\n",
    "        \n",
    "        # Aggregate each metric\n",
    "        for metric_name in metric_names:\n",
    "            values = [metrics.get(metric_name) for metrics in all_metrics if metric_name in metrics]\n",
    "            numeric_values = [v for v in values if isinstance(v, (int, float))]\n",
    "            \n",
    "            if numeric_values:\n",
    "                aggregated[metric_name] = {\n",
    "                    \"mean\": np.mean(numeric_values),\n",
    "                    \"median\": np.median(numeric_values),\n",
    "                    \"std\": np.std(numeric_values),\n",
    "                    \"count\": len(numeric_values)\n",
    "                }\n",
    "        \n",
    "        return aggregated\n",
    "    \n",
    "    def _generate_version_recommendations(self, version_data: Dict) -> List[str]:\n",
    "        \"\"\"Generate recommendations based on version comparison\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        if len(version_data) < 2:\n",
    "            recommendations.append(\"Need at least 2 versions for meaningful comparison\")\n",
    "            return recommendations\n",
    "        \n",
    "        # Compare key metrics between versions\n",
    "        versions = list(version_data.keys())\n",
    "        \n",
    "        # Look for common metrics to compare\n",
    "        common_metrics = set(version_data[versions[0]].keys())\n",
    "        for version in versions[1:]:\n",
    "            common_metrics &= set(version_data[version].keys())\n",
    "        \n",
    "        if \"accuracy\" in common_metrics or \"success_rate\" in common_metrics:\n",
    "            metric_name = \"accuracy\" if \"accuracy\" in common_metrics else \"success_rate\"\n",
    "            best_version = max(versions, key=lambda v: version_data[v][metric_name][\"mean\"])\n",
    "            recommendations.append(f\"Version {best_version} shows highest {metric_name}\")\n",
    "        \n",
    "        if \"latency\" in common_metrics or \"response_time\" in common_metrics:\n",
    "            metric_name = \"latency\" if \"latency\" in common_metrics else \"response_time\"\n",
    "            fastest_version = min(versions, key=lambda v: version_data[v][metric_name][\"mean\"])\n",
    "            recommendations.append(f\"Version {fastest_version} shows lowest {metric_name}\")\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def create_performance_dashboard(self) -> Dict[str, Any]:\n",
    "        \"\"\"Create a comprehensive performance dashboard\"\"\"\n",
    "        \n",
    "        if not self.performance_data:\n",
    "            return {\"message\": \"No performance data available\"}\n",
    "        \n",
    "        # Overall statistics\n",
    "        unique_prompts = len(set(record[\"prompt_name\"] for record in self.performance_data))\n",
    "        unique_versions = len(set(f\"{record['prompt_name']}:{record['version']}\" for record in self.performance_data))\n",
    "        \n",
    "        # Recent activity\n",
    "        recent_cutoff = datetime.now().timestamp() - (7 * 24 * 3600)  # 7 days\n",
    "        recent_data = [\n",
    "            record for record in self.performance_data\n",
    "            if datetime.fromisoformat(record[\"timestamp\"]).timestamp() >= recent_cutoff\n",
    "        ]\n",
    "        \n",
    "        # Top performing prompts\n",
    "        prompt_performance = {}\n",
    "        for record in self.performance_data:\n",
    "            prompt_name = record[\"prompt_name\"]\n",
    "            if prompt_name not in prompt_performance:\n",
    "                prompt_performance[prompt_name] = []\n",
    "            prompt_performance[prompt_name].append(record[\"metrics\"])\n",
    "        \n",
    "        dashboard = {\n",
    "            \"overview\": {\n",
    "                \"total_data_points\": len(self.performance_data),\n",
    "                \"unique_prompts\": unique_prompts,\n",
    "                \"unique_versions\": unique_versions,\n",
    "                \"recent_activity_7d\": len(recent_data)\n",
    "            },\n",
    "            \"recent_activity\": recent_data[-10:],  # Last 10 records\n",
    "            \"prompt_summary\": {\n",
    "                prompt_name: {\n",
    "                    \"total_measurements\": len(metrics_list),\n",
    "                    \"latest_metrics\": metrics_list[-1] if metrics_list else None\n",
    "                }\n",
    "                for prompt_name, metrics_list in prompt_performance.items()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return dashboard\n",
    "    \n",
    "    def visualize_performance_trends(self, prompt_name: str):\n",
    "        \"\"\"Create visualizations of prompt performance trends\"\"\"\n",
    "        \n",
    "        # Filter data for the prompt\n",
    "        prompt_data = [\n",
    "            record for record in self.performance_data\n",
    "            if record[\"prompt_name\"] == prompt_name\n",
    "        ]\n",
    "        \n",
    "        if not prompt_data:\n",
    "            print(f\"No data found for prompt: {prompt_name}\")\n",
    "            return\n",
    "        \n",
    "        # Extract timestamps and metrics\n",
    "        timestamps = [datetime.fromisoformat(record[\"timestamp\"]) for record in prompt_data]\n",
    "        \n",
    "        # Plot trends for numeric metrics\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        metrics_to_plot = [\"accuracy\", \"latency\", \"success_rate\", \"cost\"]\n",
    "        \n",
    "        for i, metric_name in enumerate(metrics_to_plot):\n",
    "            if i >= len(axes):\n",
    "                break\n",
    "            \n",
    "            values = []\n",
    "            metric_timestamps = []\n",
    "            \n",
    "            for j, record in enumerate(prompt_data):\n",
    "                if metric_name in record[\"metrics\"] and isinstance(record[\"metrics\"][metric_name], (int, float)):\n",
    "                    values.append(record[\"metrics\"][metric_name])\n",
    "                    metric_timestamps.append(timestamps[j])\n",
    "            \n",
    "            if values:\n",
    "                axes[i].plot(metric_timestamps, values, marker='o', linewidth=2, markersize=6)\n",
    "                axes[i].set_title(f'{metric_name.title()} Trend for {prompt_name}')\n",
    "                axes[i].set_xlabel('Time')\n",
    "                axes[i].set_ylabel(metric_name.title())\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "                axes[i].tick_params(axis='x', rotation=45)\n",
    "                \n",
    "                # Add trend line\n",
    "                if len(values) > 1:\n",
    "                    z = np.polyfit(range(len(values)), values, 1)\n",
    "                    p = np.poly1d(z)\n",
    "                    axes[i].plot(metric_timestamps, p(range(len(values))), \"--\", alpha=0.7, color='red')\n",
    "            else:\n",
    "                axes[i].text(0.5, 0.5, f'No {metric_name} data', transform=axes[i].transAxes, \n",
    "                           ha='center', va='center', fontsize=12)\n",
    "                axes[i].set_title(f'{metric_name.title()} - No Data')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize analytics\n",
    "analytics = PromptAnalytics(client)\n",
    "\n",
    "print(\"üìä Prompt Analytics initialized\")\n",
    "\n",
    "# Simulate performance data collection\n",
    "print(\"\\nüìà Simulating performance data collection...\")\n",
    "\n",
    "# Generate sample performance data\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "prompts_to_track = [\n",
    "    (\"Customer Service Response\", \"1.0.0\"),\n",
    "    (\"Customer Service Response\", \"1.1.0\"),\n",
    "    (\"Content Generation\", \"1.0.0\")\n",
    "]\n",
    "\n",
    "base_time = datetime.now() - timedelta(days=30)\n",
    "\n",
    "for prompt_name, version in prompts_to_track:\n",
    "    for day in range(30):\n",
    "        # Generate realistic performance metrics\n",
    "        metrics = {\n",
    "            \"accuracy\": random.uniform(0.75, 0.95),\n",
    "            \"latency\": random.uniform(0.8, 2.5),\n",
    "            \"success_rate\": random.uniform(0.85, 0.98),\n",
    "            \"cost\": random.uniform(0.001, 0.005),\n",
    "            \"user_satisfaction\": random.uniform(3.5, 4.8)\n",
    "        }\n",
    "        \n",
    "        # Simulate version improvements\n",
    "        if version == \"1.1.0\":\n",
    "            metrics[\"accuracy\"] += 0.05  # Improved version\n",
    "            metrics[\"success_rate\"] += 0.03\n",
    "            metrics[\"user_satisfaction\"] += 0.2\n",
    "        \n",
    "        # Add some noise and temporal trends\n",
    "        improvement_factor = day / 30 * 0.1  # Gradual improvement over time\n",
    "        metrics[\"accuracy\"] = min(0.98, metrics[\"accuracy\"] + improvement_factor)\n",
    "        \n",
    "        analytics.performance_data.append({\n",
    "            \"prompt_name\": prompt_name,\n",
    "            \"version\": version,\n",
    "            \"timestamp\": (base_time + timedelta(days=day)).isoformat(),\n",
    "            \"metrics\": metrics\n",
    "        })\n",
    "\n",
    "print(f\"‚úÖ Generated {len(analytics.performance_data)} performance data points\")\n",
    "\n",
    "# Analyze trends\n",
    "print(\"\\nüìä Analyzing Performance Trends...\")\n",
    "\n",
    "trend_analysis = analytics.analyze_prompt_trends(\"Customer Service Response\", days_back=30)\n",
    "if \"error\" not in trend_analysis:\n",
    "    print(f\"\\nüìà Trend Analysis for Customer Service Response:\")\n",
    "    print(f\"üìä Data Points: {trend_analysis['total_data_points']}\")\n",
    "    \n",
    "    for metric_name, trend_data in trend_analysis['metric_trends'].items():\n",
    "        print(f\"\\n{metric_name.upper()}:\")\n",
    "        print(f\"  Average: {trend_data['average']:.3f}\")\n",
    "        print(f\"  Trend: {trend_data['trend']}\")\n",
    "        print(f\"  Range: {trend_data['min']:.3f} - {trend_data['max']:.3f}\")\n",
    "\n",
    "# Compare versions\n",
    "print(\"\\nüÜö Comparing Prompt Versions...\")\n",
    "\n",
    "version_comparison = analytics.compare_prompt_versions_performance(\n",
    "    \"Customer Service Response\", [\"1.0.0\", \"1.1.0\"]\n",
    ")\n",
    "\n",
    "if \"error\" not in version_comparison:\n",
    "    print(f\"\\nüìä Version Comparison Results:\")\n",
    "    for version, metrics in version_comparison['version_metrics'].items():\n",
    "        print(f\"\\nVersion {version}:\")\n",
    "        for metric_name, metric_data in metrics.items():\n",
    "            print(f\"  {metric_name}: {metric_data['mean']:.3f} (¬±{metric_data['std']:.3f})\")\n",
    "    \n",
    "    print(f\"\\nüí° Recommendations:\")\n",
    "    for rec in version_comparison['recommendations']:\n",
    "        print(f\"  - {rec}\")\n",
    "\n",
    "# Create dashboard\n",
    "print(\"\\nüìä Performance Dashboard:\")\n",
    "dashboard = analytics.create_performance_dashboard()\n",
    "print(f\"üìà Overview: {dashboard['overview']['total_data_points']} data points across {dashboard['overview']['unique_prompts']} prompts\")\n",
    "print(f\"üîÑ Recent Activity: {dashboard['overview']['recent_activity_7d']} measurements in last 7 days\")\n",
    "\n",
    "# Visualize trends\n",
    "print(\"\\nüìà Generating Performance Visualizations...\")\n",
    "analytics.visualize_performance_trends(\"Customer Service Response\")\n",
    "\n",
    "print(\"\\n‚úÖ Prompt performance analytics demonstration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Key Takeaways and Best Practices\n",
    "\n",
    "### ‚úÖ What You've Mastered\n",
    "\n",
    "1. **Advanced Prompt Design Patterns**:\n",
    "   - Chain of Thought reasoning\n",
    "   - Few-shot learning templates\n",
    "   - Role-playing and persona-based prompts\n",
    "   - Constraint-based structured outputs\n",
    "\n",
    "2. **Prompt Hub Integration**:\n",
    "   - Template creation and management\n",
    "   - Version control and rollback strategies\n",
    "   - A/B testing frameworks\n",
    "   - Collaborative development workflows\n",
    "\n",
    "3. **Team Collaboration**:\n",
    "   - Peer review processes\n",
    "   - Approval workflows\n",
    "   - Feedback collection and analysis\n",
    "   - Team performance metrics\n",
    "\n",
    "4. **Performance Analytics**:\n",
    "   - Comprehensive metrics tracking\n",
    "   - Trend analysis and visualization\n",
    "   - Version comparison frameworks\n",
    "   - Real-time performance monitoring\n",
    "\n",
    "### üéØ Best Practices for Production\n",
    "\n",
    "1. **Prompt Design**:\n",
    "   - Start with proven patterns and adapt to your needs\n",
    "   - Use clear, specific instructions\n",
    "   - Include examples when possible\n",
    "   - Test with diverse inputs and edge cases\n",
    "\n",
    "2. **Version Management**:\n",
    "   - Maintain clear version numbering\n",
    "   - Document all changes with rationale\n",
    "   - Test new versions thoroughly before deployment\n",
    "   - Keep rollback plans ready\n",
    "\n",
    "3. **Collaboration Workflows**:\n",
    "   - Establish clear review criteria\n",
    "   - Involve domain experts in reviews\n",
    "   - Use structured feedback forms\n",
    "   - Maintain approval audit trails\n",
    "\n",
    "4. **Performance Monitoring**:\n",
    "   - Track key business metrics continuously\n",
    "   - Set up alerts for performance degradation\n",
    "   - Regular trend analysis and optimization\n",
    "   - Compare versions systematically\n",
    "\n",
    "### üîß Advanced Tips\n",
    "\n",
    "- **Template Variables**: Use descriptive variable names and provide examples\n",
    "- **Conditional Logic**: Consider different prompts for different scenarios\n",
    "- **Performance Baselines**: Establish baselines before optimization\n",
    "- **User Feedback**: Incorporate real user feedback into prompt iterations\n",
    "- **Documentation**: Maintain comprehensive prompt documentation\n",
    "\n",
    "### üö® Common Pitfalls to Avoid\n",
    "\n",
    "- **Over-optimization**: Don't optimize for metrics that don't reflect real value\n",
    "- **Version Confusion**: Always clearly identify which version is in production\n",
    "- **Insufficient Testing**: Test with realistic, diverse data\n",
    "- **Ignoring Edge Cases**: Consider unusual inputs and failure scenarios\n",
    "- **Solo Development**: Always involve others in prompt review and testing\n",
    "\n",
    "## üöÄ What's Next?\n",
    "\n",
    "You've mastered collaborative prompt engineering! Continue to:\n",
    "\n",
    "- **LSM-006: Production Monitoring** - Set up enterprise-grade monitoring with OpenTelemetry integration\n",
    "- **LSM-007: Advanced Patterns** - Explore complex use cases and integration patterns\n",
    "- **LSM-008: Tips and FAQs** - Learn pro tips and troubleshooting techniques\n",
    "\n",
    "---\n",
    "\n",
    "**Ready for enterprise-grade monitoring?** Continue to **LSM-006: Production Monitoring** to master production-grade operations and monitoring! üè≠"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}