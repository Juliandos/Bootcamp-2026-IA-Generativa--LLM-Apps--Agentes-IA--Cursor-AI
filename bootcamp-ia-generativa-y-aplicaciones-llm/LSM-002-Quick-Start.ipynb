{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSM-002: Your First LangSmith Project\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Set up your development environment with LangSmith\n",
    "- Create and run your first traced LLM application\n",
    "- Navigate the LangSmith dashboard\n",
    "- Understand basic tracing concepts through hands-on experience\n",
    "- Run your first evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Environment Setup\n",
    "\n",
    "Let's start by installing the necessary packages and setting up your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langsmith openai python-dotenv\n",
    "\n",
    "# Optional: Install langchain if you want to see LangChain integration examples\n",
    "!pip install langchain langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë Configuration\n",
    "\n",
    "Before we start coding, you'll need to configure your API keys. Create a `.env` file in your project directory with the following variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Required environment variables:\n",
    "# LANGSMITH_API_KEY=your_langsmith_api_key_here\n",
    "# LANGSMITH_PROJECT=your_project_name_here\n",
    "# OPENAI_API_KEY=your_openai_api_key_here (optional, for OpenAI examples)\n",
    "\n",
    "# Verify your setup\n",
    "langsmith_api_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "langsmith_project = os.getenv(\"LANGSMITH_PROJECT\")\n",
    "\n",
    "if not langsmith_api_key:\n",
    "    print(\"‚ö†Ô∏è  LANGSMITH_API_KEY not found. Please set it in your .env file.\")\n",
    "else:\n",
    "    print(f\"‚úÖ LangSmith API Key: {langsmith_api_key[:8]}...\")\n",
    "\n",
    "if not langsmith_project:\n",
    "    print(\"‚ö†Ô∏è  LANGSMITH_PROJECT not found. Please set it in your .env file.\")\n",
    "else:\n",
    "    print(f\"‚úÖ LangSmith Project: {langsmith_project}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Your .env File Template\n",
    "\n",
    "Create a file named `.env` in your project directory with this content:\n",
    "\n",
    "```env\n",
    "# LangSmith Configuration\n",
    "LANGSMITH_API_KEY=your_langsmith_api_key_here\n",
    "LANGSMITH_PROJECT=learning-langsmith\n",
    "LANGSMITH_TRACING=true\n",
    "\n",
    "# Optional: OpenAI API Key (for OpenAI examples)\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "\n",
    "# Optional: Other LLM API keys\n",
    "# ANTHROPIC_API_KEY=your_anthropic_key_here\n",
    "# GOOGLE_API_KEY=your_google_key_here\n",
    "```\n",
    "\n",
    "Replace the placeholder values with your actual API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Example 1: Simple LLM Call with Manual Tracing\n",
    "\n",
    "Let's start with a basic example using the LangSmith SDK directly to trace a simple LLM call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "import openai\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI()\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_openai(messages, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"A simple traced OpenAI call\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test the function\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"}\n",
    "]\n",
    "\n",
    "try:\n",
    "    result = call_openai(messages)\n",
    "    print(\"Response:\")\n",
    "    print(result)\n",
    "    print(\"\\n‚úÖ Success! Check your LangSmith dashboard to see the trace.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Make sure your OpenAI API key is set correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Example 2: Multi-Step Application with LangChain\n",
    "\n",
    "Let's create a more complex application that demonstrates multiple steps being traced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langsmith import traceable\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(temperature=0.7, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def research_assistant(topic):\n",
    "    \"\"\"A multi-step research assistant that generates questions and provides answers\"\"\"\n",
    "    \n",
    "    # Step 1: Generate research questions\n",
    "    question_messages = [\n",
    "        SystemMessage(content=\"You are a research assistant. Generate 3 interesting research questions about the given topic.\"),\n",
    "        HumanMessage(content=f\"Topic: {topic}\")\n",
    "    ]\n",
    "    \n",
    "    questions = llm.invoke(question_messages)\n",
    "    \n",
    "    # Step 2: Answer the first question in detail\n",
    "    answer_messages = [\n",
    "        SystemMessage(content=\"You are an expert researcher. Provide a detailed answer to the research question.\"),\n",
    "        HumanMessage(content=f\"Research questions: {questions.content}\\n\\nPlease answer the first question in detail.\")\n",
    "    ]\n",
    "    \n",
    "    answer = llm.invoke(answer_messages)\n",
    "    \n",
    "    return {\n",
    "        \"topic\": topic,\n",
    "        \"questions\": questions.content,\n",
    "        \"detailed_answer\": answer.content\n",
    "    }\n",
    "\n",
    "# Test the research assistant\n",
    "try:\n",
    "    result = research_assistant(\"Renewable Energy\")\n",
    "    \n",
    "    print(f\"üìö Research Topic: {result['topic']}\\n\")\n",
    "    print(f\"‚ùì Generated Questions:\\n{result['questions']}\\n\")\n",
    "    print(f\"üìñ Detailed Answer:\\n{result['detailed_answer']}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Multi-step trace created! Check your LangSmith dashboard.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Make sure your API keys are configured correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Example 3: Adding Metadata and Tags\n",
    "\n",
    "Let's enhance our tracing with metadata and tags for better organization and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "import time\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.2, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"chain\",\n",
    "    metadata={\"version\": \"1.0\", \"environment\": \"development\"},\n",
    "    tags=[\"sentiment-analysis\", \"quickstart\"]\n",
    ")\n",
    "def sentiment_analyzer(text, include_reasoning=True):\n",
    "    \"\"\"Analyze sentiment with optional reasoning\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create prompt based on requirements\n",
    "    if include_reasoning:\n",
    "        prompt = f\"\"\"Analyze the sentiment of the following text and provide reasoning:\n",
    "        \n",
    "Text: {text}\n",
    "\n",
    "Please provide:\n",
    "1. Sentiment (Positive/Negative/Neutral)\n",
    "2. Confidence score (0-1)\n",
    "3. Brief reasoning\n",
    "\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"Analyze the sentiment of the following text:\n",
    "        \n",
    "Text: {text}\n",
    "\n",
    "Respond with just: Sentiment (Positive/Negative/Neutral) and Confidence (0-1)\n",
    "\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are an expert sentiment analysis assistant.\"),\n",
    "        HumanMessage(content=prompt)\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        \"input_text\": text,\n",
    "        \"analysis\": response.content,\n",
    "        \"include_reasoning\": include_reasoning,\n",
    "        \"processing_time_seconds\": round(processing_time, 2)\n",
    "    }\n",
    "\n",
    "# Test with different types of text\n",
    "test_texts = [\n",
    "    \"I absolutely love this new product! It exceeded all my expectations.\",\n",
    "    \"The service was terrible and I waited for hours.\",\n",
    "    \"The weather today is quite ordinary, nothing special.\"\n",
    "]\n",
    "\n",
    "print(\"üé≠ Sentiment Analysis Results:\\n\")\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    try:\n",
    "        result = sentiment_analyzer(text, include_reasoning=(i % 2 == 1))\n",
    "        \n",
    "        print(f\"üìù Example {i}:\")\n",
    "        print(f\"Text: {result['input_text'][:60]}...\")\n",
    "        print(f\"Analysis: {result['analysis']}\")\n",
    "        print(f\"‚è±Ô∏è  Processing time: {result['processing_time_seconds']}s\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing example {i}: {e}\\n\")\n",
    "\n",
    "print(\"‚úÖ All sentiment analyses completed! Check your dashboard for traces with tags and metadata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Example 4: Creating Your First Dataset and Evaluation\n",
    "\n",
    "Now let's create a simple dataset and run an evaluation to test our sentiment analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "# Initialize LangSmith client\n",
    "client = Client()\n",
    "\n",
    "# Create a dataset for sentiment analysis\n",
    "dataset_name = \"sentiment-analysis-quickstart\"\n",
    "\n",
    "# Sample data for our dataset\n",
    "examples = [\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"This movie was absolutely fantastic! I loved every minute of it.\"},\n",
    "        \"outputs\": {\"expected_sentiment\": \"Positive\", \"confidence\": 0.9}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"The food was terrible and the service was even worse.\"},\n",
    "        \"outputs\": {\"expected_sentiment\": \"Negative\", \"confidence\": 0.9}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"The weather today is okay, not great but not bad either.\"},\n",
    "        \"outputs\": {\"expected_sentiment\": \"Neutral\", \"confidence\": 0.7}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"I'm thrilled with this purchase! Exactly what I needed.\"},\n",
    "        \"outputs\": {\"expected_sentiment\": \"Positive\", \"confidence\": 0.95}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"This is the worst experience I've ever had.\"},\n",
    "        \"outputs\": {\"expected_sentiment\": \"Negative\", \"confidence\": 0.95}\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Create the dataset\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=\"A small dataset for testing sentiment analysis\"\n",
    "    )\n",
    "    \n",
    "    # Add examples to the dataset\n",
    "    client.create_examples(\n",
    "        inputs=[example[\"inputs\"] for example in examples],\n",
    "        outputs=[example[\"outputs\"] for example in examples],\n",
    "        dataset_id=dataset.id\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Dataset '{dataset_name}' created successfully with {len(examples)} examples!\")\n",
    "    print(f\"üìä Dataset ID: {dataset.id}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e):\n",
    "        print(f\"‚ÑπÔ∏è  Dataset '{dataset_name}' already exists. That's okay!\")\n",
    "        # Get the existing dataset\n",
    "        datasets = list(client.list_datasets(dataset_name=dataset_name))\n",
    "        if datasets:\n",
    "            dataset = datasets[0]\n",
    "            print(f\"üìä Using existing dataset ID: {dataset.id}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error creating dataset: {e}\")\n",
    "        dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's run an evaluation\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# Create a simple evaluator function\n",
    "def sentiment_correctness_evaluator(run, example):\n",
    "    \"\"\"Evaluator that checks if the predicted sentiment matches expected sentiment\"\"\"\n",
    "    \n",
    "    # Extract the actual prediction from the run output\n",
    "    prediction = run.outputs.get(\"analysis\", \"\")\n",
    "    expected = example.outputs.get(\"expected_sentiment\", \"\")\n",
    "    \n",
    "    # Simple keyword matching (in a real scenario, you'd use more sophisticated parsing)\n",
    "    prediction_lower = prediction.lower()\n",
    "    expected_lower = expected.lower()\n",
    "    \n",
    "    # Check if the expected sentiment appears in the prediction\n",
    "    is_correct = expected_lower in prediction_lower\n",
    "    \n",
    "    return {\n",
    "        \"key\": \"sentiment_accuracy\",\n",
    "        \"score\": 1.0 if is_correct else 0.0,\n",
    "        \"comment\": f\"Expected: {expected}, Found in prediction: {is_correct}\"\n",
    "    }\n",
    "\n",
    "# Wrapper function for evaluation\n",
    "def sentiment_analyzer_for_eval(inputs):\n",
    "    \"\"\"Wrapper function that matches the evaluation interface\"\"\"\n",
    "    result = sentiment_analyzer(inputs[\"text\"], include_reasoning=False)\n",
    "    return result\n",
    "\n",
    "if dataset:\n",
    "    try:\n",
    "        print(\"üß™ Running evaluation...\")\n",
    "        \n",
    "        # Run the evaluation\n",
    "        results = evaluate(\n",
    "            sentiment_analyzer_for_eval,\n",
    "            data=dataset_name,\n",
    "            evaluators=[sentiment_correctness_evaluator],\n",
    "            experiment_prefix=\"sentiment-quickstart\",\n",
    "            description=\"Quick start sentiment analysis evaluation\"\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Evaluation completed!\")\n",
    "        print(f\"üìä Check your LangSmith dashboard to see detailed results.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error running evaluation: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping evaluation because dataset wasn't created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Understanding Your LangSmith Dashboard\n",
    "\n",
    "Now that you've created traces and run evaluations, let's explore what you can see in your dashboard:\n",
    "\n",
    "### üìä Projects View\n",
    "1. **Go to your LangSmith dashboard**: Visit [smith.langchain.com](https://smith.langchain.com)\n",
    "2. **Select your project**: Click on the project you specified in your environment variables\n",
    "\n",
    "### üîç Traces Tab\n",
    "Here you'll see all the traces from the examples above:\n",
    "- **Simple LLM calls** with input/output\n",
    "- **Multi-step research assistant** showing the chain of LLM calls\n",
    "- **Sentiment analysis** with metadata and tags\n",
    "\n",
    "**What to look for:**\n",
    "- ‚è±Ô∏è **Latency**: How long each step took\n",
    "- üí∞ **Cost**: Token usage and estimated costs\n",
    "- üè∑Ô∏è **Tags**: Filter by \"sentiment-analysis\" or \"quickstart\"\n",
    "- üìã **Metadata**: Version and environment information\n",
    "\n",
    "### üß™ Experiments Tab\n",
    "You'll find your evaluation results here:\n",
    "- **Overall accuracy** of your sentiment analyzer\n",
    "- **Individual test results** for each example\n",
    "- **Comparison views** to compare different runs\n",
    "\n",
    "### üìä Monitoring Tab\n",
    "View aggregate metrics:\n",
    "- **Request volume** over time\n",
    "- **Average latency** trends\n",
    "- **Error rates** (hopefully zero!)\n",
    "- **Cost analysis** by model and time period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Framework-Agnostic Example\n",
    "\n",
    "Let's create an example that doesn't use LangChain to demonstrate LangSmith's framework-agnostic capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langsmith import traceable\n",
    "import json\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_huggingface_api(text, model=\"microsoft/DialoGPT-medium\"):\n",
    "    \"\"\"Example using Hugging Face Inference API (free tier)\"\"\"\n",
    "    \n",
    "    # Note: This is a simplified example. In practice, you'd handle authentication properly.\n",
    "    api_url = f\"https://api-inference.huggingface.co/models/{model}\"\n",
    "    \n",
    "    # For this demo, we'll simulate the API call\n",
    "    # In a real scenario, you'd make an actual HTTP request\n",
    "    \n",
    "    # Simulated response\n",
    "    simulated_response = f\"This is a simulated response to: '{text[:50]}...'\"\n",
    "    \n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"input\": text,\n",
    "        \"response\": simulated_response,\n",
    "        \"tokens_used\": len(text.split()) + len(simulated_response.split())\n",
    "    }\n",
    "\n",
    "@traceable(run_type=\"chain\", tags=[\"custom-framework\", \"demo\"])\n",
    "def custom_chatbot(user_message):\n",
    "    \"\"\"A simple chatbot using custom framework integration\"\"\"\n",
    "    \n",
    "    # Step 1: Preprocess the message\n",
    "    processed_message = user_message.strip().lower()\n",
    "    \n",
    "    # Step 2: Generate response using our custom LLM call\n",
    "    llm_response = call_huggingface_api(processed_message)\n",
    "    \n",
    "    # Step 3: Post-process the response\n",
    "    final_response = f\"Bot: {llm_response['response']}\"\n",
    "    \n",
    "    return {\n",
    "        \"user_input\": user_message,\n",
    "        \"processed_input\": processed_message,\n",
    "        \"raw_llm_response\": llm_response,\n",
    "        \"final_response\": final_response\n",
    "    }\n",
    "\n",
    "# Test the custom chatbot\n",
    "test_messages = [\n",
    "    \"Hello, how are you today?\",\n",
    "    \"What's the weather like?\",\n",
    "    \"Can you help me with Python programming?\"\n",
    "]\n",
    "\n",
    "print(\"ü§ñ Custom Framework Chatbot Demo:\\n\")\n",
    "\n",
    "for message in test_messages:\n",
    "    try:\n",
    "        result = custom_chatbot(message)\n",
    "        print(f\"üë§ User: {result['user_input']}\")\n",
    "        print(f\"ü§ñ {result['final_response']}\")\n",
    "        print(f\"üìä Tokens used: {result['raw_llm_response']['tokens_used']}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\\n\")\n",
    "\n",
    "print(\"‚úÖ Custom framework integration complete! Check your traces in LangSmith.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Key Takeaways\n",
    "\n",
    "Congratulations! You've successfully completed your first LangSmith project. Here's what you've accomplished:\n",
    "\n",
    "### ‚úÖ What You've Built\n",
    "1. **Simple traced LLM calls** with automatic observability\n",
    "2. **Multi-step applications** showing complex execution flows\n",
    "3. **Enhanced tracing** with metadata and tags for organization\n",
    "4. **Your first dataset** with real examples\n",
    "5. **Automated evaluation** to test your application quality\n",
    "6. **Framework-agnostic integration** showing flexibility\n",
    "\n",
    "### üîç Key Concepts You've Learned\n",
    "- **Tracing**: Every function call is automatically logged with inputs, outputs, and metadata\n",
    "- **Run Types**: Different types of operations (llm, chain, tool) for better organization\n",
    "- **Metadata & Tags**: Powerful filtering and organization tools\n",
    "- **Datasets**: Collections of examples for testing and evaluation\n",
    "- **Evaluations**: Systematic testing of your application quality\n",
    "\n",
    "### üí° Best Practices You've Applied\n",
    "- **Environment variables** for secure API key management\n",
    "- **Descriptive naming** for functions and metadata\n",
    "- **Error handling** for robust applications\n",
    "- **Incremental complexity** from simple to advanced examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ What's Next?\n",
    "\n",
    "You're now ready to dive deeper into LangSmith's advanced capabilities:\n",
    "\n",
    "### üîç Deep Dive Learning Path:\n",
    "- **LSM-003: Observability Deep Dive** - Master advanced tracing, debugging, and the new Agent Observability features\n",
    "- **LSM-004: Evaluation Mastery** - Build comprehensive testing pipelines with custom evaluators\n",
    "- **LSM-005: Prompt Engineering** - Leverage the Prompt Hub for collaborative prompt development\n",
    "- **LSM-006: Production Monitoring** - Set up enterprise-grade monitoring with OpenTelemetry integration\n",
    "\n",
    "### üõ†Ô∏è Immediate Next Steps:\n",
    "1. **Explore your dashboard** - Spend time clicking through your traces and understanding the interface\n",
    "2. **Experiment with tags** - Try filtering your traces by the tags you've created\n",
    "3. **Create more examples** - Add your own examples to the dataset\n",
    "4. **Invite team members** - If you're working with others, invite them to collaborate\n",
    "\n",
    "### üìö Additional Resources:\n",
    "- [LangSmith Documentation](https://docs.langchain.com/langsmith)\n",
    "- [LangSmith Cookbook](https://github.com/langchain-ai/langsmith-cookbook)\n",
    "- [Community Examples](https://smith.langchain.com/public)\n",
    "\n",
    "---\n",
    "\n",
    "**Ready for advanced tracing and debugging?** Continue to **LSM-003: Observability Deep Dive** to master LangSmith's observability features! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}