{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSM-006: Production Monitoring - Enterprise-Grade Operations\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Implement comprehensive production monitoring for LLM applications\n",
    "- Set up real-time alerts and performance dashboards\n",
    "- Master cost tracking and optimization strategies\n",
    "- Build automated quality monitoring systems\n",
    "- Integrate LangSmith with enterprise monitoring infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè≠ Production Monitoring Overview\n",
    "\n",
    "Production monitoring for LLM applications requires a multi-layered approach:\n",
    "\n",
    "### üìä Key Monitoring Dimensions\n",
    "\n",
    "**Performance Metrics**:\n",
    "- Latency (P50, P95, P99)\n",
    "- Throughput (requests per second)\n",
    "- Error rates and failure patterns\n",
    "- Token usage and processing speed\n",
    "\n",
    "**Quality Metrics**:\n",
    "- Response quality scores\n",
    "- Semantic drift detection\n",
    "- User satisfaction ratings\n",
    "- Safety and compliance violations\n",
    "\n",
    "**Cost Metrics**:\n",
    "- Token consumption per model\n",
    "- Cost per request/user/session\n",
    "- Resource utilization efficiency\n",
    "- Budget threshold alerts\n",
    "\n",
    "**Operational Metrics**:\n",
    "- System availability\n",
    "- Deployment health\n",
    "- Data pipeline status\n",
    "- Infrastructure performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Environment Setup\n",
    "\n",
    "Let's set up our production monitoring environment with comprehensive instrumentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import asyncio\n",
    "from typing import Dict, List, Optional, Any\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import time\n",
    "import random\n",
    "\n",
    "# LangSmith and LangChain imports\n",
    "from langsmith import Client, traceable\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Monitoring and alerting\n",
    "import logging\n",
    "from collections import defaultdict, deque\n",
    "import statistics\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure environment\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"production-monitoring-demo\"\n",
    "\n",
    "# Initialize clients\n",
    "client = Client()\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "print(\"‚úÖ Production monitoring environment configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Real-Time Performance Monitoring\n",
    "\n",
    "Let's build a comprehensive performance monitoring system that tracks key metrics in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlertSeverity(Enum):\n",
    "    INFO = \"info\"\n",
    "    WARNING = \"warning\"\n",
    "    CRITICAL = \"critical\"\n",
    "\n",
    "@dataclass\n",
    "class MetricPoint:\n",
    "    timestamp: datetime\n",
    "    value: float\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "@dataclass\n",
    "class Alert:\n",
    "    severity: AlertSeverity\n",
    "    message: str\n",
    "    timestamp: datetime\n",
    "    metric: str\n",
    "    value: float\n",
    "    threshold: float\n",
    "\n",
    "class ProductionMonitor:\n",
    "    def __init__(self, window_size: int = 100):\n",
    "        self.window_size = window_size\n",
    "        self.metrics = defaultdict(lambda: deque(maxlen=window_size))\n",
    "        self.alerts = deque(maxlen=1000)\n",
    "        self.thresholds = {\n",
    "            'latency_p95': 5.0,  # seconds\n",
    "            'error_rate': 0.05,  # 5%\n",
    "            'token_usage_spike': 2.0,  # 2x normal\n",
    "            'cost_per_hour': 100.0,  # dollars\n",
    "            'quality_score': 0.7  # minimum acceptable\n",
    "        }\n",
    "        self.baseline_metrics = {}\n",
    "        self._lock = threading.Lock()\n",
    "    \n",
    "    def record_metric(self, metric_name: str, value: float, metadata: Dict = None):\n",
    "        \"\"\"Record a metric point with timestamp and metadata\"\"\"\n",
    "        with self._lock:\n",
    "            point = MetricPoint(\n",
    "                timestamp=datetime.now(),\n",
    "                value=value,\n",
    "                metadata=metadata or {}\n",
    "            )\n",
    "            self.metrics[metric_name].append(point)\n",
    "            \n",
    "            # Check for alerts\n",
    "            self._check_alerts(metric_name, value)\n",
    "    \n",
    "    def _check_alerts(self, metric_name: str, value: float):\n",
    "        \"\"\"Check if metric value triggers any alerts\"\"\"\n",
    "        threshold = self.thresholds.get(metric_name)\n",
    "        if not threshold:\n",
    "            return\n",
    "        \n",
    "        severity = None\n",
    "        message = None\n",
    "        \n",
    "        if metric_name == 'latency_p95' and value > threshold:\n",
    "            severity = AlertSeverity.WARNING if value < threshold * 1.5 else AlertSeverity.CRITICAL\n",
    "            message = f\"High latency detected: {value:.2f}s (threshold: {threshold}s)\"\n",
    "        \n",
    "        elif metric_name == 'error_rate' and value > threshold:\n",
    "            severity = AlertSeverity.WARNING if value < threshold * 2 else AlertSeverity.CRITICAL\n",
    "            message = f\"High error rate: {value:.1%} (threshold: {threshold:.1%})\"\n",
    "        \n",
    "        elif metric_name == 'quality_score' and value < threshold:\n",
    "            severity = AlertSeverity.WARNING if value > threshold * 0.8 else AlertSeverity.CRITICAL\n",
    "            message = f\"Low quality score: {value:.2f} (threshold: {threshold})\"\n",
    "        \n",
    "        if severity and message:\n",
    "            alert = Alert(\n",
    "                severity=severity,\n",
    "                message=message,\n",
    "                timestamp=datetime.now(),\n",
    "                metric=metric_name,\n",
    "                value=value,\n",
    "                threshold=threshold\n",
    "            )\n",
    "            self.alerts.append(alert)\n",
    "            print(f\"üö® {severity.value.upper()}: {message}\")\n",
    "    \n",
    "    def get_statistics(self, metric_name: str) -> Dict[str, float]:\n",
    "        \"\"\"Get statistical summary of a metric\"\"\"\n",
    "        with self._lock:\n",
    "            points = list(self.metrics[metric_name])\n",
    "            \n",
    "        if not points:\n",
    "            return {}\n",
    "        \n",
    "        values = [p.value for p in points]\n",
    "        \n",
    "        return {\n",
    "            'count': len(values),\n",
    "            'mean': statistics.mean(values),\n",
    "            'median': statistics.median(values),\n",
    "            'std': statistics.stdev(values) if len(values) > 1 else 0,\n",
    "            'min': min(values),\n",
    "            'max': max(values),\n",
    "            'p95': self._percentile(values, 0.95),\n",
    "            'p99': self._percentile(values, 0.99)\n",
    "        }\n",
    "    \n",
    "    def _percentile(self, values: List[float], p: float) -> float:\n",
    "        \"\"\"Calculate percentile of values\"\"\"\n",
    "        sorted_values = sorted(values)\n",
    "        index = int(p * len(sorted_values))\n",
    "        return sorted_values[min(index, len(sorted_values) - 1)]\n",
    "    \n",
    "    def get_recent_alerts(self, hours: int = 24) -> List[Alert]:\n",
    "        \"\"\"Get recent alerts within specified hours\"\"\"\n",
    "        cutoff = datetime.now() - timedelta(hours=hours)\n",
    "        return [alert for alert in self.alerts if alert.timestamp > cutoff]\n",
    "\n",
    "# Initialize production monitor\n",
    "monitor = ProductionMonitor()\n",
    "print(\"üìä Production monitor initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Instrumented Application\n",
    "\n",
    "Let's create a production-ready application with comprehensive instrumentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionLLMService:\n",
    "    def __init__(self, monitor: ProductionMonitor):\n",
    "        self.monitor = monitor\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "        self.request_count = 0\n",
    "        self.error_count = 0\n",
    "        \n",
    "        # Quality evaluation prompt\n",
    "        self.quality_evaluator = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "        \n",
    "    @traceable(name=\"production_customer_service\")\n",
    "    def handle_customer_inquiry(self, user_message: str, context: Dict = None) -> Dict[str, Any]:\n",
    "        \"\"\"Handle customer service inquiry with full monitoring\"\"\"\n",
    "        start_time = time.time()\n",
    "        self.request_count += 1\n",
    "        \n",
    "        try:\n",
    "            # Create system prompt\n",
    "            system_prompt = \"\"\"\n",
    "            You are a helpful customer service representative. \n",
    "            Provide accurate, empathetic, and professional responses.\n",
    "            If you cannot help with something, clearly explain limitations.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Generate response\n",
    "            messages = [\n",
    "                SystemMessage(content=system_prompt),\n",
    "                HumanMessage(content=user_message)\n",
    "            ]\n",
    "            \n",
    "            response = self.llm.invoke(messages)\n",
    "            response_text = response.content\n",
    "            \n",
    "            # Calculate metrics\n",
    "            latency = time.time() - start_time\n",
    "            token_count = len(user_message.split()) + len(response_text.split())\n",
    "            \n",
    "            # Estimate cost (approximate)\n",
    "            cost = self._estimate_cost(token_count, \"gpt-4o-mini\")\n",
    "            \n",
    "            # Record performance metrics\n",
    "            self.monitor.record_metric('latency', latency, {\n",
    "                'model': 'gpt-4o-mini',\n",
    "                'tokens': token_count\n",
    "            })\n",
    "            \n",
    "            self.monitor.record_metric('token_usage', token_count, {\n",
    "                'model': 'gpt-4o-mini',\n",
    "                'request_type': 'customer_service'\n",
    "            })\n",
    "            \n",
    "            self.monitor.record_metric('cost', cost, {\n",
    "                'model': 'gpt-4o-mini'\n",
    "            })\n",
    "            \n",
    "            # Evaluate quality asynchronously\n",
    "            quality_score = self._evaluate_response_quality(user_message, response_text)\n",
    "            self.monitor.record_metric('quality_score', quality_score)\n",
    "            \n",
    "            # Calculate and record error rate\n",
    "            error_rate = self.error_count / self.request_count\n",
    "            self.monitor.record_metric('error_rate', error_rate)\n",
    "            \n",
    "            return {\n",
    "                'response': response_text,\n",
    "                'metrics': {\n",
    "                    'latency': latency,\n",
    "                    'tokens': token_count,\n",
    "                    'cost': cost,\n",
    "                    'quality_score': quality_score\n",
    "                },\n",
    "                'status': 'success'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.error_count += 1\n",
    "            error_rate = self.error_count / self.request_count\n",
    "            self.monitor.record_metric('error_rate', error_rate)\n",
    "            \n",
    "            return {\n",
    "                'error': str(e),\n",
    "                'status': 'error',\n",
    "                'metrics': {\n",
    "                    'latency': time.time() - start_time\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    def _estimate_cost(self, token_count: int, model: str) -> float:\n",
    "        \"\"\"Estimate cost based on token usage\"\"\"\n",
    "        # Approximate pricing (as of 2025)\n",
    "        rates = {\n",
    "            'gpt-4o-mini': {'input': 0.00015, 'output': 0.0006},  # per 1K tokens\n",
    "            'gpt-4o': {'input': 0.005, 'output': 0.015}\n",
    "        }\n",
    "        \n",
    "        if model not in rates:\n",
    "            return 0.0\n",
    "        \n",
    "        # Assume 60% input, 40% output tokens\n",
    "        input_tokens = int(token_count * 0.6)\n",
    "        output_tokens = int(token_count * 0.4)\n",
    "        \n",
    "        cost = (\n",
    "            (input_tokens / 1000) * rates[model]['input'] +\n",
    "            (output_tokens / 1000) * rates[model]['output']\n",
    "        )\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def _evaluate_response_quality(self, question: str, response: str) -> float:\n",
    "        \"\"\"Evaluate response quality using LLM-as-judge\"\"\"\n",
    "        try:\n",
    "            eval_prompt = f\"\"\"\n",
    "            Evaluate the quality of this customer service response on a scale of 0-1:\n",
    "            \n",
    "            Customer Question: {question}\n",
    "            Response: {response}\n",
    "            \n",
    "            Consider:\n",
    "            - Relevance and accuracy\n",
    "            - Helpfulness and completeness\n",
    "            - Professional tone\n",
    "            - Clarity and coherence\n",
    "            \n",
    "            Respond with only a number between 0 and 1.\n",
    "            \"\"\"\n",
    "            \n",
    "            result = self.quality_evaluator.invoke([HumanMessage(content=eval_prompt)])\n",
    "            score = float(result.content.strip())\n",
    "            return max(0, min(1, score))  # Clamp to [0,1]\n",
    "            \n",
    "        except:\n",
    "            return 0.5  # Default neutral score if evaluation fails\n",
    "\n",
    "# Initialize production service\n",
    "service = ProductionLLMService(monitor)\n",
    "print(\"üè≠ Production LLM service initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Load Testing and Monitoring\n",
    "\n",
    "Let's simulate production load to see our monitoring system in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample customer inquiries for load testing\n",
    "sample_inquiries = [\n",
    "    \"I need help with my order #12345. It hasn't arrived yet.\",\n",
    "    \"Can you explain your return policy?\",\n",
    "    \"I was charged twice for the same item. Please help.\",\n",
    "    \"How do I change my shipping address?\",\n",
    "    \"Is this product compatible with iPhone 15?\",\n",
    "    \"I want to cancel my subscription.\",\n",
    "    \"The product I received is damaged. What should I do?\",\n",
    "    \"Can I get a refund for this item?\",\n",
    "    \"When will this product be back in stock?\",\n",
    "    \"I forgot my password. How do I reset it?\"\n",
    "]\n",
    "\n",
    "async def simulate_load_test(num_requests: int = 20):\n",
    "    \"\"\"Simulate production load with concurrent requests\"\"\"\n",
    "    print(f\"üöÄ Starting load test with {num_requests} requests...\")\n",
    "    \n",
    "    async def make_request(request_id: int):\n",
    "        # Random delay to simulate real traffic patterns\n",
    "        await asyncio.sleep(random.uniform(0, 2))\n",
    "        \n",
    "        inquiry = random.choice(sample_inquiries)\n",
    "        \n",
    "        # Simulate occasional errors (5% failure rate)\n",
    "        if random.random() < 0.05:\n",
    "            # Simulate timeout or API error\n",
    "            time.sleep(random.uniform(8, 12))  # Long delay\n",
    "            raise Exception(\"API timeout\")\n",
    "        \n",
    "        result = service.handle_customer_inquiry(inquiry)\n",
    "        print(f\"‚úÖ Request {request_id}: {result['status']} (latency: {result['metrics']['latency']:.2f}s)\")\n",
    "        return result\n",
    "    \n",
    "    # Execute requests concurrently\n",
    "    tasks = [make_request(i) for i in range(num_requests)]\n",
    "    \n",
    "    try:\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        # Count successes and failures\n",
    "        successes = sum(1 for r in results if isinstance(r, dict) and r.get('status') == 'success')\n",
    "        failures = len(results) - successes\n",
    "        \n",
    "        print(f\"\\nüìä Load test completed: {successes} successes, {failures} failures\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Load test error: {e}\")\n",
    "\n",
    "# Run load test\n",
    "await simulate_load_test(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Dashboard and Analytics\n",
    "\n",
    "Let's create a comprehensive dashboard to visualize our production metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_production_dashboard():\n",
    "    \"\"\"Display comprehensive production dashboard\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üè≠ PRODUCTION DASHBOARD\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Performance metrics\n",
    "    print(\"\\nüìà PERFORMANCE METRICS\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    latency_stats = monitor.get_statistics('latency')\n",
    "    if latency_stats:\n",
    "        print(f\"Latency (seconds):\")\n",
    "        print(f\"  Mean: {latency_stats['mean']:.3f}s\")\n",
    "        print(f\"  P95:  {latency_stats['p95']:.3f}s\")\n",
    "        print(f\"  P99:  {latency_stats['p99']:.3f}s\")\n",
    "        print(f\"  Max:  {latency_stats['max']:.3f}s\")\n",
    "    \n",
    "    error_stats = monitor.get_statistics('error_rate')\n",
    "    if error_stats:\n",
    "        current_error_rate = error_stats['mean']\n",
    "        print(f\"\\nError Rate: {current_error_rate:.1%}\")\n",
    "        if current_error_rate > 0.05:\n",
    "            print(\"  ‚ö†Ô∏è Above threshold (5%)\")\n",
    "        else:\n",
    "            print(\"  ‚úÖ Within acceptable range\")\n",
    "    \n",
    "    # Quality metrics\n",
    "    print(\"\\nüéØ QUALITY METRICS\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    quality_stats = monitor.get_statistics('quality_score')\n",
    "    if quality_stats:\n",
    "        print(f\"Quality Score:\")\n",
    "        print(f\"  Mean: {quality_stats['mean']:.3f}\")\n",
    "        print(f\"  Min:  {quality_stats['min']:.3f}\")\n",
    "        print(f\"  Max:  {quality_stats['max']:.3f}\")\n",
    "        \n",
    "        if quality_stats['mean'] < 0.7:\n",
    "            print(\"  ‚ö†Ô∏è Below target (0.7)\")\n",
    "        else:\n",
    "            print(\"  ‚úÖ Meeting quality targets\")\n",
    "    \n",
    "    # Cost metrics\n",
    "    print(\"\\nüí∞ COST METRICS\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    cost_stats = monitor.get_statistics('cost')\n",
    "    token_stats = monitor.get_statistics('token_usage')\n",
    "    \n",
    "    if cost_stats:\n",
    "        total_cost = sum(point.value for point in monitor.metrics['cost'])\n",
    "        avg_cost_per_request = cost_stats['mean']\n",
    "        print(f\"Total Cost: ${total_cost:.4f}\")\n",
    "        print(f\"Average Cost/Request: ${avg_cost_per_request:.4f}\")\n",
    "    \n",
    "    if token_stats:\n",
    "        total_tokens = sum(point.value for point in monitor.metrics['token_usage'])\n",
    "        print(f\"Total Tokens: {total_tokens:,}\")\n",
    "        print(f\"Average Tokens/Request: {token_stats['mean']:.0f}\")\n",
    "    \n",
    "    # Recent alerts\n",
    "    print(\"\\nüö® RECENT ALERTS (Last 24 hours)\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    recent_alerts = monitor.get_recent_alerts(24)\n",
    "    if recent_alerts:\n",
    "        for alert in recent_alerts[-5:]:  # Show last 5 alerts\n",
    "            severity_icon = {\n",
    "                AlertSeverity.INFO: \"‚ÑπÔ∏è\",\n",
    "                AlertSeverity.WARNING: \"‚ö†Ô∏è\",\n",
    "                AlertSeverity.CRITICAL: \"üö®\"\n",
    "            }[alert.severity]\n",
    "            \n",
    "            print(f\"{severity_icon} {alert.timestamp.strftime('%H:%M:%S')}: {alert.message}\")\n",
    "    else:\n",
    "        print(\"‚úÖ No alerts in the last 24 hours\")\n",
    "    \n",
    "    # System health summary\n",
    "    print(\"\\nüè• SYSTEM HEALTH SUMMARY\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    health_score = calculate_health_score()\n",
    "    health_icon = \"üü¢\" if health_score > 0.8 else \"üü°\" if health_score > 0.6 else \"üî¥\"\n",
    "    print(f\"Overall Health: {health_icon} {health_score:.1%}\")\n",
    "    \n",
    "    if health_score < 0.8:\n",
    "        print(\"\\nüîß RECOMMENDED ACTIONS:\")\n",
    "        if latency_stats and latency_stats['p95'] > 3:\n",
    "            print(\"  ‚Ä¢ Investigate high latency issues\")\n",
    "        if error_stats and error_stats['mean'] > 0.05:\n",
    "            print(\"  ‚Ä¢ Review and fix error patterns\")\n",
    "        if quality_stats and quality_stats['mean'] < 0.7:\n",
    "            print(\"  ‚Ä¢ Improve prompt engineering and model selection\")\n",
    "\n",
    "def calculate_health_score() -> float:\n",
    "    \"\"\"Calculate overall system health score\"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    # Latency score (0-1, where 1 is best)\n",
    "    latency_stats = monitor.get_statistics('latency')\n",
    "    if latency_stats:\n",
    "        latency_score = max(0, min(1, 1 - (latency_stats['p95'] - 1) / 4))  # 1s = 1.0, 5s = 0.0\n",
    "        scores.append(latency_score)\n",
    "    \n",
    "    # Error rate score\n",
    "    error_stats = monitor.get_statistics('error_rate')\n",
    "    if error_stats:\n",
    "        error_score = max(0, min(1, 1 - error_stats['mean'] / 0.1))  # 0% = 1.0, 10% = 0.0\n",
    "        scores.append(error_score)\n",
    "    \n",
    "    # Quality score\n",
    "    quality_stats = monitor.get_statistics('quality_score')\n",
    "    if quality_stats:\n",
    "        scores.append(quality_stats['mean'])\n",
    "    \n",
    "    return statistics.mean(scores) if scores else 0.5\n",
    "\n",
    "# Display dashboard\n",
    "display_production_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîî Advanced Alerting System\n",
    "\n",
    "Let's implement a sophisticated alerting system with different notification channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Callable\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "import requests\n",
    "\n",
    "class AlertChannel(ABC):\n",
    "    \"\"\"Abstract base class for alert notification channels\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def send_alert(self, alert: Alert) -> bool:\n",
    "        \"\"\"Send alert notification\"\"\"\n",
    "        pass\n",
    "\n",
    "class ConsoleAlertChannel(AlertChannel):\n",
    "    \"\"\"Console-based alert notifications\"\"\"\n",
    "    \n",
    "    def send_alert(self, alert: Alert) -> bool:\n",
    "        severity_icons = {\n",
    "            AlertSeverity.INFO: \"‚ÑπÔ∏è\",\n",
    "            AlertSeverity.WARNING: \"‚ö†Ô∏è\",\n",
    "            AlertSeverity.CRITICAL: \"üö®\"\n",
    "        }\n",
    "        \n",
    "        icon = severity_icons.get(alert.severity, \"‚ùì\")\n",
    "        print(f\"\\n{icon} ALERT [{alert.severity.value.upper()}] - {alert.timestamp.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"   Metric: {alert.metric}\")\n",
    "        print(f\"   Message: {alert.message}\")\n",
    "        print(f\"   Value: {alert.value} (Threshold: {alert.threshold})\")\n",
    "        return True\n",
    "\n",
    "class SlackAlertChannel(AlertChannel):\n",
    "    \"\"\"Slack webhook alert notifications\"\"\"\n",
    "    \n",
    "    def __init__(self, webhook_url: str):\n",
    "        self.webhook_url = webhook_url\n",
    "    \n",
    "    def send_alert(self, alert: Alert) -> bool:\n",
    "        \"\"\"Send alert to Slack (simulated)\"\"\"\n",
    "        # In real implementation, you would send to actual Slack webhook\n",
    "        color = {\n",
    "            AlertSeverity.INFO: \"good\",\n",
    "            AlertSeverity.WARNING: \"warning\", \n",
    "            AlertSeverity.CRITICAL: \"danger\"\n",
    "        }[alert.severity]\n",
    "        \n",
    "        payload = {\n",
    "            \"attachments\": [{\n",
    "                \"color\": color,\n",
    "                \"title\": f\"Production Alert - {alert.severity.value.title()}\",\n",
    "                \"fields\": [\n",
    "                    {\"title\": \"Metric\", \"value\": alert.metric, \"short\": True},\n",
    "                    {\"title\": \"Value\", \"value\": f\"{alert.value}\", \"short\": True},\n",
    "                    {\"title\": \"Threshold\", \"value\": f\"{alert.threshold}\", \"short\": True},\n",
    "                    {\"title\": \"Message\", \"value\": alert.message, \"short\": False}\n",
    "                ],\n",
    "                \"timestamp\": alert.timestamp.timestamp()\n",
    "            }]\n",
    "        }\n",
    "        \n",
    "        print(f\"üì± [SIMULATED] Slack alert sent: {alert.message}\")\n",
    "        # requests.post(self.webhook_url, json=payload)\n",
    "        return True\n",
    "\n",
    "class EmailAlertChannel(AlertChannel):\n",
    "    \"\"\"Email alert notifications\"\"\"\n",
    "    \n",
    "    def __init__(self, smtp_host: str, smtp_port: int, username: str, password: str, recipients: List[str]):\n",
    "        self.smtp_host = smtp_host\n",
    "        self.smtp_port = smtp_port\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.recipients = recipients\n",
    "    \n",
    "    def send_alert(self, alert: Alert) -> bool:\n",
    "        \"\"\"Send alert via email (simulated)\"\"\"\n",
    "        subject = f\"Production Alert [{alert.severity.value.upper()}] - {alert.metric}\"\n",
    "        \n",
    "        body = f\"\"\"\n",
    "Production Alert Details:\n",
    "\n",
    "Severity: {alert.severity.value.upper()}\n",
    "Metric: {alert.metric}\n",
    "Value: {alert.value}\n",
    "Threshold: {alert.threshold}\n",
    "Timestamp: {alert.timestamp}\n",
    "\n",
    "Message: {alert.message}\n",
    "\n",
    "Please investigate and take appropriate action.\n",
    "\n",
    "Best regards,\n",
    "Production Monitoring System\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"üìß [SIMULATED] Email alert sent to {len(self.recipients)} recipients\")\n",
    "        print(f\"   Subject: {subject}\")\n",
    "        # In real implementation:\n",
    "        # msg = MIMEText(body)\n",
    "        # msg['Subject'] = subject\n",
    "        # msg['From'] = self.username\n",
    "        # msg['To'] = ', '.join(self.recipients)\n",
    "        # \n",
    "        # with smtplib.SMTP(self.smtp_host, self.smtp_port) as server:\n",
    "        #     server.starttls()\n",
    "        #     server.login(self.username, self.password)\n",
    "        #     server.send_message(msg)\n",
    "        \n",
    "        return True\n",
    "\n",
    "class AlertManager:\n",
    "    \"\"\"Manages alert routing and notification channels\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.channels: List[AlertChannel] = []\n",
    "        self.rules: List[Callable[[Alert], bool]] = []\n",
    "        self.alert_history = deque(maxlen=1000)\n",
    "        self.suppression_rules = {}\n",
    "    \n",
    "    def add_channel(self, channel: AlertChannel):\n",
    "        \"\"\"Add alert notification channel\"\"\"\n",
    "        self.channels.append(channel)\n",
    "    \n",
    "    def add_suppression_rule(self, metric: str, min_interval_minutes: int):\n",
    "        \"\"\"Add rule to suppress duplicate alerts\"\"\"\n",
    "        self.suppression_rules[metric] = min_interval_minutes\n",
    "    \n",
    "    def should_suppress_alert(self, alert: Alert) -> bool:\n",
    "        \"\"\"Check if alert should be suppressed due to recent similar alerts\"\"\"\n",
    "        if alert.metric not in self.suppression_rules:\n",
    "            return False\n",
    "        \n",
    "        min_interval = timedelta(minutes=self.suppression_rules[alert.metric])\n",
    "        cutoff_time = alert.timestamp - min_interval\n",
    "        \n",
    "        # Check for recent similar alerts\n",
    "        for recent_alert in reversed(list(self.alert_history)):\n",
    "            if recent_alert.timestamp < cutoff_time:\n",
    "                break\n",
    "            \n",
    "            if (recent_alert.metric == alert.metric and \n",
    "                recent_alert.severity == alert.severity):\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def send_alert(self, alert: Alert):\n",
    "        \"\"\"Process and send alert through appropriate channels\"\"\"\n",
    "        # Check suppression rules\n",
    "        if self.should_suppress_alert(alert):\n",
    "            print(f\"üîá Alert suppressed: {alert.message}\")\n",
    "            return\n",
    "        \n",
    "        # Add to history\n",
    "        self.alert_history.append(alert)\n",
    "        \n",
    "        # Send through all channels based on severity\n",
    "        for channel in self.channels:\n",
    "            try:\n",
    "                if self._should_send_to_channel(alert, channel):\n",
    "                    channel.send_alert(alert)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to send alert via {type(channel).__name__}: {e}\")\n",
    "    \n",
    "    def _should_send_to_channel(self, alert: Alert, channel: AlertChannel) -> bool:\n",
    "        \"\"\"Determine if alert should be sent to specific channel\"\"\"\n",
    "        # Send all alerts to console for demo\n",
    "        if isinstance(channel, ConsoleAlertChannel):\n",
    "            return True\n",
    "        \n",
    "        # Send WARNING and CRITICAL to Slack\n",
    "        if isinstance(channel, SlackAlertChannel):\n",
    "            return alert.severity in [AlertSeverity.WARNING, AlertSeverity.CRITICAL]\n",
    "        \n",
    "        # Send only CRITICAL to email\n",
    "        if isinstance(channel, EmailAlertChannel):\n",
    "            return alert.severity == AlertSeverity.CRITICAL\n",
    "        \n",
    "        return True\n",
    "\n",
    "# Set up alert manager with multiple channels\n",
    "alert_manager = AlertManager()\n",
    "alert_manager.add_channel(ConsoleAlertChannel())\n",
    "alert_manager.add_channel(SlackAlertChannel(\"https://hooks.slack.com/services/fake/webhook\"))\n",
    "alert_manager.add_channel(EmailAlertChannel(\n",
    "    smtp_host=\"smtp.company.com\",\n",
    "    smtp_port=587,\n",
    "    username=\"alerts@company.com\",\n",
    "    password=\"password\",\n",
    "    recipients=[\"devops@company.com\", \"team-lead@company.com\"]\n",
    "))\n",
    "\n",
    "# Add suppression rules\n",
    "alert_manager.add_suppression_rule('latency_p95', 15)  # Max 1 alert per 15 minutes\n",
    "alert_manager.add_suppression_rule('error_rate', 10)   # Max 1 alert per 10 minutes\n",
    "\n",
    "print(\"üîî Advanced alerting system configured\")\n",
    "\n",
    "# Test alerting system with sample alerts\n",
    "test_alerts = [\n",
    "    Alert(AlertSeverity.WARNING, \"Test warning alert\", datetime.now(), \"test_metric\", 0.8, 0.7),\n",
    "    Alert(AlertSeverity.CRITICAL, \"Test critical alert\", datetime.now(), \"test_metric_critical\", 0.9, 0.5)\n",
    "]\n",
    "\n",
    "print(\"\\nüß™ Testing alert system...\")\n",
    "for alert in test_alerts:\n",
    "    alert_manager.send_alert(alert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Integration with Enterprise Infrastructure\n",
    "\n",
    "Let's explore how to integrate LangSmith with existing enterprise monitoring and observability tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration patterns for enterprise infrastructure\n",
    "\n",
    "class OpenTelemetryIntegration:\n",
    "    \"\"\"Integration with OpenTelemetry for distributed tracing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # In real implementation, you would configure OpenTelemetry\n",
    "        # from opentelemetry import trace, metrics\n",
    "        # from opentelemetry.exporter.jaeger.thrift import JaegerExporter\n",
    "        # from opentelemetry.sdk.trace import TracerProvider\n",
    "        # from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "        pass\n",
    "    \n",
    "    def configure_tracing(self):\n",
    "        \"\"\"Configure OpenTelemetry tracing\"\"\"\n",
    "        print(\"üîß Configuring OpenTelemetry tracing...\")\n",
    "        \n",
    "        # Example configuration\n",
    "        config_example = \"\"\"\n",
    "        # OpenTelemetry Configuration Example\n",
    "        \n",
    "        import os\n",
    "        from opentelemetry import trace\n",
    "        from opentelemetry.exporter.jaeger.thrift import JaegerExporter\n",
    "        from opentelemetry.sdk.trace import TracerProvider\n",
    "        from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "        from opentelemetry.instrumentation.langchain import LangChainInstrumentor\n",
    "        \n",
    "        # Configure tracer\n",
    "        trace.set_tracer_provider(TracerProvider())\n",
    "        tracer = trace.get_tracer(__name__)\n",
    "        \n",
    "        # Configure Jaeger exporter\n",
    "        jaeger_exporter = JaegerExporter(\n",
    "            agent_host_name=\"jaeger-agent\",\n",
    "            agent_port=6831,\n",
    "        )\n",
    "        \n",
    "        # Add span processor\n",
    "        span_processor = BatchSpanProcessor(jaeger_exporter)\n",
    "        trace.get_tracer_provider().add_span_processor(span_processor)\n",
    "        \n",
    "        # Auto-instrument LangChain\n",
    "        LangChainInstrumentor().instrument()\n",
    "        \n",
    "        # Configure environment variables\n",
    "        os.environ[\"LANGSMITH_TRACING_V2\"] = \"true\"\n",
    "        os.environ[\"LANGCHAIN_PROJECT\"] = \"production-app\"\n",
    "        \"\"\"\n",
    "        \n",
    "        print(config_example)\n",
    "        return config_example\n",
    "\n",
    "class PrometheusIntegration:\n",
    "    \"\"\"Integration with Prometheus metrics collection\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics_registry = {}\n",
    "    \n",
    "    def setup_metrics_export(self):\n",
    "        \"\"\"Setup Prometheus metrics export\"\"\"\n",
    "        print(\"üìä Setting up Prometheus metrics export...\")\n",
    "        \n",
    "        prometheus_config = \"\"\"\n",
    "        # Prometheus Integration Configuration\n",
    "        \n",
    "        from prometheus_client import Counter, Histogram, Gauge, start_http_server\n",
    "        import time\n",
    "        \n",
    "        # Define metrics\n",
    "        request_count = Counter('llm_requests_total', \n",
    "                               'Total LLM requests', \n",
    "                               ['model', 'status'])\n",
    "        \n",
    "        request_duration = Histogram('llm_request_duration_seconds',\n",
    "                                   'LLM request duration',\n",
    "                                   ['model'])\n",
    "        \n",
    "        token_usage = Counter('llm_tokens_total',\n",
    "                            'Total tokens processed',\n",
    "                            ['model', 'type'])\n",
    "        \n",
    "        quality_score = Gauge('llm_quality_score',\n",
    "                            'Current quality score',\n",
    "                            ['model'])\n",
    "        \n",
    "        # Start metrics server\n",
    "        start_http_server(8000)\n",
    "        \n",
    "        # Example usage in your LLM application:\n",
    "        @traceable(name=\"monitored_llm_call\")\n",
    "        def make_llm_call(prompt, model=\"gpt-4o-mini\"):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Your LLM call logic here\n",
    "                response = llm.invoke(prompt)\n",
    "                \n",
    "                # Record metrics\n",
    "                request_count.labels(model=model, status='success').inc()\n",
    "                request_duration.labels(model=model).observe(time.time() - start_time)\n",
    "                token_usage.labels(model=model, type='input').inc(len(prompt.split()))\n",
    "                token_usage.labels(model=model, type='output').inc(len(response.split()))\n",
    "                \n",
    "                return response\n",
    "                \n",
    "            except Exception as e:\n",
    "                request_count.labels(model=model, status='error').inc()\n",
    "                raise\n",
    "        \"\"\"\n",
    "        \n",
    "        print(prometheus_config)\n",
    "        return prometheus_config\n",
    "\n",
    "class DatadogIntegration:\n",
    "    \"\"\"Integration with Datadog APM and logging\"\"\"\n",
    "    \n",
    "    def setup_datadog_integration(self):\n",
    "        \"\"\"Setup Datadog integration\"\"\"\n",
    "        print(\"üêï Setting up Datadog integration...\")\n",
    "        \n",
    "        datadog_config = \"\"\"\n",
    "        # Datadog Integration Configuration\n",
    "        \n",
    "        from datadog import initialize, statsd\n",
    "        import logging\n",
    "        \n",
    "        # Initialize Datadog\n",
    "        options = {\n",
    "            'api_key': os.getenv('DATADOG_API_KEY'),\n",
    "            'app_key': os.getenv('DATADOG_APP_KEY')\n",
    "        }\n",
    "        initialize(**options)\n",
    "        \n",
    "        # Configure logging\n",
    "        logging.basicConfig(\n",
    "            format='%(asctime)s %(levelname)s %(name)s %(message)s',\n",
    "            level=logging.INFO\n",
    "        )\n",
    "        \n",
    "        # Custom metrics function\n",
    "        def send_metrics(metric_name, value, tags=None):\n",
    "            statsd.gauge(f'langsmith.{metric_name}', value, tags=tags or [])\n",
    "        \n",
    "        # Example usage:\n",
    "        @traceable(name=\"datadog_monitored_call\")\n",
    "        def monitored_llm_call(prompt):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                response = llm.invoke(prompt)\n",
    "                \n",
    "                # Send custom metrics to Datadog\n",
    "                duration = time.time() - start_time\n",
    "                send_metrics('llm.duration', duration, ['model:gpt-4o-mini'])\n",
    "                send_metrics('llm.tokens', len(prompt.split()), ['type:input'])\n",
    "                send_metrics('llm.requests', 1, ['status:success'])\n",
    "                \n",
    "                # Log structured data\n",
    "                logging.info('LLM call completed', extra={\n",
    "                    'duration': duration,\n",
    "                    'model': 'gpt-4o-mini',\n",
    "                    'tokens': len(prompt.split()),\n",
    "                    'dd.trace_id': get_trace_id(),  # Datadog trace correlation\n",
    "                    'dd.span_id': get_span_id()\n",
    "                })\n",
    "                \n",
    "                return response\n",
    "                \n",
    "            except Exception as e:\n",
    "                send_metrics('llm.requests', 1, ['status:error'])\n",
    "                logging.error(f'LLM call failed: {e}')\n",
    "                raise\n",
    "        \"\"\"\n",
    "        \n",
    "        print(datadog_config)\n",
    "        return datadog_config\n",
    "\n",
    "# Demonstrate integrations\n",
    "print(\"üè¢ Enterprise Infrastructure Integrations\\n\")\n",
    "\n",
    "# OpenTelemetry\n",
    "otel = OpenTelemetryIntegration()\n",
    "otel.configure_tracing()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Prometheus\n",
    "prometheus = PrometheusIntegration()\n",
    "prometheus.setup_metrics_export()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Datadog\n",
    "datadog = DatadogIntegration()\n",
    "datadog.setup_datadog_integration()\n",
    "\n",
    "print(\"\\n‚úÖ All enterprise integrations configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí∞ Cost Optimization and Budget Management\n",
    "\n",
    "Let's implement advanced cost tracking and optimization strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "from typing import Dict, List, Tuple\n",
    "import calendar\n",
    "\n",
    "class CostManager:\n",
    "    \"\"\"Advanced cost tracking and optimization\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cost_history = deque(maxlen=10000)\n",
    "        self.model_pricing = {\n",
    "            'gpt-4o': {'input': 0.005, 'output': 0.015},\n",
    "            'gpt-4o-mini': {'input': 0.00015, 'output': 0.0006},\n",
    "            'claude-3-sonnet': {'input': 0.003, 'output': 0.015},\n",
    "            'claude-3-haiku': {'input': 0.00025, 'output': 0.00125}\n",
    "        }\n",
    "        self.budgets = {}\n",
    "        self.cost_alerts = []\n",
    "    \n",
    "    def set_budget(self, period: str, amount: float, alert_threshold: float = 0.8):\n",
    "        \"\"\"Set budget for a time period (daily, weekly, monthly)\"\"\"\n",
    "        self.budgets[period] = {\n",
    "            'amount': amount,\n",
    "            'alert_threshold': alert_threshold,\n",
    "            'start_date': date.today()\n",
    "        }\n",
    "    \n",
    "    def track_cost(self, model: str, input_tokens: int, output_tokens: int, \n",
    "                   user_id: str = None, project: str = None):\n",
    "        \"\"\"Track cost for a specific model usage\"\"\"\n",
    "        if model not in self.model_pricing:\n",
    "            return 0\n",
    "        \n",
    "        pricing = self.model_pricing[model]\n",
    "        cost = (\n",
    "            (input_tokens / 1000) * pricing['input'] +\n",
    "            (output_tokens / 1000) * pricing['output']\n",
    "        )\n",
    "        \n",
    "        cost_record = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'model': model,\n",
    "            'input_tokens': input_tokens,\n",
    "            'output_tokens': output_tokens,\n",
    "            'cost': cost,\n",
    "            'user_id': user_id,\n",
    "            'project': project\n",
    "        }\n",
    "        \n",
    "        self.cost_history.append(cost_record)\n",
    "        self._check_budget_alerts()\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def _check_budget_alerts(self):\n",
    "        \"\"\"Check if current spending exceeds budget thresholds\"\"\"\n",
    "        today = date.today()\n",
    "        \n",
    "        for period, budget_info in self.budgets.items():\n",
    "            if period == 'daily':\n",
    "                period_cost = self.get_daily_cost(today)\n",
    "            elif period == 'weekly':\n",
    "                period_cost = self.get_weekly_cost(today)\n",
    "            elif period == 'monthly':\n",
    "                period_cost = self.get_monthly_cost(today.year, today.month)\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            threshold_amount = budget_info['amount'] * budget_info['alert_threshold']\n",
    "            \n",
    "            if period_cost > threshold_amount:\n",
    "                alert = {\n",
    "                    'timestamp': datetime.now(),\n",
    "                    'period': period,\n",
    "                    'current_cost': period_cost,\n",
    "                    'budget': budget_info['amount'],\n",
    "                    'threshold': threshold_amount,\n",
    "                    'percentage': (period_cost / budget_info['amount']) * 100\n",
    "                }\n",
    "                \n",
    "                if period_cost > budget_info['amount']:\n",
    "                    print(f\"üö® BUDGET EXCEEDED: {period} spending (${period_cost:.2f}) exceeded budget (${budget_info['amount']:.2f})\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è BUDGET WARNING: {period} spending (${period_cost:.2f}) is {alert['percentage']:.1f}% of budget\")\n",
    "    \n",
    "    def get_daily_cost(self, target_date: date) -> float:\n",
    "        \"\"\"Get total cost for a specific day\"\"\"\n",
    "        start = datetime.combine(target_date, datetime.min.time())\n",
    "        end = start + timedelta(days=1)\n",
    "        \n",
    "        return sum(record['cost'] for record in self.cost_history \n",
    "                  if start <= record['timestamp'] < end)\n",
    "    \n",
    "    def get_weekly_cost(self, target_date: date) -> float:\n",
    "        \"\"\"Get total cost for the week containing target_date\"\"\"\n",
    "        week_start = target_date - timedelta(days=target_date.weekday())\n",
    "        week_end = week_start + timedelta(days=7)\n",
    "        \n",
    "        start = datetime.combine(week_start, datetime.min.time())\n",
    "        end = datetime.combine(week_end, datetime.min.time())\n",
    "        \n",
    "        return sum(record['cost'] for record in self.cost_history \n",
    "                  if start <= record['timestamp'] < end)\n",
    "    \n",
    "    def get_monthly_cost(self, year: int, month: int) -> float:\n",
    "        \"\"\"Get total cost for a specific month\"\"\"\n",
    "        start = datetime(year, month, 1)\n",
    "        if month == 12:\n",
    "            end = datetime(year + 1, 1, 1)\n",
    "        else:\n",
    "            end = datetime(year, month + 1, 1)\n",
    "        \n",
    "        return sum(record['cost'] for record in self.cost_history \n",
    "                  if start <= record['timestamp'] < end)\n",
    "    \n",
    "    def get_cost_breakdown(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get detailed cost breakdown\"\"\"\n",
    "        if not self.cost_history:\n",
    "            return {}\n",
    "        \n",
    "        total_cost = sum(record['cost'] for record in self.cost_history)\n",
    "        \n",
    "        # By model\n",
    "        model_costs = defaultdict(float)\n",
    "        model_tokens = defaultdict(lambda: {'input': 0, 'output': 0})\n",
    "        \n",
    "        for record in self.cost_history:\n",
    "            model_costs[record['model']] += record['cost']\n",
    "            model_tokens[record['model']]['input'] += record['input_tokens']\n",
    "            model_tokens[record['model']]['output'] += record['output_tokens']\n",
    "        \n",
    "        # By project\n",
    "        project_costs = defaultdict(float)\n",
    "        for record in self.cost_history:\n",
    "            project = record.get('project', 'unknown')\n",
    "            project_costs[project] += record['cost']\n",
    "        \n",
    "        # By user\n",
    "        user_costs = defaultdict(float)\n",
    "        for record in self.cost_history:\n",
    "            user = record.get('user_id', 'unknown')\n",
    "            user_costs[user] += record['cost']\n",
    "        \n",
    "        return {\n",
    "            'total_cost': total_cost,\n",
    "            'total_records': len(self.cost_history),\n",
    "            'by_model': dict(model_costs),\n",
    "            'by_project': dict(project_costs),\n",
    "            'by_user': dict(user_costs),\n",
    "            'token_usage': dict(model_tokens)\n",
    "        }\n",
    "    \n",
    "    def get_optimization_recommendations(self) -> List[str]:\n",
    "        \"\"\"Get cost optimization recommendations\"\"\"\n",
    "        recommendations = []\n",
    "        breakdown = self.get_cost_breakdown()\n",
    "        \n",
    "        if not breakdown:\n",
    "            return recommendations\n",
    "        \n",
    "        # Check model usage patterns\n",
    "        model_costs = breakdown['by_model']\n",
    "        total_cost = breakdown['total_cost']\n",
    "        \n",
    "        # Recommend cheaper models if expensive ones dominate\n",
    "        if model_costs.get('gpt-4o', 0) > total_cost * 0.6:\n",
    "            recommendations.append(\n",
    "                \"Consider using gpt-4o-mini for simpler tasks to reduce costs by up to 95%\"\n",
    "            )\n",
    "        \n",
    "        # Check for high token usage\n",
    "        token_usage = breakdown['token_usage']\n",
    "        for model, tokens in token_usage.items():\n",
    "            avg_tokens_per_request = (tokens['input'] + tokens['output']) / max(1, breakdown['total_records'])\n",
    "            if avg_tokens_per_request > 2000:\n",
    "                recommendations.append(\n",
    "                    f\"High token usage detected for {model} (avg: {avg_tokens_per_request:.0f} tokens/request). \"\n",
    "                    \"Consider prompt optimization or input truncation.\"\n",
    "                )\n",
    "        \n",
    "        # Check for uneven project distribution\n",
    "        project_costs = breakdown['by_project']\n",
    "        if len(project_costs) > 1:\n",
    "            max_project_cost = max(project_costs.values())\n",
    "            if max_project_cost > total_cost * 0.8:\n",
    "                expensive_project = max(project_costs.keys(), key=project_costs.get)\n",
    "                recommendations.append(\n",
    "                    f\"Project '{expensive_project}' accounts for {(max_project_cost/total_cost)*100:.1f}% \"\n",
    "                    \"of total costs. Consider review and optimization.\"\n",
    "                )\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# Initialize cost manager\n",
    "cost_manager = CostManager()\n",
    "\n",
    "# Set budgets\n",
    "cost_manager.set_budget('daily', 50.0, alert_threshold=0.8)   # $50/day\n",
    "cost_manager.set_budget('weekly', 300.0, alert_threshold=0.8)  # $300/week\n",
    "cost_manager.set_budget('monthly', 1200.0, alert_threshold=0.8) # $1200/month\n",
    "\n",
    "print(\"üí∞ Cost management system initialized\")\n",
    "print(\"üìä Budgets set: Daily $50, Weekly $300, Monthly $1200\")\n",
    "\n",
    "# Simulate some usage to demonstrate cost tracking\n",
    "print(\"\\nüß™ Simulating API usage for cost tracking...\")\n",
    "\n",
    "# Simulate various model usage patterns\n",
    "usage_patterns = [\n",
    "    ('gpt-4o-mini', 150, 200, 'user1', 'project-a'),\n",
    "    ('gpt-4o', 300, 150, 'user2', 'project-b'),\n",
    "    ('gpt-4o-mini', 200, 300, 'user1', 'project-a'),\n",
    "    ('gpt-4o-mini', 100, 150, 'user3', 'project-c'),\n",
    "    ('gpt-4o', 250, 180, 'user2', 'project-b'),\n",
    "]\n",
    "\n",
    "for model, input_tokens, output_tokens, user, project in usage_patterns:\n",
    "    cost = cost_manager.track_cost(model, input_tokens, output_tokens, user, project)\n",
    "    print(f\"  üí∏ {model}: {input_tokens}+{output_tokens} tokens = ${cost:.4f} (User: {user}, Project: {project})\")\n",
    "\n",
    "# Display cost breakdown\n",
    "print(\"\\nüìà Cost Analysis Dashboard:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "breakdown = cost_manager.get_cost_breakdown()\n",
    "print(f\"Total Cost: ${breakdown['total_cost']:.4f}\")\n",
    "print(f\"Total Requests: {breakdown['total_records']}\")\n",
    "print(f\"Average Cost/Request: ${breakdown['total_cost']/breakdown['total_records']:.4f}\")\n",
    "\n",
    "print(\"\\nCost by Model:\")\n",
    "for model, cost in breakdown['by_model'].items():\n",
    "    percentage = (cost / breakdown['total_cost']) * 100\n",
    "    print(f\"  {model}: ${cost:.4f} ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\nCost by Project:\")\n",
    "for project, cost in breakdown['by_project'].items():\n",
    "    percentage = (cost / breakdown['total_cost']) * 100\n",
    "    print(f\"  {project}: ${cost:.4f} ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\nCost by User:\")\n",
    "for user, cost in breakdown['by_user'].items():\n",
    "    percentage = (cost / breakdown['total_cost']) * 100\n",
    "    print(f\"  {user}: ${cost:.4f} ({percentage:.1f}%)\")\n",
    "\n",
    "# Show optimization recommendations\n",
    "print(\"\\nüéØ Optimization Recommendations:\")\n",
    "print(\"=\" * 40)\n",
    "recommendations = cost_manager.get_optimization_recommendations()\n",
    "if recommendations:\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. {rec}\")\n",
    "else:\n",
    "    print(\"‚úÖ No optimization recommendations at this time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Production Best Practices Summary\n",
    "\n",
    "Here are the key production monitoring best practices covered in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_production_best_practices():\n",
    "    \"\"\"Summary of production monitoring best practices\"\"\"\n",
    "    \n",
    "    best_practices = {\n",
    "        \"üîç Observability\": [\n",
    "            \"Implement comprehensive tracing with LangSmith for all LLM interactions\",\n",
    "            \"Track key metrics: latency, throughput, error rates, and quality scores\",\n",
    "            \"Use structured logging with correlation IDs for distributed systems\",\n",
    "            \"Implement health checks and readiness probes for services\"\n",
    "        ],\n",
    "        \n",
    "        \"üö® Alerting\": [\n",
    "            \"Set up multi-channel alerting (console, Slack, email, PagerDuty)\",\n",
    "            \"Configure alert suppression to prevent notification fatigue\",\n",
    "            \"Use severity-based routing (INFO ‚Üí console, CRITICAL ‚Üí on-call)\",\n",
    "            \"Implement escalation policies for unacknowledged critical alerts\"\n",
    "        ],\n",
    "        \n",
    "        \"üìä Metrics & Monitoring\": [\n",
    "            \"Monitor the four golden signals: latency, traffic, errors, saturation\",\n",
    "            \"Track business metrics: quality scores, user satisfaction, conversion rates\",\n",
    "            \"Implement SLI/SLO monitoring with error budgets\",\n",
    "            \"Use percentile-based latency monitoring (P50, P95, P99)\"\n",
    "        ],\n",
    "        \n",
    "        \"üí∞ Cost Management\": [\n",
    "            \"Implement real-time cost tracking with budget alerts\",\n",
    "            \"Monitor token usage patterns and optimize for efficiency\",\n",
    "            \"Use model routing (cheaper models for simple tasks)\",\n",
    "            \"Track cost attribution by user, project, and feature\"\n",
    "        ],\n",
    "        \n",
    "        \"üèóÔ∏è Infrastructure\": [\n",
    "            \"Integrate with existing monitoring stack (Prometheus, Grafana, Datadog)\",\n",
    "            \"Use OpenTelemetry for standardized distributed tracing\",\n",
    "            \"Implement circuit breakers and retry logic with exponential backoff\",\n",
    "            \"Set up automated scaling based on demand and performance metrics\"\n",
    "        ],\n",
    "        \n",
    "        \"üîí Security & Compliance\": [\n",
    "            \"Monitor for PII and sensitive data in prompts and responses\",\n",
    "            \"Implement audit logs for all LLM interactions\",\n",
    "            \"Set up anomaly detection for unusual usage patterns\",\n",
    "            \"Ensure compliance with data retention and privacy policies\"\n",
    "        ],\n",
    "        \n",
    "        \"üîÑ Continuous Improvement\": [\n",
    "            \"Regular review of monitoring metrics and alert thresholds\",\n",
    "            \"Conduct post-incident reviews and update monitoring accordingly\",\n",
    "            \"Implement A/B testing for monitoring new features and optimizations\",\n",
    "            \"Use monitoring data to inform capacity planning and architecture decisions\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üè≠ PRODUCTION MONITORING BEST PRACTICES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for category, practices in best_practices.items():\n",
    "        print(f\"\\n{category}\")\n",
    "        print(\"-\" * (len(category) - 2))  # Subtract 2 for emoji\n",
    "        for practice in practices:\n",
    "            print(f\"‚Ä¢ {practice}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ Remember: Production monitoring is an iterative process.\")\n",
    "    print(\"   Start with basic monitoring and gradually add sophistication.\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "# Display best practices\n",
    "print_production_best_practices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully completed the Production Monitoring notebook! Here's what you've accomplished:\n",
    "\n",
    "### ‚úÖ Key Achievements\n",
    "- **Built a comprehensive monitoring system** with real-time metrics tracking\n",
    "- **Implemented multi-channel alerting** with smart suppression rules\n",
    "- **Created cost management systems** with budget tracking and optimization\n",
    "- **Learned enterprise integration patterns** for existing monitoring infrastructure\n",
    "- **Mastered production best practices** for LLM application operations\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "1. **Implement in your production environment**:\n",
    "   - Start with basic metrics and alerting\n",
    "   - Gradually add sophisticated monitoring features\n",
    "   - Integrate with your existing infrastructure\n",
    "\n",
    "2. **Continue with advanced topics**:\n",
    "   - **LSM-007**: Advanced Patterns - Complex use cases and integrations\n",
    "   - **LSM-008**: Tips and FAQs - Pro tips and troubleshooting\n",
    "\n",
    "3. **Join the community**:\n",
    "   - Share your monitoring setups and learnings\n",
    "   - Contribute to LangSmith documentation and examples\n",
    "\n",
    "### üí° Pro Tips for Production\n",
    "- **Start simple**: Begin with basic monitoring and add complexity gradually\n",
    "- **Monitor the monitors**: Ensure your monitoring system is reliable\n",
    "- **Test your alerts**: Regularly test alert channels and escalation paths\n",
    "- **Review and iterate**: Continuously improve based on production experience\n",
    "\n",
    "---\n",
    "\n",
    "**Ready for advanced patterns?** Continue to LSM-007 for complex use cases and integration patterns! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}