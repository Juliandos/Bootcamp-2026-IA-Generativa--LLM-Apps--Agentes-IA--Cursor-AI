{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Notebook 26: Understanding Vector Databases and Embeddings - The Magic Behind RAG\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "You've successfully implemented a RAG system, but now it's time to understand the **mathematical magic** that makes it work! In this notebook, we'll dive deep into the concepts that power semantic search: **embeddings** and **vector databases**.\n",
    "\n",
    "Understanding these concepts will make you a more effective AI developer and help you optimize your RAG systems for better performance and accuracy.\n",
    "\n",
    "## üß† The Core Question We're Answering\n",
    "\n",
    "**How does a computer understand that these are related?**\n",
    "- \"What are the benefits of exercise?\"\n",
    "- \"Physical activity improves cardiovascular health.\"\n",
    "\n",
    "**Traditional computers**: See completely different strings of characters  \n",
    "**AI with embeddings**: Understand semantic similarity and meaning\n",
    "\n",
    "## üîç What We'll Explore\n",
    "\n",
    "1. **What are embeddings?** The mathematical representation of meaning\n",
    "2. **How do vector databases work?** Fast similarity search at scale\n",
    "3. **FAISS deep dive** Understanding Facebook's vector search library\n",
    "4. **Optimization strategies** Making your RAG system faster and more accurate\n",
    "5. **Alternative approaches** When to use different embedding models and vector stores\n",
    "\n",
    "---\n",
    "\n",
    "**üí° Key Insight**: Embeddings transform the fuzzy concept of \"meaning\" into precise mathematics, enabling computers to understand semantic relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Part 1: What Are Embeddings? - Converting Words to Mathematics\n",
    "\n",
    "### The Fundamental Problem\n",
    "\n",
    "**ü§î How do you teach a computer about meaning?**\n",
    "\n",
    "**Traditional approach (keyword matching):**\n",
    "```python\n",
    "question = \"What helps with fitness?\"\n",
    "document = \"Regular exercise improves health.\"\n",
    "\n",
    "# No common words = no match found!\n",
    "common_words = set(question.split()) & set(document.split())\n",
    "print(common_words)  # {}\n",
    "```\n",
    "\n",
    "**Embedding approach (semantic understanding):**\n",
    "```python\n",
    "# Convert to mathematical vectors\n",
    "question_vector = [0.2, -0.1, 0.8, 0.3, ...] # 1536 dimensions\n",
    "document_vector = [0.25, -0.05, 0.75, 0.28, ...] # 1536 dimensions\n",
    "\n",
    "# Measure similarity\n",
    "similarity = cosine_similarity(question_vector, document_vector)\n",
    "print(similarity)  # 0.87 (highly similar!)\n",
    "```\n",
    "\n",
    "### What Exactly Is an Embedding?\n",
    "\n",
    "**üìä Mathematical Definition:**\n",
    "An embedding is a **dense vector representation** of text that captures semantic meaning in high-dimensional space.\n",
    "\n",
    "**üéØ In Simple Terms:**\n",
    "- **Input**: \"The cat sat on the mat\" (human language)\n",
    "- **Output**: [0.1, -0.3, 0.7, 0.2, ...] (1536 numbers)\n",
    "- **Magic**: Similar concepts get similar numbers!\n",
    "\n",
    "### How OpenAI Embeddings Work\n",
    "\n",
    "**üîß The Process:**\n",
    "```python\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Convert text to vector\n",
    "text = \"Machine learning is fascinating\"\n",
    "vector = embeddings.embed_query(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Vector length: {len(vector)}\")  # 1536 dimensions\n",
    "print(f\"First 5 dimensions: {vector[:5]}\")\n",
    "```\n",
    "\n",
    "**üìà What Each Dimension Represents:**\n",
    "Each of the 1536 dimensions captures different aspects of meaning:\n",
    "- Dimension 42 might represent \"technology concepts\"\n",
    "- Dimension 156 might represent \"positive/negative sentiment\"\n",
    "- Dimension 892 might represent \"abstract/concrete concepts\"\n",
    "\n",
    "**üé® Visualization (Simplified to 2D):**\n",
    "```\n",
    "Semantic Space:\n",
    "\n",
    "    |\n",
    "    |    ‚Ä¢ \"AI\"        ‚Ä¢ \"machine learning\"\n",
    "    |         ‚Ä¢ \"neural networks\"\n",
    "    |\n",
    "----+--------------------------------\n",
    "    |                    ‚Ä¢ \"cooking\"\n",
    "    |               ‚Ä¢ \"recipes\"\n",
    "    |    ‚Ä¢ \"food\"\n",
    "    |\n",
    "\n",
    "Related concepts cluster together in vector space!\n",
    "```\n",
    "\n",
    "### Why Embeddings Are Revolutionary\n",
    "\n",
    "**üåü Semantic Understanding:**\n",
    "- \"car\" and \"automobile\" have similar embeddings\n",
    "- \"king\" - \"man\" + \"woman\" ‚âà \"queen\" (famous example)\n",
    "- \"happy\" and \"joyful\" cluster together\n",
    "\n",
    "**üîç Cross-Language Similarity:**\n",
    "- \"hello\" (English) and \"hola\" (Spanish) have similar embeddings\n",
    "- Enables multilingual search without translation\n",
    "\n",
    "**üéØ Context Awareness:**\n",
    "- \"bank\" near \"river\" vs \"bank\" near \"money\" get different embeddings\n",
    "- Same word, different contexts, different vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Part 2: Vector Similarity - The Mathematics of Meaning\n",
    "\n",
    "### How Do We Measure Similarity?\n",
    "\n",
    "**üìê Cosine Similarity (Most Common):**\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vector_a, vector_b):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors.\n",
    "    Returns value between -1 and 1:\n",
    "    - 1.0 = identical meaning\n",
    "    - 0.0 = completely unrelated\n",
    "    - -1.0 = opposite meaning\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vector_a, vector_b)\n",
    "    norm_a = np.linalg.norm(vector_a)\n",
    "    norm_b = np.linalg.norm(vector_b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "# Example with real embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "vector_dog = embeddings.embed_query(\"dog\")\n",
    "vector_puppy = embeddings.embed_query(\"puppy\")\n",
    "vector_car = embeddings.embed_query(\"car\")\n",
    "\n",
    "print(f\"dog vs puppy: {cosine_similarity(vector_dog, vector_puppy):.3f}\")  # ~0.85\n",
    "print(f\"dog vs car: {cosine_similarity(vector_dog, vector_car):.3f}\")     # ~0.2\n",
    "```\n",
    "\n",
    "**üìä Alternative Distance Metrics:**\n",
    "\n",
    "**Euclidean Distance:**\n",
    "```python\n",
    "def euclidean_distance(vector_a, vector_b):\n",
    "    \"\"\"Straight-line distance between two points\"\"\"\n",
    "    return np.linalg.norm(vector_a - vector_b)\n",
    "```\n",
    "- **Lower values** = more similar\n",
    "- **Good for**: When magnitude matters\n",
    "\n",
    "**Dot Product:**\n",
    "```python\n",
    "def dot_product_similarity(vector_a, vector_b):\n",
    "    \"\"\"Raw dot product (faster but less normalized)\"\"\"\n",
    "    return np.dot(vector_a, vector_b)\n",
    "```\n",
    "- **Higher values** = more similar\n",
    "- **Good for**: When you need speed over precision\n",
    "\n",
    "### Understanding Similarity Thresholds\n",
    "\n",
    "**üéØ Typical Cosine Similarity Ranges:**\n",
    "- **0.9 - 1.0**: Nearly identical (synonyms, very similar phrases)\n",
    "- **0.8 - 0.9**: Highly related (same topic, related concepts)\n",
    "- **0.6 - 0.8**: Moderately related (same domain, tangentially related)\n",
    "- **0.4 - 0.6**: Somewhat related (broad topical similarity)\n",
    "- **0.0 - 0.4**: Weakly related or unrelated\n",
    "\n",
    "**üìã Real Examples from Document Search:**\n",
    "```python\n",
    "# Query: \"How to improve team productivity?\"\n",
    "# Document chunks and their similarity scores:\n",
    "\n",
    "chunks_with_scores = [\n",
    "    (\"Team productivity can be enhanced through better communication...\", 0.92),\n",
    "    (\"Effective project management increases workplace efficiency...\", 0.78),\n",
    "    (\"Employee satisfaction leads to better performance metrics...\", 0.65),\n",
    "    (\"The company cafeteria serves lunch from 12-2 PM...\", 0.12),\n",
    "]\n",
    "\n",
    "# Only chunks above threshold (e.g., 0.5) would be used for RAG\n",
    "relevant_chunks = [(text, score) for text, score in chunks_with_scores if score > 0.5]\n",
    "```\n",
    "\n",
    "### The Curse of Dimensionality\n",
    "\n",
    "**üåå Why 1536 Dimensions?**\n",
    "- **More dimensions** = more nuanced meaning representation\n",
    "- **Fewer dimensions** = faster processing but less precision\n",
    "- **1536** = OpenAI's sweet spot for text-embedding-ada-002\n",
    "\n",
    "**‚ö° Performance Implications:**\n",
    "```python\n",
    "# Computing similarity for different dimension sizes\n",
    "dimensions = [128, 256, 512, 1024, 1536]\n",
    "processing_times = [0.1, 0.2, 0.5, 1.2, 2.1]  # milliseconds per comparison\n",
    "\n",
    "# Trade-off: Accuracy vs Speed\n",
    "# 1536 dimensions: Maximum accuracy, slower processing\n",
    "# 128 dimensions: Faster processing, reduced accuracy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Part 3: FAISS Deep Dive - Facebook's Vector Search Engine\n",
    "\n",
    "### What Is FAISS?\n",
    "\n",
    "**üöÄ FAISS = Facebook AI Similarity Search**\n",
    "- **Purpose**: Efficient similarity search in high-dimensional vectors\n",
    "- **Performance**: Search millions of vectors in milliseconds\n",
    "- **Memory efficient**: Optimized storage and retrieval algorithms\n",
    "- **Scalable**: From thousands to billions of vectors\n",
    "\n",
    "### How FAISS Works Under the Hood\n",
    "\n",
    "**üìä The Naive Approach (What We Don't Want):**\n",
    "```python\n",
    "def naive_search(query_vector, all_vectors):\n",
    "    \"\"\"Brute force: Compare against every vector\"\"\"\n",
    "    similarities = []\n",
    "    for vector in all_vectors:  # Could be millions!\n",
    "        sim = cosine_similarity(query_vector, vector)\n",
    "        similarities.append(sim)\n",
    "    return sorted(similarities, reverse=True)[:k]  # Return top k\n",
    "\n",
    "# Problem: O(n) complexity - gets slower with more documents!\n",
    "```\n",
    "\n",
    "**‚ö° The FAISS Approach (Optimized):**\n",
    "```python\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = 1536  # OpenAI embedding dimension\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner Product (similar to cosine)\n",
    "\n",
    "# Add vectors to index (preprocessing)\n",
    "vectors = np.array(all_document_embeddings).astype('float32')\n",
    "index.add(vectors)\n",
    "\n",
    "# Search (lightning fast!)\n",
    "query_vector = np.array([query_embedding]).astype('float32')\n",
    "k = 4  # Find top 4 similar vectors\n",
    "scores, indices = index.search(query_vector, k)\n",
    "\n",
    "# Result: Top 4 most similar documents in milliseconds!\n",
    "```\n",
    "\n",
    "### FAISS Index Types\n",
    "\n",
    "**üéØ IndexFlatIP (Exact Search):**\n",
    "```python\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "```\n",
    "- **Accuracy**: 100% exact results\n",
    "- **Speed**: Good for up to ~100K vectors\n",
    "- **Memory**: Stores all vectors in full precision\n",
    "\n",
    "**‚ö° IndexIVFFlat (Faster Approximate Search):**\n",
    "```python\n",
    "nlist = 100  # Number of clusters\n",
    "quantizer = faiss.IndexFlatIP(dimension)\n",
    "index = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n",
    "\n",
    "# Training phase (clusters vectors)\n",
    "index.train(training_vectors)\n",
    "index.add(vectors)\n",
    "```\n",
    "- **Accuracy**: ~95% with proper tuning\n",
    "- **Speed**: 10-100x faster than flat search\n",
    "- **Use case**: Millions of vectors\n",
    "\n",
    "**üíæ IndexIVFPQ (Memory Optimized):**\n",
    "```python\n",
    "m = 64  # Number of subquantizers\n",
    "bits = 8  # Bits per subquantizer\n",
    "nlist = 100\n",
    "quantizer = faiss.IndexFlatIP(dimension)\n",
    "index = faiss.IndexIVFPQ(quantizer, dimension, nlist, m, bits)\n",
    "```\n",
    "- **Memory**: 32x less memory usage\n",
    "- **Accuracy**: ~85-90% with tuning\n",
    "- **Use case**: Billions of vectors, limited memory\n",
    "\n",
    "### LangChain + FAISS Integration\n",
    "\n",
    "**üîß How LangChain Uses FAISS:**\n",
    "```python\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# What happens behind the scenes:\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = FAISS.from_documents(document_chunks, embeddings)\n",
    "\n",
    "# LangChain automatically:\n",
    "# 1. Converts each chunk to embedding\n",
    "# 2. Creates FAISS index\n",
    "# 3. Adds all vectors to index\n",
    "# 4. Provides search interface\n",
    "\n",
    "# Search similar documents\n",
    "similar_docs = vector_store.similarity_search(\"your query\", k=4)\n",
    "```\n",
    "\n",
    "**üéõÔ∏è Customizing FAISS in LangChain:**\n",
    "```python\n",
    "# Create custom FAISS index\n",
    "import faiss\n",
    "\n",
    "# For large document collections\n",
    "index = faiss.IndexIVFFlat(\n",
    "    faiss.IndexFlatIP(1536),  # quantizer\n",
    "    1536,  # dimension\n",
    "    100    # number of clusters\n",
    ")\n",
    "\n",
    "# Use custom index with LangChain\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings.embed_query,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore({}),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Part 4: Optimizing Your RAG System - Advanced Techniques\n",
    "\n",
    "### Text Chunking Strategies\n",
    "\n",
    "**üìè Chunk Size Impact on Embeddings:**\n",
    "```python\n",
    "# Different chunking strategies for different content types\n",
    "\n",
    "# Strategy 1: Small chunks (more precise, less context)\n",
    "small_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,    # ~250 words\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \", \", \" \"]\n",
    ")\n",
    "# Good for: FAQ documents, definitions, specific facts\n",
    "# Embedding quality: High precision, may miss broader context\n",
    "\n",
    "# Strategy 2: Medium chunks (balanced)\n",
    "medium_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=3000,    # ~750 words\n",
    "    chunk_overlap=400,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \", \"]\n",
    ")\n",
    "# Good for: General documents, articles, reports\n",
    "# Embedding quality: Good balance of precision and context\n",
    "\n",
    "# Strategy 3: Large chunks (more context, less precise)\n",
    "large_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=6000,    # ~1500 words\n",
    "    chunk_overlap=800,\n",
    "    separators=[\"\\n\\n\", \"\\n\"]\n",
    ")\n",
    "# Good for: Books, research papers, complex narratives\n",
    "# Embedding quality: Rich context, may dilute specific details\n",
    "```\n",
    "\n",
    "**üéØ Content-Aware Chunking:**\n",
    "```python\n",
    "def smart_chunk_by_content_type(document_text: str, content_type: str):\n",
    "    \"\"\"\n",
    "    Adjust chunking strategy based on document type\n",
    "    \"\"\"\n",
    "    if content_type == \"code_documentation\":\n",
    "        return RecursiveCharacterTextSplitter(\n",
    "            chunk_size=2000,\n",
    "            chunk_overlap=200,\n",
    "            separators=[\"```\", \"###\", \"##\", \"\\n\\n\", \"\\n\"]\n",
    "        )\n",
    "    elif content_type == \"legal_document\":\n",
    "        return RecursiveCharacterTextSplitter(\n",
    "            chunk_size=4000,\n",
    "            chunk_overlap=600,\n",
    "            separators=[\"Section\", \"Article\", \"\\n\\n\", \". \"]\n",
    "        )\n",
    "    elif content_type == \"scientific_paper\":\n",
    "        return RecursiveCharacterTextSplitter(\n",
    "            chunk_size=3500,\n",
    "            chunk_overlap=500,\n",
    "            separators=[\"Abstract\", \"Introduction\", \"Methods\", \"\\n\\n\"]\n",
    "        )\n",
    "    else:\n",
    "        return RecursiveCharacterTextSplitter(\n",
    "            chunk_size=3000,\n",
    "            chunk_overlap=400\n",
    "        )\n",
    "```\n",
    "\n",
    "### Embedding Quality Optimization\n",
    "\n",
    "**üßπ Text Preprocessing for Better Embeddings:**\n",
    "```python\n",
    "import re\n",
    "\n",
    "def preprocess_text_for_embedding(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean text to improve embedding quality\n",
    "    \"\"\"\n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove page numbers and headers/footers\n",
    "    text = re.sub(r'Page \\d+|\\d+ of \\d+', '', text)\n",
    "    \n",
    "    # Normalize bullet points\n",
    "    text = re.sub(r'[‚Ä¢¬∑‚ñ™‚ñ´‚ó¶‚Ä£‚ÅÉ]', '-', text)\n",
    "    \n",
    "    # Clean up table formatting\n",
    "    text = re.sub(r'\\|+', ' ', text)\n",
    "    text = re.sub(r'-{3,}', '', text)\n",
    "    \n",
    "    # Remove excessive punctuation\n",
    "    text = re.sub(r'[.]{3,}', '...', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Apply preprocessing before creating embeddings\n",
    "processed_chunks = [preprocess_text_for_embedding(chunk.page_content) \n",
    "                   for chunk in document_chunks]\n",
    "```\n",
    "\n",
    "**üéØ Query Enhancement Techniques:**\n",
    "```python\n",
    "def enhance_query_for_better_retrieval(original_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Expand query to improve retrieval accuracy\n",
    "    \"\"\"\n",
    "    # Add context keywords\n",
    "    query_enhancements = {\n",
    "        \"benefits\": [\"advantages\", \"pros\", \"positive effects\"],\n",
    "        \"problems\": [\"issues\", \"challenges\", \"difficulties\"],\n",
    "        \"methods\": [\"approaches\", \"techniques\", \"strategies\"],\n",
    "        \"results\": [\"outcomes\", \"findings\", \"conclusions\"]\n",
    "    }\n",
    "    \n",
    "    enhanced_query = original_query\n",
    "    for keyword, synonyms in query_enhancements.items():\n",
    "        if keyword in original_query.lower():\n",
    "            enhanced_query += f\" {' '.join(synonyms)}\"\n",
    "    \n",
    "    return enhanced_query\n",
    "\n",
    "# Use in RAG pipeline\n",
    "enhanced_question = enhance_query_for_better_retrieval(question)\n",
    "relevant_chunks = vector_store.similarity_search(enhanced_question, k=4)\n",
    "```\n",
    "\n",
    "### Retrieval Parameter Tuning\n",
    "\n",
    "**üîß Advanced Retrieval Configuration:**\n",
    "```python\n",
    "# Standard similarity search\n",
    "basic_retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n",
    "\n",
    "# Similarity search with score threshold\n",
    "threshold_retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\n",
    "        \"k\": 10,                    # Fetch up to 10 chunks\n",
    "        \"score_threshold\": 0.6      # Only keep chunks above 0.6 similarity\n",
    "    }\n",
    ")\n",
    "\n",
    "# Maximum Marginal Relevance (MMR) for diversity\n",
    "mmr_retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 4,                     # Return 4 chunks\n",
    "        \"fetch_k\": 20,              # Consider top 20 candidates\n",
    "        \"lambda_mult\": 0.7          # Balance relevance (0.7) vs diversity (0.3)\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "**üìä A/B Testing Retrieval Quality:**\n",
    "```python\n",
    "def evaluate_retrieval_quality(test_questions, ground_truth_answers):\n",
    "    \"\"\"\n",
    "    Compare different retrieval configurations\n",
    "    \"\"\"\n",
    "    configurations = {\n",
    "        \"basic\": {\"search_type\": \"similarity\", \"k\": 4},\n",
    "        \"threshold\": {\"search_type\": \"similarity_score_threshold\", \n",
    "                     \"score_threshold\": 0.6, \"k\": 10},\n",
    "        \"mmr\": {\"search_type\": \"mmr\", \"k\": 4, \"fetch_k\": 20, \"lambda_mult\": 0.7}\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for config_name, config in configurations.items():\n",
    "        retriever = vector_store.as_retriever(search_kwargs=config)\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm, retriever=retriever\n",
    "        )\n",
    "        \n",
    "        accuracy_scores = []\n",
    "        for question, expected_answer in zip(test_questions, ground_truth_answers):\n",
    "            actual_answer = qa_chain.run(question)\n",
    "            score = calculate_answer_similarity(actual_answer, expected_answer)\n",
    "            accuracy_scores.append(score)\n",
    "        \n",
    "        results[config_name] = np.mean(accuracy_scores)\n",
    "    \n",
    "    return results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Part 5: Alternative Embedding Models and Vector Stores\n",
    "\n",
    "### Beyond OpenAI Embeddings\n",
    "\n",
    "**üåê Open Source Alternatives:**\n",
    "\n",
    "**Sentence Transformers (Hugging Face):**\n",
    "```python\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Free, runs locally, no API calls!\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Pros: Free, private, fast\n",
    "# Cons: Slightly lower quality than OpenAI, requires local GPU for speed\n",
    "```\n",
    "\n",
    "**Cohere Embeddings:**\n",
    "```python\n",
    "from langchain.embeddings import CohereEmbeddings\n",
    "\n",
    "embeddings = CohereEmbeddings(\n",
    "    cohere_api_key=\"your-cohere-key\",\n",
    "    model=\"embed-english-v2.0\"\n",
    ")\n",
    "\n",
    "# Pros: High quality, competitive pricing\n",
    "# Cons: Another API dependency\n",
    "```\n",
    "\n",
    "**Google PaLM Embeddings:**\n",
    "```python\n",
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "\n",
    "embeddings = GooglePalmEmbeddings(\n",
    "    google_api_key=\"your-google-key\"\n",
    ")\n",
    "\n",
    "# Pros: Google's quality, integrated with other Google services\n",
    "# Cons: Newer, less documentation\n",
    "```\n",
    "\n",
    "### Vector Store Alternatives to FAISS\n",
    "\n",
    "**‚òÅÔ∏è Pinecone (Managed Vector Database):**\n",
    "```python\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=\"your-pinecone-key\",\n",
    "    environment=\"us-west1-gcp\"\n",
    ")\n",
    "\n",
    "vector_store = Pinecone.from_documents(\n",
    "    documents=document_chunks,\n",
    "    embedding=embeddings,\n",
    "    index_name=\"pdf-rag-index\"\n",
    ")\n",
    "\n",
    "# Pros: Fully managed, scales to billions, real-time updates\n",
    "# Cons: Monthly cost, external dependency\n",
    "```\n",
    "\n",
    "**üåä Weaviate (Open Source + Cloud):**\n",
    "```python\n",
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "\n",
    "client = weaviate.Client(\n",
    "    url=\"https://your-weaviate-cluster.weaviate.network\",\n",
    "    auth_client_secret=weaviate.AuthApiKey(\"your-weaviate-key\")\n",
    ")\n",
    "\n",
    "vector_store = Weaviate.from_documents(\n",
    "    documents=document_chunks,\n",
    "    embedding=embeddings,\n",
    "    client=client,\n",
    "    index_name=\"PDFDocuments\"\n",
    ")\n",
    "\n",
    "# Pros: GraphQL API, hybrid search, self-hostable\n",
    "# Cons: More complex setup\n",
    "```\n",
    "\n",
    "**üé® Chroma (Developer-Friendly):**\n",
    "```python\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=document_chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\"  # Local persistence\n",
    ")\n",
    "\n",
    "# Pros: Easy setup, built for LLM apps, persistent storage\n",
    "# Cons: Newer project, smaller ecosystem\n",
    "```\n",
    "\n",
    "### When to Choose Which Option\n",
    "\n",
    "**üéØ Decision Matrix:**\n",
    "\n",
    "**For Learning/Prototyping:**\n",
    "- **Embeddings**: OpenAI (quality) or HuggingFace (free)\n",
    "- **Vector Store**: FAISS (simple) or Chroma (persistent)\n",
    "\n",
    "**For Small Production Apps:**\n",
    "- **Embeddings**: OpenAI or Cohere\n",
    "- **Vector Store**: FAISS with Redis caching\n",
    "\n",
    "**For Enterprise Applications:**\n",
    "- **Embeddings**: OpenAI (performance) or HuggingFace (privacy)\n",
    "- **Vector Store**: Pinecone (managed) or Weaviate (self-hosted)\n",
    "\n",
    "**Cost Comparison (Approximate):**\n",
    "```python\n",
    "# Monthly costs for 1M document chunks\n",
    "cost_comparison = {\n",
    "    \"OpenAI + FAISS\": {\n",
    "        \"embedding_cost\": 100,  # $100 for embeddings\n",
    "        \"storage_cost\": 0,      # FAISS is local\n",
    "        \"total\": 100\n",
    "    },\n",
    "    \"HuggingFace + Chroma\": {\n",
    "        \"embedding_cost\": 0,    # Free local embeddings\n",
    "        \"storage_cost\": 10,     # Server hosting\n",
    "        \"total\": 10\n",
    "    },\n",
    "    \"OpenAI + Pinecone\": {\n",
    "        \"embedding_cost\": 100,  # OpenAI embeddings\n",
    "        \"storage_cost\": 50,     # Pinecone managed service\n",
    "        \"total\": 150\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Part 6: Advanced RAG Patterns and Techniques\n",
    "\n",
    "### Hybrid Search (Combining Vector + Keyword)\n",
    "\n",
    "**üîÑ The Best of Both Worlds:**\n",
    "```python\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.retrievers import BM25Retriever\n",
    "\n",
    "def create_hybrid_retriever(documents, embeddings):\n",
    "    \"\"\"\n",
    "    Combine semantic (vector) and keyword (BM25) search\n",
    "    \"\"\"\n",
    "    # Vector-based retriever\n",
    "    vector_store = FAISS.from_documents(documents, embeddings)\n",
    "    vector_retriever = vector_store.as_retriever(search_kwargs={\"k\": 6})\n",
    "    \n",
    "    # Keyword-based retriever\n",
    "    bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "    bm25_retriever.k = 6\n",
    "    \n",
    "    # Combine both with weights\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[vector_retriever, bm25_retriever],\n",
    "        weights=[0.7, 0.3]  # 70% semantic, 30% keyword\n",
    "    )\n",
    "    \n",
    "    return ensemble_retriever\n",
    "\n",
    "# Benefits:\n",
    "# - Catches exact term matches (keyword)\n",
    "# - Understands semantic similarity (vector)\n",
    "# - More robust overall retrieval\n",
    "```\n",
    "\n",
    "### Contextual Compression\n",
    "\n",
    "**üóúÔ∏è Filter Retrieved Content for Relevance:**\n",
    "```python\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "def create_contextual_retriever(base_retriever, llm):\n",
    "    \"\"\"\n",
    "    Only keep parts of documents that are actually relevant\n",
    "    \"\"\"\n",
    "    compressor = LLMChainExtractor.from_llm(llm)\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor,\n",
    "        base_retriever=base_retriever\n",
    "    )\n",
    "    return compression_retriever\n",
    "\n",
    "# How it works:\n",
    "# 1. Retrieve potentially relevant documents\n",
    "# 2. Use LLM to extract only relevant parts\n",
    "# 3. Pass compressed context to final answer generation\n",
    "# \n",
    "# Result: More focused, less noisy context for better answers\n",
    "```\n",
    "\n",
    "### Multi-Query Retrieval\n",
    "\n",
    "**üîÑ Generate Multiple Perspectives:**\n",
    "```python\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "def create_multi_query_retriever(vector_store, llm):\n",
    "    \"\"\"\n",
    "    Generate multiple query variations for better retrieval\n",
    "    \"\"\"\n",
    "    retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "        retriever=vector_store.as_retriever(),\n",
    "        llm=llm\n",
    "    )\n",
    "    return retriever_from_llm\n",
    "\n",
    "# Example:\n",
    "# Original query: \"How to improve team productivity?\"\n",
    "# Generated variations:\n",
    "# - \"Methods for increasing team efficiency\"\n",
    "# - \"Strategies to boost workplace performance\"\n",
    "# - \"Ways to enhance team collaboration and output\"\n",
    "#\n",
    "# Searches with all variations, combines results\n",
    "```\n",
    "\n",
    "### Parent Document Retrieval\n",
    "\n",
    "**üìÑ Retrieve Small, Return Large:**\n",
    "```python\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "def create_parent_document_retriever(documents, embeddings):\n",
    "    \"\"\"\n",
    "    Index small chunks, but return larger parent sections\n",
    "    \"\"\"\n",
    "    # Small chunks for precise search\n",
    "    child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "    \n",
    "    # Larger parent chunks for context\n",
    "    parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "    \n",
    "    vector_store = FAISS.from_documents([], embeddings)\n",
    "    store = InMemoryStore()\n",
    "    \n",
    "    retriever = ParentDocumentRetriever(\n",
    "        vectorstore=vector_store,\n",
    "        docstore=store,\n",
    "        child_splitter=child_splitter,\n",
    "        parent_splitter=parent_splitter,\n",
    "    )\n",
    "    \n",
    "    retriever.add_documents(documents)\n",
    "    return retriever\n",
    "\n",
    "# Benefits:\n",
    "# - Precise search (small chunks)\n",
    "# - Rich context (large parent sections)\n",
    "# - Best of both worlds\n",
    "```\n",
    "\n",
    "### Self-Query Retrieval\n",
    "\n",
    "**üß† Let LLM Decide Search Strategy:**\n",
    "```python\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "def create_self_query_retriever(vector_store, llm):\n",
    "    \"\"\"\n",
    "    LLM analyzes query and chooses optimal retrieval strategy\n",
    "    \"\"\"\n",
    "    metadata_field_info = [\n",
    "        AttributeInfo(\n",
    "            name=\"source\",\n",
    "            description=\"The document source\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        AttributeInfo(\n",
    "            name=\"page\",\n",
    "            description=\"The page number\",\n",
    "            type=\"integer\",\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    document_content_description = \"PDF document content\"\n",
    "    \n",
    "    retriever = SelfQueryRetriever.from_llm(\n",
    "        llm,\n",
    "        vector_store,\n",
    "        document_content_description,\n",
    "        metadata_field_info,\n",
    "        verbose=True\n",
    "    )\n",
    "    return retriever\n",
    "\n",
    "# Example:\n",
    "# Query: \"What did the introduction say about methodology?\"\n",
    "# LLM extracts:\n",
    "# - Content filter: \"methodology\"\n",
    "# - Metadata filter: page < 5 (introduction pages)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### The Mathematical Foundation You Now Understand:\n",
    "\n",
    "1. **üß† Embeddings**: Text becomes 1536-dimensional vectors that capture semantic meaning\n",
    "2. **üìê Similarity**: Cosine similarity measures how \"close\" concepts are in vector space\n",
    "3. **‚ö° FAISS**: Facebook's optimized engine makes vector search lightning-fast\n",
    "4. **üéØ Optimization**: Proper chunking and retrieval tuning dramatically improve results\n",
    "5. **üîß Flexibility**: Multiple embedding models and vector stores for different needs\n",
    "\n",
    "### Technical Mastery Achieved:\n",
    "\n",
    "‚úÖ **Vector Mathematics**: Understanding cosine similarity, distance metrics, dimensionality  \n",
    "‚úÖ **FAISS Internals**: Index types, performance trade-offs, scaling strategies  \n",
    "‚úÖ **Embedding Optimization**: Text preprocessing, chunking strategies, quality tuning  \n",
    "‚úÖ **Alternative Solutions**: Open source embeddings, managed vector databases  \n",
    "‚úÖ **Advanced Patterns**: Hybrid search, compression, multi-query techniques  \n",
    "\n",
    "### Real-World Applications:\n",
    "\n",
    "**üè¢ Enterprise Search:**\n",
    "- Index millions of company documents\n",
    "- Instant semantic search across knowledge bases\n",
    "- Multilingual document understanding\n",
    "\n",
    "**üî¨ Research & Development:**\n",
    "- Scientific paper analysis and synthesis\n",
    "- Patent search and prior art discovery\n",
    "- Literature review automation\n",
    "\n",
    "**üíº Customer Support:**\n",
    "- Intelligent FAQ systems\n",
    "- Automated ticket routing\n",
    "- Knowledge base recommendations\n",
    "\n",
    "### The Power of What You've Built:\n",
    "\n",
    "```python\n",
    "# Your RAG system now intelligently:\n",
    "# 1. Converts documents to mathematical meaning\n",
    "# 2. Finds semantically similar content\n",
    "# 3. Provides context-aware answers\n",
    "# 4. Scales to massive document collections\n",
    "# 5. Adapts to different content types\n",
    "```\n",
    "\n",
    "### Career Impact:\n",
    "\n",
    "**üöÄ Skills That Set You Apart:**\n",
    "- Understanding the mathematical foundations of semantic search\n",
    "- Ability to optimize embedding and retrieval systems\n",
    "- Knowledge of production-scale vector database solutions\n",
    "- Experience with multiple embedding providers and approaches\n",
    "\n",
    "**üíº Job Opportunities:**\n",
    "- **RAG Engineer**: Specialized role building document Q&A systems\n",
    "- **Vector Database Developer**: Working with Pinecone, Weaviate, etc.\n",
    "- **AI Platform Engineer**: Building embedding and retrieval infrastructure\n",
    "- **Search Intelligence Developer**: Enhancing traditional search with AI\n",
    "\n",
    "### What's Next:\n",
    "\n",
    "**üé® Frontend Integration (Notebook 27):**\n",
    "Now that you understand the mathematical foundation, it's time to build the user interface that makes RAG accessible and intuitive. You'll create:\n",
    "- Intelligent PDF selection interface\n",
    "- Real-time question and answer components\n",
    "- Loading states for embedding processing\n",
    "- Error handling for various failure modes\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You now understand the mathematical and technical foundations that power modern AI search systems. This knowledge puts you at the forefront of AI application development.\n",
    "\n",
    "**Ready to complete your RAG journey?** In Notebook 27, we'll build the frontend that brings your mathematical understanding to life through an intuitive user interface!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}