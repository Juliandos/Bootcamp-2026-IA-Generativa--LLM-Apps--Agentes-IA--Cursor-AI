{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSM-003: Mastering LLM Observability\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Master advanced tracing techniques and patterns\n",
    "- Understand and implement Agent Observability (2025 feature)\n",
    "- Learn debugging strategies using LangSmith traces\n",
    "- Implement custom instrumentation and context propagation\n",
    "- Analyze performance bottlenecks and optimization opportunities\n",
    "- Work with complex multi-agent and tool-using applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup and Imports\n",
    "\n",
    "Let's start by setting up our environment with the latest LangSmith capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langsmith langchain langchain-openai langchain-community\n",
    "!pip install tavily-python wikipedia-api\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import traceable, Client\n",
    "from langsmith.run_helpers import tracing_context\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import create_openai_functions_agent, AgentExecutor\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify setup\n",
    "print(f\"‚úÖ LangSmith Project: {os.getenv('LANGSMITH_PROJECT', 'Not set')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Advanced Tracing Patterns\n",
    "\n",
    "Let's explore sophisticated tracing techniques that go beyond basic function decoration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 1: Dynamic run naming and metadata\n",
    "@traceable(\n",
    "    run_type=\"chain\",\n",
    "    tags=[\"advanced-tracing\", \"dynamic-metadata\"]\n",
    ")\n",
    "def adaptive_text_processor(text: str, processing_mode: str = \"standard\"):\n",
    "    \"\"\"Demonstrates dynamic metadata and run naming\"\"\"\n",
    "    \n",
    "    # Update run info dynamically\n",
    "    from langsmith.run_helpers import get_current_run_tree\n",
    "    \n",
    "    current_run = get_current_run_tree()\n",
    "    if current_run:\n",
    "        current_run.name = f\"text_processor_{processing_mode}\"\n",
    "        current_run.extra[\"processing_mode\"] = processing_mode\n",
    "        current_run.extra[\"text_length\"] = len(text)\n",
    "        current_run.extra[\"word_count\"] = len(text.split())\n",
    "    \n",
    "    # Different processing based on mode\n",
    "    if processing_mode == \"summarize\":\n",
    "        return _summarize_text(text)\n",
    "    elif processing_mode == \"analyze\":\n",
    "        return _analyze_text(text)\n",
    "    else:\n",
    "        return _standard_processing(text)\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def _summarize_text(text: str):\n",
    "    \"\"\"Summarize the given text\"\"\"\n",
    "    llm = ChatOpenAI(temperature=0.3, model=\"gpt-3.5-turbo\")\n",
    "    messages = [\n",
    "        SystemMessage(content=\"Summarize the following text concisely:\"),\n",
    "        HumanMessage(content=text)\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"summary\": response.content, \"original_length\": len(text)}\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def _analyze_text(text: str):\n",
    "    \"\"\"Analyze the text for key themes\"\"\"\n",
    "    llm = ChatOpenAI(temperature=0.2, model=\"gpt-3.5-turbo\")\n",
    "    messages = [\n",
    "        SystemMessage(content=\"Analyze this text for key themes, sentiment, and complexity:\"),\n",
    "        HumanMessage(content=text)\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"analysis\": response.content, \"complexity_score\": len(text) / 100}\n",
    "\n",
    "@traceable(run_type=\"transform\")\n",
    "def _standard_processing(text: str):\n",
    "    \"\"\"Standard text processing\"\"\"\n",
    "    return {\n",
    "        \"processed_text\": text.strip().title(),\n",
    "        \"stats\": {\n",
    "            \"character_count\": len(text),\n",
    "            \"word_count\": len(text.split()),\n",
    "            \"sentence_count\": text.count('.') + text.count('!') + text.count('?')\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test adaptive processing\n",
    "sample_text = \"\"\"Artificial intelligence is transforming industries across the globe. \n",
    "From healthcare to finance, AI applications are becoming more sophisticated and widespread. \n",
    "Machine learning algorithms can now process vast amounts of data and identify patterns \n",
    "that would be impossible for humans to detect manually.\"\"\"\n",
    "\n",
    "print(\"üîÑ Testing Adaptive Text Processing:\\n\")\n",
    "\n",
    "for mode in [\"standard\", \"summarize\", \"analyze\"]:\n",
    "    try:\n",
    "        result = adaptive_text_processor(sample_text, mode)\n",
    "        print(f\"üìä Mode: {mode}\")\n",
    "        print(f\"Result: {str(result)[:100]}...\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in {mode} mode: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Agent Observability - 2025 Feature Deep Dive\n",
    "\n",
    "LangSmith's new Agent Observability provides enhanced insights into agent behavior, tool usage, and decision-making processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom tools for our agent\n",
    "\n",
    "@traceable(run_type=\"tool\", tags=[\"web-search\", \"agent-tool\"])\n",
    "def web_search_tool(query: str) -> str:\n",
    "    \"\"\"Simulate web search - in production, you'd use a real search API\"\"\"\n",
    "    # Add tool-specific metadata\n",
    "    from langsmith.run_helpers import get_current_run_tree\n",
    "    current_run = get_current_run_tree()\n",
    "    if current_run:\n",
    "        current_run.extra[\"search_query\"] = query\n",
    "        current_run.extra[\"search_timestamp\"] = datetime.now().isoformat()\n",
    "    \n",
    "    # Simulate search latency\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    # Simulated search results\n",
    "    results = f\"\"\"Search results for '{query}':\n",
    "1. Recent developments in {query} show significant progress\n",
    "2. Industry experts predict {query} will continue to evolve\n",
    "3. Key challenges in {query} include scalability and adoption\"\"\"\n",
    "    \n",
    "    return results\n",
    "\n",
    "@traceable(run_type=\"tool\", tags=[\"calculator\", \"agent-tool\"])\n",
    "def calculator_tool(expression: str) -> str:\n",
    "    \"\"\"Safe calculator for mathematical expressions\"\"\"\n",
    "    try:\n",
    "        # Simple evaluation - in production, use a safer approach\n",
    "        allowed_chars = set('0123456789+-*/.()')\n",
    "        if all(c in allowed_chars for c in expression.replace(' ', '')):\n",
    "            result = eval(expression)\n",
    "            return f\"Result: {result}\"\n",
    "        else:\n",
    "            return \"Error: Invalid characters in expression\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "@traceable(run_type=\"tool\", tags=[\"knowledge-base\", \"agent-tool\"])\n",
    "def knowledge_lookup(topic: str) -> str:\n",
    "    \"\"\"Lookup information from knowledge base\"\"\"\n",
    "    \n",
    "    # Simulate knowledge base lookup\n",
    "    knowledge_base = {\n",
    "        \"langsmith\": \"LangSmith is a platform for building production-grade LLM applications with observability and evaluation.\",\n",
    "        \"ai\": \"Artificial Intelligence involves creating systems that can perform tasks typically requiring human intelligence.\",\n",
    "        \"python\": \"Python is a high-level programming language known for its simplicity and versatility.\",\n",
    "        \"machine learning\": \"Machine Learning is a subset of AI that enables systems to learn from data without explicit programming.\"\n",
    "    }\n",
    "    \n",
    "    topic_lower = topic.lower()\n",
    "    for key, value in knowledge_base.items():\n",
    "        if key in topic_lower:\n",
    "            return f\"Knowledge base entry for '{topic}': {value}\"\n",
    "    \n",
    "    return f\"No specific information found for '{topic}' in knowledge base.\"\n",
    "\n",
    "# Create LangChain tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"web_search\",\n",
    "        func=web_search_tool,\n",
    "        description=\"Search the web for current information about any topic\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"calculator\",\n",
    "        func=calculator_tool,\n",
    "        description=\"Perform mathematical calculations. Input should be a mathematical expression.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"knowledge_lookup\",\n",
    "        func=knowledge_lookup,\n",
    "        description=\"Look up information from the internal knowledge base\"\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"üõ†Ô∏è Created agent tools:\")\n",
    "for tool in tools:\n",
    "    print(f\"  - {tool.name}: {tool.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an advanced agent with enhanced observability\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"agent\",\n",
    "    tags=[\"research-agent\", \"multi-tool\", \"agent-observability\"]\n",
    ")\n",
    "def create_research_agent():\n",
    "    \"\"\"Create a research agent with comprehensive observability\"\"\"\n",
    "    \n",
    "    # Initialize the LLM\n",
    "    llm = ChatOpenAI(\n",
    "        temperature=0.1,\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        model_kwargs={\"seed\": 42}  # For reproducibility\n",
    "    )\n",
    "    \n",
    "    # Create a detailed system prompt\n",
    "    system_prompt = \"\"\"You are a research assistant with access to multiple tools.\n",
    "    \n",
    "Your capabilities:\n",
    "- web_search: Find current information on the internet\n",
    "- calculator: Perform mathematical calculations\n",
    "- knowledge_lookup: Access internal knowledge base\n",
    "\n",
    "Instructions:\n",
    "1. Always think step by step\n",
    "2. Use tools when you need external information or calculations\n",
    "3. Provide comprehensive answers with sources when possible\n",
    "4. If you use multiple tools, explain why each was necessary\n",
    "\n",
    "Be thorough but concise in your responses.\"\"\"\n",
    "    \n",
    "    # Create the prompt template\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "    ])\n",
    "    \n",
    "    # Create the agent\n",
    "    agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "    \n",
    "    # Create agent executor with enhanced configuration\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=agent,\n",
    "        tools=tools,\n",
    "        verbose=True,\n",
    "        max_iterations=5,\n",
    "        early_stopping_method=\"generate\",\n",
    "        handle_parsing_errors=True,\n",
    "        return_intermediate_steps=True\n",
    "    )\n",
    "    \n",
    "    return agent_executor\n",
    "\n",
    "# Initialize the research agent\n",
    "research_agent = create_research_agent()\n",
    "print(\"‚úÖ Research agent created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent with complex queries that require multiple tools\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"workflow\",\n",
    "    tags=[\"agent-interaction\", \"multi-step-reasoning\"]\n",
    ")\n",
    "def run_research_query(agent_executor, query: str, context: str = \"\"):\n",
    "    \"\"\"Execute a research query with full observability\"\"\"\n",
    "    \n",
    "    # Add query metadata\n",
    "    from langsmith.run_helpers import get_current_run_tree\n",
    "    current_run = get_current_run_tree()\n",
    "    if current_run:\n",
    "        current_run.extra[\"query_length\"] = len(query)\n",
    "        current_run.extra[\"has_context\"] = bool(context)\n",
    "        current_run.extra[\"query_timestamp\"] = datetime.now().isoformat()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Execute the agent\n",
    "        result = agent_executor.invoke({\n",
    "            \"input\": query,\n",
    "            \"chat_history\": []\n",
    "        })\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        # Extract intermediate steps for analysis\n",
    "        intermediate_steps = result.get(\"intermediate_steps\", [])\n",
    "        tool_calls = []\n",
    "        \n",
    "        for step in intermediate_steps:\n",
    "            if len(step) >= 2:\n",
    "                action, observation = step[0], step[1]\n",
    "                tool_calls.append({\n",
    "                    \"tool\": getattr(action, 'tool', 'unknown'),\n",
    "                    \"input\": getattr(action, 'tool_input', {}),\n",
    "                    \"output_length\": len(str(observation))\n",
    "                })\n",
    "        \n",
    "        # Update run with execution metrics\n",
    "        if current_run:\n",
    "            current_run.extra[\"execution_time_seconds\"] = round(execution_time, 2)\n",
    "            current_run.extra[\"tool_calls_count\"] = len(tool_calls)\n",
    "            current_run.extra[\"tools_used\"] = [tc[\"tool\"] for tc in tool_calls]\n",
    "            current_run.extra[\"total_iterations\"] = len(intermediate_steps)\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"answer\": result[\"output\"],\n",
    "            \"execution_time\": execution_time,\n",
    "            \"tool_calls\": tool_calls,\n",
    "            \"iterations\": len(intermediate_steps)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        execution_time = time.time() - start_time\n",
    "        if current_run:\n",
    "            current_run.extra[\"error\"] = str(e)\n",
    "            current_run.extra[\"execution_time_seconds\"] = round(execution_time, 2)\n",
    "        raise\n",
    "\n",
    "# Test queries that showcase agent observability\n",
    "test_queries = [\n",
    "    {\n",
    "        \"query\": \"What is LangSmith and how much would it cost to process 10,000 API calls?\",\n",
    "        \"description\": \"Knowledge lookup + calculation\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Find recent developments in AI and calculate the percentage growth if adoption increased from 30% to 45%\",\n",
    "        \"description\": \"Web search + calculation\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Compare machine learning with traditional programming approaches\",\n",
    "        \"description\": \"Knowledge base synthesis\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing Agent Observability with Complex Queries:\\n\")\n",
    "\n",
    "for i, test in enumerate(test_queries, 1):\n",
    "    print(f\"üìù Query {i}: {test['description']}\")\n",
    "    print(f\"Question: {test['query'][:80]}...\")\n",
    "    \n",
    "    try:\n",
    "        result = run_research_query(research_agent, test[\"query\"])\n",
    "        \n",
    "        print(f\"‚úÖ Completed in {result['execution_time']:.2f}s\")\n",
    "        print(f\"üõ†Ô∏è Tools used: {', '.join(set(tc['tool'] for tc in result['tool_calls']))}\")\n",
    "        print(f\"üîÑ Iterations: {result['iterations']}\")\n",
    "        print(f\"üìÑ Answer: {result['answer'][:150]}...\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Custom Instrumentation and Context Propagation\n",
    "\n",
    "Learn how to implement custom instrumentation for non-LangChain components and propagate context across async operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced instrumentation patterns\n",
    "\n",
    "from langsmith.run_helpers import tracing_context\n",
    "from contextlib import contextmanager\n",
    "import uuid\n",
    "\n",
    "@contextmanager\n",
    "def custom_trace_context(name: str, run_type: str = \"custom\", **kwargs):\n",
    "    \"\"\"Custom context manager for manual tracing\"\"\"\n",
    "    from langsmith import Client\n",
    "    from langsmith.run_helpers import get_current_run_tree\n",
    "    \n",
    "    client = Client()\n",
    "    parent_run = get_current_run_tree()\n",
    "    \n",
    "    # Create a new run\n",
    "    run_id = uuid.uuid4()\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # Start the run\n",
    "        run = client.create_run(\n",
    "            name=name,\n",
    "            run_type=run_type,\n",
    "            inputs=kwargs.get('inputs', {}),\n",
    "            parent_run_id=parent_run.id if parent_run else None,\n",
    "            extra=kwargs.get('extra', {}),\n",
    "            tags=kwargs.get('tags', [])\n",
    "        )\n",
    "        \n",
    "        yield run\n",
    "        \n",
    "        # End the run successfully\n",
    "        client.update_run(\n",
    "            run.id,\n",
    "            outputs=kwargs.get('outputs', {}),\n",
    "            end_time=datetime.now()\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Handle errors\n",
    "        client.update_run(\n",
    "            run.id,\n",
    "            error=str(e),\n",
    "            end_time=datetime.now()\n",
    "        )\n",
    "        raise\n",
    "\n",
    "@traceable(run_type=\"workflow\", tags=[\"custom-instrumentation\", \"async\"])\n",
    "def complex_data_pipeline(data_source: str, transformations: List[str]):\n",
    "    \"\"\"A complex data pipeline with custom instrumentation\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Step 1: Data loading with custom tracing\n",
    "    with custom_trace_context(\n",
    "        name=\"data_loading\",\n",
    "        run_type=\"retrieval\",\n",
    "        inputs={\"source\": data_source},\n",
    "        extra={\"data_source_type\": \"simulated\"}\n",
    "    ) as load_run:\n",
    "        \n",
    "        # Simulate data loading\n",
    "        raw_data = [f\"record_{i}\" for i in range(100)]\n",
    "        time.sleep(0.2)  # Simulate I/O\n",
    "        \n",
    "        load_run.outputs = {\n",
    "            \"records_loaded\": len(raw_data),\n",
    "            \"data_sample\": raw_data[:3]\n",
    "        }\n",
    "    \n",
    "    # Step 2: Apply transformations with individual tracing\n",
    "    processed_data = raw_data\n",
    "    \n",
    "    for i, transformation in enumerate(transformations):\n",
    "        with custom_trace_context(\n",
    "            name=f\"transformation_{i+1}_{transformation}\",\n",
    "            run_type=\"transform\",\n",
    "            inputs={\n",
    "                \"transformation\": transformation,\n",
    "                \"input_size\": len(processed_data)\n",
    "            }\n",
    "        ) as transform_run:\n",
    "            \n",
    "            # Apply transformation\n",
    "            if transformation == \"uppercase\":\n",
    "                processed_data = [item.upper() for item in processed_data]\n",
    "            elif transformation == \"filter\":\n",
    "                processed_data = [item for item in processed_data if \"5\" not in item]\n",
    "            elif transformation == \"prefix\":\n",
    "                processed_data = [f\"processed_{item}\" for item in processed_data]\n",
    "            \n",
    "            transform_run.outputs = {\n",
    "                \"output_size\": len(processed_data),\n",
    "                \"transformation_applied\": transformation\n",
    "            }\n",
    "            \n",
    "            time.sleep(0.1)  # Simulate processing time\n",
    "    \n",
    "    # Step 3: Final aggregation\n",
    "    final_result = {\n",
    "        \"original_count\": len(raw_data),\n",
    "        \"final_count\": len(processed_data),\n",
    "        \"transformations_applied\": transformations,\n",
    "        \"sample_output\": processed_data[:3]\n",
    "    }\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "# Test the custom instrumentation\n",
    "try:\n",
    "    pipeline_result = complex_data_pipeline(\n",
    "        data_source=\"user_interactions_db\",\n",
    "        transformations=[\"uppercase\", \"filter\", \"prefix\"]\n",
    "    )\n",
    "    \n",
    "    print(\"üè≠ Data Pipeline Results:\")\n",
    "    print(f\"üìä Original records: {pipeline_result['original_count']}\")\n",
    "    print(f\"üîÑ Transformations: {', '.join(pipeline_result['transformations_applied'])}\")\n",
    "    print(f\"üìà Final records: {pipeline_result['final_count']}\")\n",
    "    print(f\"üîç Sample output: {pipeline_result['sample_output']}\")\n",
    "    print(\"\\n‚úÖ Custom instrumentation pipeline completed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Pipeline error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üêõ Advanced Debugging Techniques\n",
    "\n",
    "Learn how to use LangSmith traces for debugging complex issues and performance optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging helper functions\n",
    "\n",
    "@traceable(run_type=\"chain\", tags=[\"debugging\", \"error-handling\"])\n",
    "def robust_llm_chain(user_input: str, max_retries: int = 3):\n",
    "    \"\"\"LLM chain with comprehensive error handling and debugging info\"\"\"\n",
    "    \n",
    "    from langsmith.run_helpers import get_current_run_tree\n",
    "    current_run = get_current_run_tree()\n",
    "    \n",
    "    if current_run:\n",
    "        current_run.extra[\"max_retries\"] = max_retries\n",
    "        current_run.extra[\"input_length\"] = len(user_input)\n",
    "    \n",
    "    llm = ChatOpenAI(temperature=0.7, model=\"gpt-3.5-turbo\")\n",
    "    attempts = []\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        attempt_start = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Create messages\n",
    "            messages = [\n",
    "                SystemMessage(content=\"You are a helpful assistant. Respond clearly and concisely.\"),\n",
    "                HumanMessage(content=user_input)\n",
    "            ]\n",
    "            \n",
    "            # Make the LLM call\n",
    "            response = llm.invoke(messages)\n",
    "            \n",
    "            attempt_time = time.time() - attempt_start\n",
    "            \n",
    "            # Record successful attempt\n",
    "            attempts.append({\n",
    "                \"attempt\": attempt + 1,\n",
    "                \"status\": \"success\",\n",
    "                \"duration\": round(attempt_time, 3),\n",
    "                \"response_length\": len(response.content)\n",
    "            })\n",
    "            \n",
    "            if current_run:\n",
    "                current_run.extra[\"attempts\"] = attempts\n",
    "                current_run.extra[\"successful_attempt\"] = attempt + 1\n",
    "            \n",
    "            return {\n",
    "                \"response\": response.content,\n",
    "                \"attempts\": attempts,\n",
    "                \"success\": True\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            attempt_time = time.time() - attempt_start\n",
    "            \n",
    "            # Record failed attempt\n",
    "            attempts.append({\n",
    "                \"attempt\": attempt + 1,\n",
    "                \"status\": \"failed\",\n",
    "                \"duration\": round(attempt_time, 3),\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "            \n",
    "            # If this was the last attempt, raise the error\n",
    "            if attempt == max_retries - 1:\n",
    "                if current_run:\n",
    "                    current_run.extra[\"attempts\"] = attempts\n",
    "                    current_run.extra[\"all_attempts_failed\"] = True\n",
    "                \n",
    "                return {\n",
    "                    \"response\": None,\n",
    "                    \"attempts\": attempts,\n",
    "                    \"success\": False,\n",
    "                    \"final_error\": str(e)\n",
    "                }\n",
    "            \n",
    "            # Wait before retry\n",
    "            time.sleep(0.5 * (attempt + 1))\n",
    "\n",
    "@traceable(run_type=\"evaluation\", tags=[\"performance-analysis\"])\n",
    "def analyze_performance_patterns(test_inputs: List[str]):\n",
    "    \"\"\"Analyze performance patterns across multiple inputs\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for i, input_text in enumerate(test_inputs):\n",
    "        print(f\"üìä Processing input {i+1}/{len(test_inputs)}...\")\n",
    "        \n",
    "        result = robust_llm_chain(input_text)\n",
    "        results.append({\n",
    "            \"input\": input_text,\n",
    "            \"input_length\": len(input_text),\n",
    "            \"success\": result[\"success\"],\n",
    "            \"attempts_count\": len(result[\"attempts\"]),\n",
    "            \"total_duration\": sum(attempt[\"duration\"] for attempt in result[\"attempts\"]),\n",
    "            \"response_length\": len(result[\"response\"]) if result[\"response\"] else 0\n",
    "        })\n",
    "    \n",
    "    total_time = time.time() - total_start_time\n",
    "    \n",
    "    # Analyze patterns\n",
    "    successful_results = [r for r in results if r[\"success\"]]\n",
    "    avg_duration = sum(r[\"total_duration\"] for r in successful_results) / len(successful_results) if successful_results else 0\n",
    "    \n",
    "    analysis = {\n",
    "        \"total_inputs\": len(test_inputs),\n",
    "        \"successful_count\": len(successful_results),\n",
    "        \"success_rate\": len(successful_results) / len(test_inputs) * 100,\n",
    "        \"average_duration\": round(avg_duration, 3),\n",
    "        \"total_processing_time\": round(total_time, 3),\n",
    "        \"detailed_results\": results\n",
    "    }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Test with various input types to analyze patterns\n",
    "test_inputs = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain quantum computing in detail with examples and mathematical formulas.\",\n",
    "    \"Hi\",\n",
    "    \"Write a comprehensive analysis of the impact of artificial intelligence on modern society, including economic, social, and ethical considerations.\",\n",
    "    \"42\"\n",
    "]\n",
    "\n",
    "print(\"üîç Performance Analysis Starting...\\n\")\n",
    "\n",
    "try:\n",
    "    performance_analysis = analyze_performance_patterns(test_inputs)\n",
    "    \n",
    "    print(\"\\nüìà Performance Analysis Results:\")\n",
    "    print(f\"‚úÖ Success Rate: {performance_analysis['success_rate']:.1f}%\")\n",
    "    print(f\"‚è±Ô∏è Average Duration: {performance_analysis['average_duration']}s\")\n",
    "    print(f\"üïí Total Processing Time: {performance_analysis['total_processing_time']}s\")\n",
    "    \n",
    "    print(\"\\nüìä Individual Results:\")\n",
    "    for i, result in enumerate(performance_analysis['detailed_results'], 1):\n",
    "        status = \"‚úÖ\" if result[\"success\"] else \"‚ùå\"\n",
    "        print(f\"{status} Input {i}: {result['input_length']} chars ‚Üí {result['response_length']} chars ({result['total_duration']:.3f}s)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Analysis failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Trace Analysis and Insights\n",
    "\n",
    "Learn how to extract insights from your traces programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatic trace analysis\n",
    "\n",
    "from langsmith import Client\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def analyze_project_traces(project_name: str = None, hours_back: int = 24):\n",
    "    \"\"\"Analyze traces from your LangSmith project\"\"\"\n",
    "    \n",
    "    client = Client()\n",
    "    \n",
    "    if not project_name:\n",
    "        project_name = os.getenv(\"LANGSMITH_PROJECT\")\n",
    "    \n",
    "    if not project_name:\n",
    "        print(\"‚ö†Ô∏è No project name provided or found in environment\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate time range\n",
    "    end_time = datetime.now()\n",
    "    start_time = end_time - timedelta(hours=hours_back)\n",
    "    \n",
    "    print(f\"üìä Analyzing traces from project '{project_name}'...\")\n",
    "    print(f\"‚è∞ Time range: {start_time.strftime('%Y-%m-%d %H:%M')} to {end_time.strftime('%Y-%m-%d %H:%M')}\")\n",
    "    \n",
    "    try:\n",
    "        # Fetch traces\n",
    "        runs = list(client.list_runs(\n",
    "            project_name=project_name,\n",
    "            start_time=start_time,\n",
    "            end_time=end_time\n",
    "        ))\n",
    "        \n",
    "        if not runs:\n",
    "            print(\"üì≠ No traces found in the specified time range\")\n",
    "            return None\n",
    "        \n",
    "        # Analyze traces\n",
    "        analysis = {\n",
    "            \"total_runs\": len(runs),\n",
    "            \"run_types\": {},\n",
    "            \"tags\": {},\n",
    "            \"errors\": 0,\n",
    "            \"avg_latency\": 0,\n",
    "            \"total_tokens\": 0\n",
    "        }\n",
    "        \n",
    "        latencies = []\n",
    "        \n",
    "        for run in runs:\n",
    "            # Count run types\n",
    "            run_type = getattr(run, 'run_type', 'unknown')\n",
    "            analysis[\"run_types\"][run_type] = analysis[\"run_types\"].get(run_type, 0) + 1\n",
    "            \n",
    "            # Count tags\n",
    "            if hasattr(run, 'tags') and run.tags:\n",
    "                for tag in run.tags:\n",
    "                    analysis[\"tags\"][tag] = analysis[\"tags\"].get(tag, 0) + 1\n",
    "            \n",
    "            # Check for errors\n",
    "            if getattr(run, 'error', None):\n",
    "                analysis[\"errors\"] += 1\n",
    "            \n",
    "            # Calculate latency\n",
    "            if hasattr(run, 'start_time') and hasattr(run, 'end_time') and run.end_time:\n",
    "                if run.start_time and run.end_time:\n",
    "                    latency = (run.end_time - run.start_time).total_seconds()\n",
    "                    latencies.append(latency)\n",
    "            \n",
    "            # Count tokens (if available in extra data)\n",
    "            if hasattr(run, 'extra') and run.extra:\n",
    "                extra = run.extra if isinstance(run.extra, dict) else {}\n",
    "                if 'tokens' in extra:\n",
    "                    analysis[\"total_tokens\"] += extra['tokens']\n",
    "        \n",
    "        # Calculate averages\n",
    "        if latencies:\n",
    "            analysis[\"avg_latency\"] = round(sum(latencies) / len(latencies), 3)\n",
    "            analysis[\"min_latency\"] = round(min(latencies), 3)\n",
    "            analysis[\"max_latency\"] = round(max(latencies), 3)\n",
    "        \n",
    "        return analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing traces: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run the analysis\n",
    "trace_analysis = analyze_project_traces()\n",
    "\n",
    "if trace_analysis:\n",
    "    print(\"\\nüìà Trace Analysis Results:\")\n",
    "    print(f\"üìä Total Runs: {trace_analysis['total_runs']}\")\n",
    "    print(f\"‚ùå Errors: {trace_analysis['errors']} ({trace_analysis['errors']/trace_analysis['total_runs']*100:.1f}%)\")\n",
    "    print(f\"‚è±Ô∏è Average Latency: {trace_analysis['avg_latency']}s\")\n",
    "    \n",
    "    if 'min_latency' in trace_analysis:\n",
    "        print(f\"‚ö° Min Latency: {trace_analysis['min_latency']}s\")\n",
    "        print(f\"üêå Max Latency: {trace_analysis['max_latency']}s\")\n",
    "    \n",
    "    print(f\"üî¢ Total Tokens: {trace_analysis['total_tokens']}\")\n",
    "    \n",
    "    print(\"\\nüè∑Ô∏è Run Types:\")\n",
    "    for run_type, count in trace_analysis['run_types'].items():\n",
    "        print(f\"  - {run_type}: {count}\")\n",
    "    \n",
    "    if trace_analysis['tags']:\n",
    "        print(\"\\nüîñ Most Common Tags:\")\n",
    "        sorted_tags = sorted(trace_analysis['tags'].items(), key=lambda x: x[1], reverse=True)\n",
    "        for tag, count in sorted_tags[:5]:\n",
    "            print(f\"  - {tag}: {count}\")\n",
    "\n",
    "print(\"\\n‚úÖ Observability deep dive completed! Check your LangSmith dashboard for detailed trace views.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Key Takeaways and Best Practices\n",
    "\n",
    "### ‚úÖ What You've Mastered\n",
    "\n",
    "1. **Advanced Tracing Patterns**:\n",
    "   - Dynamic metadata and run naming\n",
    "   - Custom instrumentation for non-LangChain components\n",
    "   - Context propagation across complex workflows\n",
    "\n",
    "2. **Agent Observability (2025 Feature)**:\n",
    "   - Tool usage analytics and performance insights\n",
    "   - Multi-step reasoning analysis\n",
    "   - Agent decision-making transparency\n",
    "\n",
    "3. **Debugging and Performance Analysis**:\n",
    "   - Error handling and retry logic tracing\n",
    "   - Performance pattern analysis\n",
    "   - Programmatic trace analysis\n",
    "\n",
    "### üéØ Best Practices for Production\n",
    "\n",
    "1. **Comprehensive Instrumentation**:\n",
    "   - Instrument all critical paths in your application\n",
    "   - Use meaningful run names and metadata\n",
    "   - Tag runs consistently for easy filtering\n",
    "\n",
    "2. **Performance Monitoring**:\n",
    "   - Track latency trends across different input types\n",
    "   - Monitor token usage and costs\n",
    "   - Set up alerts for error rate spikes\n",
    "\n",
    "3. **Agent Optimization**:\n",
    "   - Analyze tool usage patterns to optimize agent workflows\n",
    "   - Monitor tool call latencies and success rates\n",
    "   - Use intermediate steps for detailed debugging\n",
    "\n",
    "### üîß Advanced Tips\n",
    "\n",
    "- **Sampling**: Use trace sampling in high-volume environments\n",
    "- **Context**: Leverage context managers for complex instrumentation\n",
    "- **Metadata**: Store business metrics in run metadata for analysis\n",
    "- **Tags**: Use hierarchical tags for multi-dimensional filtering\n",
    "\n",
    "## üöÄ What's Next?\n",
    "\n",
    "You're now equipped with advanced observability skills! Continue to:\n",
    "\n",
    "- **LSM-004: Evaluation Mastery** - Build comprehensive testing pipelines\n",
    "- **LSM-005: Prompt Engineering** - Master collaborative prompt development\n",
    "- **LSM-006: Production Monitoring** - Set up enterprise-grade monitoring\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to build robust evaluation pipelines?** Continue to **LSM-004: Evaluation Mastery** to master systematic testing of your LLM applications! üß™"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}