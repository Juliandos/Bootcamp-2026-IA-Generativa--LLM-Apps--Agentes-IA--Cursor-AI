{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSM-007: Advanced Patterns - Complex Use Cases and Integrations\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Master complex multi-agent system observability\n",
    "- Implement advanced RAG pipeline monitoring\n",
    "- Build streaming response monitoring systems\n",
    "- Create custom LangSmith integrations and extensions\n",
    "- Handle enterprise-scale deployment patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Advanced Architecture Patterns\n",
    "\n",
    "In this section, we'll explore complex LangSmith patterns for enterprise applications:\n",
    "\n",
    "### ðŸ¤– Multi-Agent Systems\n",
    "- Agent orchestration and coordination\n",
    "- Inter-agent communication tracing\n",
    "- Performance optimization across agent networks\n",
    "\n",
    "### ðŸ” Advanced RAG Architectures\n",
    "- Multi-stage retrieval pipelines\n",
    "- Hybrid search and reranking\n",
    "- Knowledge graph integration\n",
    "\n",
    "### ðŸŒŠ Streaming and Real-time Processing\n",
    "- Streaming response monitoring\n",
    "- Real-time evaluation and feedback\n",
    "- Live performance optimization\n",
    "\n",
    "### ðŸ”— Custom Integrations\n",
    "- Building custom LangSmith tools\n",
    "- Third-party service integration\n",
    "- Custom evaluation frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Environment Setup\n",
    "\n",
    "Let's set up our advanced patterns environment with multiple frameworks and tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import asyncio\n",
    "from typing import Dict, List, Optional, Any, AsyncGenerator, Callable\n",
    "import json\n",
    "import uuid\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "import time\n",
    "import random\n",
    "from collections import defaultdict, deque\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import logging\n",
    "\n",
    "# LangSmith and LangChain imports\n",
    "from langsmith import Client, traceable, RunTree\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Vector store and retrieval\n",
    "try:\n",
    "    from langchain_community.vectorstores import FAISS\n",
    "    from langchain_community.document_loaders import TextLoader\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "except ImportError:\n",
    "    print(\"ðŸ“ Note: Some advanced features require additional dependencies\")\n",
    "    print(\"   Install with: pip install langchain-community faiss-cpu\")\n",
    "\n",
    "# Streaming support\n",
    "from langchain_core.callbacks import AsyncCallbackHandler\n",
    "from langchain_core.outputs import LLMResult, ChatGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure environment\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"advanced-patterns-demo\"\n",
    "\n",
    "# Initialize clients\n",
    "client = Client()\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"âœ… Advanced patterns environment configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– Multi-Agent System Observability\n",
    "\n",
    "Let's build a sophisticated multi-agent system with comprehensive observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentRole(Enum):\n",
    "    COORDINATOR = \"coordinator\"\n",
    "    RESEARCHER = \"researcher\"\n",
    "    ANALYST = \"analyst\"\n",
    "    WRITER = \"writer\"\n",
    "    REVIEWER = \"reviewer\"\n",
    "\n",
    "@dataclass\n",
    "class AgentTask:\n",
    "    task_id: str\n",
    "    agent_role: AgentRole\n",
    "    instruction: str\n",
    "    context: Dict[str, Any] = field(default_factory=dict)\n",
    "    dependencies: List[str] = field(default_factory=list)\n",
    "    priority: int = 1\n",
    "    max_retries: int = 3\n",
    "\n",
    "@dataclass\n",
    "class AgentResult:\n",
    "    task_id: str\n",
    "    agent_role: AgentRole\n",
    "    success: bool\n",
    "    result: Any\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    execution_time: float = 0.0\n",
    "    retry_count: int = 0\n",
    "\n",
    "class AdvancedAgent:\n",
    "    \"\"\"Advanced agent with comprehensive observability\"\"\"\n",
    "    \n",
    "    def __init__(self, role: AgentRole, llm: ChatOpenAI, system_prompt: str):\n",
    "        self.role = role\n",
    "        self.llm = llm\n",
    "        self.system_prompt = system_prompt\n",
    "        self.task_history = deque(maxlen=100)\n",
    "        self.performance_metrics = {\n",
    "            'total_tasks': 0,\n",
    "            'successful_tasks': 0,\n",
    "            'failed_tasks': 0,\n",
    "            'average_execution_time': 0.0,\n",
    "            'total_retries': 0\n",
    "        }\n",
    "    \n",
    "    @traceable(name=\"agent_execute_task\")\n",
    "    async def execute_task(self, task: AgentTask) -> AgentResult:\n",
    "        \"\"\"Execute a task with full observability\"\"\"\n",
    "        start_time = time.time()\n",
    "        retry_count = 0\n",
    "        \n",
    "        # Add task metadata to trace\n",
    "        current_run = RunTree.get_current_run()\n",
    "        if current_run:\n",
    "            current_run.add_metadata({\n",
    "                \"agent_role\": self.role.value,\n",
    "                \"task_id\": task.task_id,\n",
    "                \"priority\": task.priority,\n",
    "                \"dependencies\": task.dependencies\n",
    "            })\n",
    "        \n",
    "        while retry_count <= task.max_retries:\n",
    "            try:\n",
    "                # Create context-aware prompt\n",
    "                context_str = \"\\n\".join([f\"{k}: {v}\" for k, v in task.context.items()])\n",
    "                \n",
    "                messages = [\n",
    "                    SystemMessage(content=f\"{self.system_prompt}\\n\\nContext:\\n{context_str}\"),\n",
    "                    HumanMessage(content=task.instruction)\n",
    "                ]\n",
    "                \n",
    "                # Execute with agent-specific tracing\n",
    "                with traceable(name=f\"{self.role.value}_reasoning\") as run:\n",
    "                    response = await self.llm.ainvoke(messages)\n",
    "                    \n",
    "                    if run:\n",
    "                        run.add_metadata({\n",
    "                            \"retry_count\": retry_count,\n",
    "                            \"task_priority\": task.priority\n",
    "                        })\n",
    "                \n",
    "                execution_time = time.time() - start_time\n",
    "                \n",
    "                result = AgentResult(\n",
    "                    task_id=task.task_id,\n",
    "                    agent_role=self.role,\n",
    "                    success=True,\n",
    "                    result=response.content,\n",
    "                    metadata={\n",
    "                        \"model\": self.llm.model_name,\n",
    "                        \"tokens_used\": len(response.content.split()),\n",
    "                        \"context_size\": len(context_str)\n",
    "                    },\n",
    "                    execution_time=execution_time,\n",
    "                    retry_count=retry_count\n",
    "                )\n",
    "                \n",
    "                self._update_metrics(result)\n",
    "                self.task_history.append(result)\n",
    "                \n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                if retry_count > task.max_retries:\n",
    "                    execution_time = time.time() - start_time\n",
    "                    result = AgentResult(\n",
    "                        task_id=task.task_id,\n",
    "                        agent_role=self.role,\n",
    "                        success=False,\n",
    "                        result=f\"Failed after {retry_count-1} retries: {str(e)}\",\n",
    "                        execution_time=execution_time,\n",
    "                        retry_count=retry_count-1\n",
    "                    )\n",
    "                    \n",
    "                    self._update_metrics(result)\n",
    "                    self.task_history.append(result)\n",
    "                    return result\n",
    "                \n",
    "                await asyncio.sleep(2 ** retry_count)  # Exponential backoff\n",
    "    \n",
    "    def _update_metrics(self, result: AgentResult):\n",
    "        \"\"\"Update performance metrics\"\"\"\n",
    "        self.performance_metrics['total_tasks'] += 1\n",
    "        \n",
    "        if result.success:\n",
    "            self.performance_metrics['successful_tasks'] += 1\n",
    "        else:\n",
    "            self.performance_metrics['failed_tasks'] += 1\n",
    "        \n",
    "        self.performance_metrics['total_retries'] += result.retry_count\n",
    "        \n",
    "        # Update running average\n",
    "        total_time = (self.performance_metrics['average_execution_time'] * \n",
    "                     (self.performance_metrics['total_tasks'] - 1) + result.execution_time)\n",
    "        self.performance_metrics['average_execution_time'] = total_time / self.performance_metrics['total_tasks']\n",
    "    \n",
    "    def get_performance_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get agent performance summary\"\"\"\n",
    "        metrics = self.performance_metrics.copy()\n",
    "        if metrics['total_tasks'] > 0:\n",
    "            metrics['success_rate'] = metrics['successful_tasks'] / metrics['total_tasks']\n",
    "            metrics['failure_rate'] = metrics['failed_tasks'] / metrics['total_tasks']\n",
    "            metrics['average_retries'] = metrics['total_retries'] / metrics['total_tasks']\n",
    "        else:\n",
    "            metrics['success_rate'] = 0\n",
    "            metrics['failure_rate'] = 0\n",
    "            metrics['average_retries'] = 0\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "class MultiAgentOrchestrator:\n",
    "    \"\"\"Orchestrates multiple agents with advanced coordination\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.agents: Dict[AgentRole, AdvancedAgent] = {}\n",
    "        self.task_queue = asyncio.Queue()\n",
    "        self.results = {}\n",
    "        self.dependency_graph = {}\n",
    "        self.running = False\n",
    "    \n",
    "    def add_agent(self, agent: AdvancedAgent):\n",
    "        \"\"\"Add an agent to the orchestrator\"\"\"\n",
    "        self.agents[agent.role] = agent\n",
    "    \n",
    "    @traceable(name=\"multi_agent_coordination\")\n",
    "    async def execute_workflow(self, tasks: List[AgentTask]) -> Dict[str, AgentResult]:\n",
    "        \"\"\"Execute a complex multi-agent workflow\"\"\"\n",
    "        self.running = True\n",
    "        \n",
    "        # Build dependency graph\n",
    "        self._build_dependency_graph(tasks)\n",
    "        \n",
    "        # Sort tasks by priority and dependencies\n",
    "        sorted_tasks = self._topological_sort(tasks)\n",
    "        \n",
    "        # Execute tasks with dependency resolution\n",
    "        for task in sorted_tasks:\n",
    "            # Wait for dependencies\n",
    "            await self._wait_for_dependencies(task)\n",
    "            \n",
    "            # Find appropriate agent\n",
    "            agent = self.agents.get(task.agent_role)\n",
    "            if not agent:\n",
    "                logger.error(f\"No agent found for role {task.agent_role}\")\n",
    "                continue\n",
    "            \n",
    "            # Add dependency results to task context\n",
    "            for dep_id in task.dependencies:\n",
    "                if dep_id in self.results:\n",
    "                    task.context[f\"dependency_{dep_id}\"] = self.results[dep_id].result\n",
    "            \n",
    "            # Execute task\n",
    "            with traceable(name=f\"task_{task.task_id}\") as run:\n",
    "                result = await agent.execute_task(task)\n",
    "                self.results[task.task_id] = result\n",
    "                \n",
    "                if run:\n",
    "                    run.add_metadata({\n",
    "                        \"task_success\": result.success,\n",
    "                        \"execution_time\": result.execution_time,\n",
    "                        \"retry_count\": result.retry_count\n",
    "                    })\n",
    "        \n",
    "        self.running = False\n",
    "        return self.results\n",
    "    \n",
    "    def _build_dependency_graph(self, tasks: List[AgentTask]):\n",
    "        \"\"\"Build task dependency graph\"\"\"\n",
    "        self.dependency_graph = {task.task_id: task.dependencies for task in tasks}\n",
    "    \n",
    "    def _topological_sort(self, tasks: List[AgentTask]) -> List[AgentTask]:\n",
    "        \"\"\"Sort tasks respecting dependencies and priority\"\"\"\n",
    "        # Simple topological sort with priority\n",
    "        visited = set()\n",
    "        result = []\n",
    "        \n",
    "        def visit(task):\n",
    "            if task.task_id in visited:\n",
    "                return\n",
    "            \n",
    "            visited.add(task.task_id)\n",
    "            \n",
    "            # Visit dependencies first\n",
    "            for dep_id in task.dependencies:\n",
    "                dep_task = next((t for t in tasks if t.task_id == dep_id), None)\n",
    "                if dep_task:\n",
    "                    visit(dep_task)\n",
    "            \n",
    "            result.append(task)\n",
    "        \n",
    "        # Sort by priority first\n",
    "        tasks_by_priority = sorted(tasks, key=lambda t: t.priority, reverse=True)\n",
    "        \n",
    "        for task in tasks_by_priority:\n",
    "            visit(task)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    async def _wait_for_dependencies(self, task: AgentTask):\n",
    "        \"\"\"Wait for task dependencies to complete\"\"\"\n",
    "        while any(dep_id not in self.results for dep_id in task.dependencies):\n",
    "            await asyncio.sleep(0.1)\n",
    "    \n",
    "    def get_orchestrator_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get overall orchestrator performance metrics\"\"\"\n",
    "        agent_metrics = {}\n",
    "        total_tasks = 0\n",
    "        total_successful = 0\n",
    "        total_execution_time = 0\n",
    "        \n",
    "        for role, agent in self.agents.items():\n",
    "            metrics = agent.get_performance_summary()\n",
    "            agent_metrics[role.value] = metrics\n",
    "            \n",
    "            total_tasks += metrics['total_tasks']\n",
    "            total_successful += metrics['successful_tasks']\n",
    "            total_execution_time += metrics['average_execution_time'] * metrics['total_tasks']\n",
    "        \n",
    "        return {\n",
    "            'agent_metrics': agent_metrics,\n",
    "            'total_tasks': total_tasks,\n",
    "            'overall_success_rate': total_successful / total_tasks if total_tasks > 0 else 0,\n",
    "            'average_execution_time': total_execution_time / total_tasks if total_tasks > 0 else 0,\n",
    "            'active_agents': len(self.agents),\n",
    "            'completed_workflows': len(self.results)\n",
    "        }\n",
    "\n",
    "# Create specialized agents\n",
    "agents = {\n",
    "    AgentRole.COORDINATOR: AdvancedAgent(\n",
    "        AgentRole.COORDINATOR,\n",
    "        llm,\n",
    "        \"You are a coordination agent. Break down complex tasks into manageable subtasks and coordinate their execution.\"\n",
    "    ),\n",
    "    AgentRole.RESEARCHER: AdvancedAgent(\n",
    "        AgentRole.RESEARCHER,\n",
    "        llm,\n",
    "        \"You are a research agent. Gather information, analyze data, and provide comprehensive research summaries.\"\n",
    "    ),\n",
    "    AgentRole.ANALYST: AdvancedAgent(\n",
    "        AgentRole.ANALYST,\n",
    "        llm,\n",
    "        \"You are an analysis agent. Analyze data, identify patterns, and provide insights and recommendations.\"\n",
    "    ),\n",
    "    AgentRole.WRITER: AdvancedAgent(\n",
    "        AgentRole.WRITER,\n",
    "        llm,\n",
    "        \"You are a writing agent. Create clear, engaging, and well-structured written content based on provided information.\"\n",
    "    ),\n",
    "    AgentRole.REVIEWER: AdvancedAgent(\n",
    "        AgentRole.REVIEWER,\n",
    "        llm,\n",
    "        \"You are a review agent. Critically evaluate content for quality, accuracy, and completeness, providing constructive feedback.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Initialize orchestrator\n",
    "orchestrator = MultiAgentOrchestrator()\n",
    "for agent in agents.values():\n",
    "    orchestrator.add_agent(agent)\n",
    "\n",
    "print(\"ðŸ¤– Multi-agent system initialized with 5 specialized agents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Multi-Agent Workflow Demo\n",
    "\n",
    "Let's create a complex workflow that demonstrates agent coordination and observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complex workflow: Market research report generation\n",
    "workflow_tasks = [\n",
    "    AgentTask(\n",
    "        task_id=\"coord_001\",\n",
    "        agent_role=AgentRole.COORDINATOR,\n",
    "        instruction=\"Plan a comprehensive market research report for electric vehicles in 2025. Break this down into research areas and analysis requirements.\",\n",
    "        priority=5\n",
    "    ),\n",
    "    \n",
    "    AgentTask(\n",
    "        task_id=\"research_001\", \n",
    "        agent_role=AgentRole.RESEARCHER,\n",
    "        instruction=\"Research current electric vehicle market trends, major players, and technological developments.\",\n",
    "        dependencies=[\"coord_001\"],\n",
    "        priority=4\n",
    "    ),\n",
    "    \n",
    "    AgentTask(\n",
    "        task_id=\"research_002\",\n",
    "        agent_role=AgentRole.RESEARCHER, \n",
    "        instruction=\"Research consumer sentiment, adoption barriers, and government policies affecting electric vehicles.\",\n",
    "        dependencies=[\"coord_001\"],\n",
    "        priority=4\n",
    "    ),\n",
    "    \n",
    "    AgentTask(\n",
    "        task_id=\"analysis_001\",\n",
    "        agent_role=AgentRole.ANALYST,\n",
    "        instruction=\"Analyze market trends and competitive landscape data to identify opportunities and threats.\",\n",
    "        dependencies=[\"research_001\", \"research_002\"],\n",
    "        priority=3\n",
    "    ),\n",
    "    \n",
    "    AgentTask(\n",
    "        task_id=\"write_001\",\n",
    "        agent_role=AgentRole.WRITER,\n",
    "        instruction=\"Write a comprehensive market research report executive summary based on the research and analysis.\",\n",
    "        dependencies=[\"analysis_001\"],\n",
    "        priority=2\n",
    "    ),\n",
    "    \n",
    "    AgentTask(\n",
    "        task_id=\"review_001\",\n",
    "        agent_role=AgentRole.REVIEWER,\n",
    "        instruction=\"Review the market research report for accuracy, completeness, and professional quality. Provide improvement suggestions.\",\n",
    "        dependencies=[\"write_001\"],\n",
    "        priority=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"ðŸš€ Starting complex multi-agent workflow...\")\n",
    "print(f\"ðŸ“Š Workflow contains {len(workflow_tasks)} interdependent tasks\")\n",
    "\n",
    "# Execute workflow with full observability\n",
    "workflow_results = await orchestrator.execute_workflow(workflow_tasks)\n",
    "\n",
    "print(\"\\nâœ… Workflow completed successfully!\")\n",
    "print(f\"ðŸ“ˆ Processed {len(workflow_results)} tasks\")\n",
    "\n",
    "# Display workflow results\n",
    "for task_id, result in workflow_results.items():\n",
    "    status = \"âœ… SUCCESS\" if result.success else \"âŒ FAILED\"\n",
    "    print(f\"\\n{status} | {result.agent_role.value.title()} | {task_id}\")\n",
    "    print(f\"   â±ï¸  Execution time: {result.execution_time:.2f}s\")\n",
    "    print(f\"   ðŸ”„ Retries: {result.retry_count}\")\n",
    "    if result.success:\n",
    "        preview = result.result[:150] + \"...\" if len(result.result) > 150 else result.result\n",
    "        print(f\"   ðŸ“ Result preview: {preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Advanced RAG Pipeline Monitoring\n",
    "\n",
    "Let's build a sophisticated RAG system with multi-stage retrieval and comprehensive monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "@dataclass\n",
    "class RetrievalMetrics:\n",
    "    query: str\n",
    "    stage: str\n",
    "    retrieved_count: int\n",
    "    retrieval_time: float\n",
    "    relevance_scores: List[float]\n",
    "    average_relevance: float\n",
    "    diversity_score: float\n",
    "\n",
    "@dataclass \n",
    "class RAGMetrics:\n",
    "    query: str\n",
    "    retrieval_metrics: List[RetrievalMetrics]\n",
    "    generation_time: float\n",
    "    total_time: float\n",
    "    context_length: int\n",
    "    response_quality: Optional[float] = None\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    \"\"\"Advanced RAG pipeline with multi-stage retrieval and monitoring\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, embeddings, llm):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.embeddings = embeddings\n",
    "        self.llm = llm\n",
    "        self.metrics_history = deque(maxlen=1000)\n",
    "        \n",
    "        # Quality evaluator\n",
    "        self.quality_evaluator = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "        \n",
    "    @traceable(name=\"advanced_rag_pipeline\")\n",
    "    async def query(self, question: str, top_k: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Execute multi-stage RAG query with comprehensive monitoring\"\"\"\n",
    "        start_time = time.time()\n",
    "        retrieval_metrics = []\n",
    "        \n",
    "        # Stage 1: Initial vector retrieval\n",
    "        stage1_metrics = await self._vector_retrieval_stage(\n",
    "            question, top_k * 2, \"initial_vector_search\"\n",
    "        )\n",
    "        retrieval_metrics.append(stage1_metrics)\n",
    "        initial_docs = stage1_metrics.retrieved_count\n",
    "        \n",
    "        # Stage 2: Query expansion and re-retrieval\n",
    "        expanded_query = await self._expand_query(question)\n",
    "        stage2_metrics = await self._vector_retrieval_stage(\n",
    "            expanded_query, top_k, \"expanded_query_search\"\n",
    "        )\n",
    "        retrieval_metrics.append(stage2_metrics)\n",
    "        \n",
    "        # Stage 3: Reranking (simulated)\n",
    "        stage3_metrics = await self._reranking_stage(\n",
    "            question, stage2_metrics, \"semantic_reranking\"\n",
    "        )\n",
    "        retrieval_metrics.append(stage3_metrics)\n",
    "        \n",
    "        # Get final context\n",
    "        context_docs = await self._get_final_context(question, top_k)\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in context_docs])\n",
    "        \n",
    "        # Generation stage\n",
    "        generation_start = time.time()\n",
    "        response = await self._generate_response(question, context)\n",
    "        generation_time = time.time() - generation_start\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate response quality\n",
    "        quality_score = await self._evaluate_response_quality(\n",
    "            question, context, response\n",
    "        )\n",
    "        \n",
    "        # Create comprehensive metrics\n",
    "        metrics = RAGMetrics(\n",
    "            query=question,\n",
    "            retrieval_metrics=retrieval_metrics,\n",
    "            generation_time=generation_time,\n",
    "            total_time=total_time,\n",
    "            context_length=len(context),\n",
    "            response_quality=quality_score\n",
    "        )\n",
    "        \n",
    "        self.metrics_history.append(metrics)\n",
    "        \n",
    "        # Add metrics to trace\n",
    "        current_run = RunTree.get_current_run()\n",
    "        if current_run:\n",
    "            current_run.add_metadata({\n",
    "                \"retrieval_stages\": len(retrieval_metrics),\n",
    "                \"total_retrieved_docs\": sum(m.retrieved_count for m in retrieval_metrics),\n",
    "                \"average_relevance\": np.mean([m.average_relevance for m in retrieval_metrics]),\n",
    "                \"context_length\": len(context),\n",
    "                \"generation_time\": generation_time,\n",
    "                \"total_time\": total_time,\n",
    "                \"response_quality\": quality_score\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'answer': response,\n",
    "            'context': context,\n",
    "            'source_documents': context_docs,\n",
    "            'metrics': metrics,\n",
    "            'expanded_query': expanded_query\n",
    "        }\n",
    "    \n",
    "    @traceable(name=\"vector_retrieval_stage\")\n",
    "    async def _vector_retrieval_stage(self, query: str, k: int, stage_name: str) -> RetrievalMetrics:\n",
    "        \"\"\"Execute vector retrieval stage with monitoring\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Simulate vector search (in real implementation, use actual vectorstore)\n",
    "        docs = await self._simulate_vector_search(query, k)\n",
    "        retrieval_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate relevance scores (simulated)\n",
    "        relevance_scores = [random.uniform(0.6, 0.95) for _ in docs]\n",
    "        average_relevance = np.mean(relevance_scores) if relevance_scores else 0\n",
    "        \n",
    "        # Calculate diversity score (simulated)\n",
    "        diversity_score = random.uniform(0.7, 0.9)\n",
    "        \n",
    "        return RetrievalMetrics(\n",
    "            query=query,\n",
    "            stage=stage_name,\n",
    "            retrieved_count=len(docs),\n",
    "            retrieval_time=retrieval_time,\n",
    "            relevance_scores=relevance_scores,\n",
    "            average_relevance=average_relevance,\n",
    "            diversity_score=diversity_score\n",
    "        )\n",
    "    \n",
    "    @traceable(name=\"query_expansion\")\n",
    "    async def _expand_query(self, original_query: str) -> str:\n",
    "        \"\"\"Expand query for better retrieval\"\"\"\n",
    "        expansion_prompt = f\"\"\"\n",
    "        Given this search query, generate 2-3 alternative phrasings or related terms \n",
    "        that would help retrieve relevant information. Combine them with the original query.\n",
    "        \n",
    "        Original query: {original_query}\n",
    "        \n",
    "        Respond with only the expanded query:\n",
    "        \"\"\"\n",
    "        \n",
    "        response = await self.llm.ainvoke([HumanMessage(content=expansion_prompt)])\n",
    "        return response.content.strip()\n",
    "    \n",
    "    @traceable(name=\"semantic_reranking\")\n",
    "    async def _reranking_stage(self, query: str, prev_metrics: RetrievalMetrics, stage_name: str) -> RetrievalMetrics:\n",
    "        \"\"\"Simulate semantic reranking stage\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Simulate reranking (improve relevance scores)\n",
    "        improved_scores = [min(score * 1.1, 1.0) for score in prev_metrics.relevance_scores]\n",
    "        retrieval_time = time.time() - start_time\n",
    "        \n",
    "        return RetrievalMetrics(\n",
    "            query=query,\n",
    "            stage=stage_name,\n",
    "            retrieved_count=prev_metrics.retrieved_count,\n",
    "            retrieval_time=retrieval_time,\n",
    "            relevance_scores=improved_scores,\n",
    "            average_relevance=np.mean(improved_scores),\n",
    "            diversity_score=prev_metrics.diversity_score * 1.05\n",
    "        )\n",
    "    \n",
    "    async def _simulate_vector_search(self, query: str, k: int) -> List[Document]:\n",
    "        \"\"\"Simulate vector search results\"\"\"\n",
    "        # In real implementation, this would be: self.vectorstore.similarity_search(query, k=k)\n",
    "        docs = []\n",
    "        for i in range(min(k, 10)):\n",
    "            docs.append(Document(\n",
    "                page_content=f\"Relevant document {i+1} content related to: {query[:50]}...\",\n",
    "                metadata={\"source\": f\"doc_{i+1}.txt\", \"relevance_score\": random.uniform(0.6, 0.95)}\n",
    "            ))\n",
    "        return docs\n",
    "    \n",
    "    async def _get_final_context(self, query: str, k: int) -> List[Document]:\n",
    "        \"\"\"Get final context documents after all retrieval stages\"\"\"\n",
    "        return await self._simulate_vector_search(query, k)\n",
    "    \n",
    "    @traceable(name=\"rag_generation\")\n",
    "    async def _generate_response(self, question: str, context: str) -> str:\n",
    "        \"\"\"Generate response using retrieved context\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Answer the following question based on the provided context. \n",
    "        Be accurate, comprehensive, and cite relevant information from the context.\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        \n",
    "        response = await self.llm.ainvoke([HumanMessage(content=prompt)])\n",
    "        return response.content\n",
    "    \n",
    "    @traceable(name=\"quality_evaluation\")\n",
    "    async def _evaluate_response_quality(self, question: str, context: str, response: str) -> float:\n",
    "        \"\"\"Evaluate response quality using LLM-as-judge\"\"\"\n",
    "        eval_prompt = f\"\"\"\n",
    "        Evaluate the quality of this RAG system response on a scale of 0-1.\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Context: {context[:500]}...\n",
    "        \n",
    "        Response: {response}\n",
    "        \n",
    "        Consider:\n",
    "        - Relevance to the question\n",
    "        - Use of provided context\n",
    "        - Accuracy and completeness\n",
    "        - Clarity and coherence\n",
    "        \n",
    "        Respond with only a number between 0 and 1:\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            result = await self.quality_evaluator.ainvoke([HumanMessage(content=eval_prompt)])\n",
    "            score = float(result.content.strip())\n",
    "            return max(0, min(1, score))\n",
    "        except:\n",
    "            return 0.75  # Default score if evaluation fails\n",
    "    \n",
    "    def get_pipeline_analytics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive pipeline analytics\"\"\"\n",
    "        if not self.metrics_history:\n",
    "            return {}\n",
    "        \n",
    "        # Overall metrics\n",
    "        avg_total_time = np.mean([m.total_time for m in self.metrics_history])\n",
    "        avg_generation_time = np.mean([m.generation_time for m in self.metrics_history])\n",
    "        avg_context_length = np.mean([m.context_length for m in self.metrics_history])\n",
    "        avg_quality = np.mean([m.response_quality for m in self.metrics_history if m.response_quality])\n",
    "        \n",
    "        # Retrieval stage analytics\n",
    "        stage_analytics = defaultdict(list)\n",
    "        for metrics in self.metrics_history:\n",
    "            for r_metrics in metrics.retrieval_metrics:\n",
    "                stage_analytics[r_metrics.stage].append({\n",
    "                    'retrieval_time': r_metrics.retrieval_time,\n",
    "                    'retrieved_count': r_metrics.retrieved_count,\n",
    "                    'average_relevance': r_metrics.average_relevance,\n",
    "                    'diversity_score': r_metrics.diversity_score\n",
    "                })\n",
    "        \n",
    "        stage_summary = {}\n",
    "        for stage, data in stage_analytics.items():\n",
    "            stage_summary[stage] = {\n",
    "                'avg_retrieval_time': np.mean([d['retrieval_time'] for d in data]),\n",
    "                'avg_retrieved_count': np.mean([d['retrieved_count'] for d in data]),\n",
    "                'avg_relevance': np.mean([d['average_relevance'] for d in data]),\n",
    "                'avg_diversity': np.mean([d['diversity_score'] for d in data])\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'total_queries': len(self.metrics_history),\n",
    "            'avg_total_time': avg_total_time,\n",
    "            'avg_generation_time': avg_generation_time,\n",
    "            'avg_context_length': avg_context_length,\n",
    "            'avg_response_quality': avg_quality,\n",
    "            'retrieval_stages': stage_summary,\n",
    "            'performance_trend': self._calculate_performance_trend()\n",
    "        }\n",
    "    \n",
    "    def _calculate_performance_trend(self) -> Dict[str, str]:\n",
    "        \"\"\"Calculate performance trends over recent queries\"\"\"\n",
    "        if len(self.metrics_history) < 10:\n",
    "            return {\"trend\": \"insufficient_data\"}\n",
    "        \n",
    "        recent = list(self.metrics_history)[-10:]\n",
    "        older = list(self.metrics_history)[-20:-10]\n",
    "        \n",
    "        recent_quality = np.mean([m.response_quality for m in recent if m.response_quality])\n",
    "        older_quality = np.mean([m.response_quality for m in older if m.response_quality])\n",
    "        \n",
    "        recent_time = np.mean([m.total_time for m in recent])\n",
    "        older_time = np.mean([m.total_time for m in older])\n",
    "        \n",
    "        quality_trend = \"improving\" if recent_quality > older_quality else \"declining\"\n",
    "        speed_trend = \"faster\" if recent_time < older_time else \"slower\"\n",
    "        \n",
    "        return {\n",
    "            \"quality_trend\": quality_trend,\n",
    "            \"speed_trend\": speed_trend,\n",
    "            \"quality_change\": f\"{((recent_quality - older_quality) / older_quality * 100):.1f}%\",\n",
    "            \"speed_change\": f\"{((older_time - recent_time) / older_time * 100):.1f}%\"\n",
    "        }\n",
    "\n",
    "# Initialize RAG pipeline\n",
    "# In a real implementation, you would load actual documents and create a real vectorstore\n",
    "rag_pipeline = AdvancedRAGPipeline(\n",
    "    vectorstore=None,  # Would be actual FAISS or similar\n",
    "    embeddings=embeddings,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "print(\"ðŸ” Advanced RAG pipeline initialized with multi-stage retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª RAG Pipeline Demo\n",
    "\n",
    "Let's test our advanced RAG pipeline with sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries for RAG pipeline\n",
    "test_queries = [\n",
    "    \"What are the main advantages of electric vehicles over traditional cars?\",\n",
    "    \"How does battery technology affect electric vehicle performance?\",\n",
    "    \"What are the infrastructure challenges for electric vehicle adoption?\",\n",
    "    \"Compare the total cost of ownership between electric and gasoline vehicles.\",\n",
    "    \"What government incentives exist for electric vehicle purchases?\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ” Testing Advanced RAG Pipeline...\")\n",
    "print(f\"ðŸ“Š Processing {len(test_queries)} test queries\\n\")\n",
    "\n",
    "# Process queries\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\nðŸ“ Query {i}: {query}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    result = await rag_pipeline.query(query)\n",
    "    metrics = result['metrics']\n",
    "    \n",
    "    # Display metrics\n",
    "    print(f\"â±ï¸  Total time: {metrics.total_time:.2f}s\")\n",
    "    print(f\"ðŸ”„ Retrieval stages: {len(metrics.retrieval_metrics)}\")\n",
    "    print(f\"ðŸ“Š Context length: {metrics.context_length} chars\")\n",
    "    print(f\"â­ Quality score: {metrics.response_quality:.2f}\")\n",
    "    \n",
    "    # Display retrieval stage breakdown\n",
    "    print(\"\\nðŸ” Retrieval Stages:\")\n",
    "    for r_metrics in metrics.retrieval_metrics:\n",
    "        print(f\"  â€¢ {r_metrics.stage}: {r_metrics.retrieved_count} docs, \"\n",
    "              f\"relevance: {r_metrics.average_relevance:.2f}, \"\n",
    "              f\"time: {r_metrics.retrieval_time:.3f}s\")\n",
    "    \n",
    "    # Display answer preview\n",
    "    answer_preview = result['answer'][:200] + \"...\" if len(result['answer']) > 200 else result['answer']\n",
    "    print(f\"\\nðŸ’¬ Answer preview: {answer_preview}\")\n",
    "    \n",
    "    print(f\"ðŸ” Expanded query: {result['expanded_query']}\")\n",
    "\n",
    "# Display pipeline analytics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ˆ RAG PIPELINE ANALYTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "analytics = rag_pipeline.get_pipeline_analytics()\n",
    "\n",
    "print(f\"\\nðŸ“Š Overall Performance:\")\n",
    "print(f\"  Total queries processed: {analytics['total_queries']}\")\n",
    "print(f\"  Average total time: {analytics['avg_total_time']:.2f}s\")\n",
    "print(f\"  Average generation time: {analytics['avg_generation_time']:.2f}s\")\n",
    "print(f\"  Average context length: {analytics['avg_context_length']:.0f} chars\")\n",
    "print(f\"  Average response quality: {analytics['avg_response_quality']:.2f}\")\n",
    "\n",
    "print(f\"\\nðŸ” Retrieval Stage Performance:\")\n",
    "for stage, metrics in analytics['retrieval_stages'].items():\n",
    "    print(f\"  {stage}:\")\n",
    "    print(f\"    â€¢ Avg retrieval time: {metrics['avg_retrieval_time']:.3f}s\")\n",
    "    print(f\"    â€¢ Avg documents: {metrics['avg_retrieved_count']:.1f}\")\n",
    "    print(f\"    â€¢ Avg relevance: {metrics['avg_relevance']:.2f}\")\n",
    "    print(f\"    â€¢ Avg diversity: {metrics['avg_diversity']:.2f}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Performance Trends:\")\n",
    "trends = analytics['performance_trend']\n",
    "if 'quality_trend' in trends:\n",
    "    print(f\"  Quality trend: {trends['quality_trend']} ({trends['quality_change']})\")\n",
    "    print(f\"  Speed trend: {trends['speed_trend']} ({trends['speed_change']})\")\n",
    "else:\n",
    "    print(f\"  {trends['trend']}\")\n",
    "\n",
    "print(\"\\nâœ… RAG pipeline analysis completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒŠ Streaming Response Monitoring\n",
    "\n",
    "Let's implement advanced streaming response monitoring with real-time quality assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import AsyncGenerator\n",
    "import re\n",
    "\n",
    "@dataclass\n",
    "class StreamingMetrics:\n",
    "    chunk_count: int = 0\n",
    "    total_tokens: int = 0\n",
    "    first_chunk_latency: float = 0\n",
    "    total_streaming_time: float = 0\n",
    "    average_chunk_size: float = 0\n",
    "    streaming_rate: float = 0  # tokens per second\n",
    "    coherence_score: float = 0\n",
    "    partial_quality_scores: List[float] = field(default_factory=list)\n",
    "\n",
    "class StreamingMonitor:\n",
    "    \"\"\"Advanced streaming response monitoring\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: ChatOpenAI):\n",
    "        self.llm = llm\n",
    "        self.quality_evaluator = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "        self.streaming_history = deque(maxlen=100)\n",
    "    \n",
    "    @traceable(name=\"streaming_response_with_monitoring\")\n",
    "    async def stream_with_monitoring(self, prompt: str, \n",
    "                                   quality_check_interval: int = 50) -> AsyncGenerator[Dict[str, Any], None]:\n",
    "        \"\"\"Stream response with real-time monitoring and quality assessment\"\"\"\n",
    "        start_time = time.time()\n",
    "        metrics = StreamingMetrics()\n",
    "        accumulated_response = \"\"\n",
    "        first_chunk_received = False\n",
    "        \n",
    "        # Add metadata to trace\n",
    "        current_run = RunTree.get_current_run()\n",
    "        if current_run:\n",
    "            current_run.add_metadata({\n",
    "                \"streaming_enabled\": True,\n",
    "                \"quality_check_interval\": quality_check_interval,\n",
    "                \"model\": self.llm.model_name\n",
    "            })\n",
    "        \n",
    "        # Create streaming request\n",
    "        messages = [HumanMessage(content=prompt)]\n",
    "        \n",
    "        try:\n",
    "            # Simulate streaming (in real implementation, use llm.astream)\n",
    "            async for chunk in self._simulate_streaming_response(prompt):\n",
    "                chunk_time = time.time()\n",
    "                \n",
    "                if not first_chunk_received:\n",
    "                    metrics.first_chunk_latency = chunk_time - start_time\n",
    "                    first_chunk_received = True\n",
    "                \n",
    "                # Process chunk\n",
    "                chunk_content = chunk.get('content', '')\n",
    "                if chunk_content:\n",
    "                    metrics.chunk_count += 1\n",
    "                    chunk_tokens = len(chunk_content.split())\n",
    "                    metrics.total_tokens += chunk_tokens\n",
    "                    accumulated_response += chunk_content\n",
    "                    \n",
    "                    # Calculate streaming metrics\n",
    "                    current_time = chunk_time - start_time\n",
    "                    if current_time > 0:\n",
    "                        metrics.streaming_rate = metrics.total_tokens / current_time\n",
    "                    \n",
    "                    # Periodic quality assessment\n",
    "                    if (metrics.total_tokens % quality_check_interval == 0 and \n",
    "                        len(accumulated_response) > 100):\n",
    "                        \n",
    "                        quality_score = await self._assess_partial_quality(\n",
    "                            prompt, accumulated_response\n",
    "                        )\n",
    "                        metrics.partial_quality_scores.append(quality_score)\n",
    "                    \n",
    "                    # Real-time coherence check\n",
    "                    coherence = self._calculate_coherence_score(accumulated_response)\n",
    "                    metrics.coherence_score = coherence\n",
    "                    \n",
    "                    # Yield chunk with metrics\n",
    "                    yield {\n",
    "                        'type': 'chunk',\n",
    "                        'content': chunk_content,\n",
    "                        'accumulated_response': accumulated_response,\n",
    "                        'metrics': {\n",
    "                            'chunk_count': metrics.chunk_count,\n",
    "                            'total_tokens': metrics.total_tokens,\n",
    "                            'streaming_rate': metrics.streaming_rate,\n",
    "                            'coherence_score': coherence,\n",
    "                            'latest_quality_score': metrics.partial_quality_scores[-1] if metrics.partial_quality_scores else None\n",
    "                        }\n",
    "                    }\n",
    "        \n",
    "        except Exception as e:\n",
    "            yield {\n",
    "                'type': 'error',\n",
    "                'error': str(e),\n",
    "                'partial_response': accumulated_response,\n",
    "                'metrics': metrics\n",
    "            }\n",
    "            return\n",
    "        \n",
    "        # Final metrics calculation\n",
    "        metrics.total_streaming_time = time.time() - start_time\n",
    "        metrics.average_chunk_size = metrics.total_tokens / max(metrics.chunk_count, 1)\n",
    "        \n",
    "        # Final quality assessment\n",
    "        final_quality = await self._assess_final_quality(prompt, accumulated_response)\n",
    "        \n",
    "        # Store metrics\n",
    "        self.streaming_history.append({\n",
    "            'prompt': prompt,\n",
    "            'response': accumulated_response,\n",
    "            'metrics': metrics,\n",
    "            'final_quality': final_quality,\n",
    "            'timestamp': datetime.now()\n",
    "        })\n",
    "        \n",
    "        # Add final metrics to trace\n",
    "        if current_run:\n",
    "            current_run.add_metadata({\n",
    "                \"streaming_completed\": True,\n",
    "                \"total_chunks\": metrics.chunk_count,\n",
    "                \"total_tokens\": metrics.total_tokens,\n",
    "                \"first_chunk_latency\": metrics.first_chunk_latency,\n",
    "                \"total_streaming_time\": metrics.total_streaming_time,\n",
    "                \"streaming_rate\": metrics.streaming_rate,\n",
    "                \"final_quality\": final_quality,\n",
    "                \"coherence_score\": metrics.coherence_score\n",
    "            })\n",
    "        \n",
    "        # Yield completion event\n",
    "        yield {\n",
    "            'type': 'complete',\n",
    "            'final_response': accumulated_response,\n",
    "            'final_metrics': {\n",
    "                'chunk_count': metrics.chunk_count,\n",
    "                'total_tokens': metrics.total_tokens,\n",
    "                'first_chunk_latency': metrics.first_chunk_latency,\n",
    "                'total_streaming_time': metrics.total_streaming_time,\n",
    "                'average_chunk_size': metrics.average_chunk_size,\n",
    "                'streaming_rate': metrics.streaming_rate,\n",
    "                'coherence_score': metrics.coherence_score,\n",
    "                'partial_quality_scores': metrics.partial_quality_scores,\n",
    "                'final_quality': final_quality\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    async def _simulate_streaming_response(self, prompt: str) -> AsyncGenerator[Dict[str, str], None]:\n",
    "        \"\"\"Simulate streaming response (replace with real streaming in production)\"\"\"\n",
    "        # In real implementation: async for chunk in self.llm.astream(messages):\n",
    "        \n",
    "        # Generate a sample response\n",
    "        full_response = await self.llm.ainvoke([HumanMessage(content=prompt)])\n",
    "        response_text = full_response.content\n",
    "        \n",
    "        # Simulate chunked streaming\n",
    "        words = response_text.split()\n",
    "        chunk_size = 3  # words per chunk\n",
    "        \n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk_words = words[i:i+chunk_size]\n",
    "            chunk_content = ' '.join(chunk_words)\n",
    "            \n",
    "            # Add space if not the last chunk\n",
    "            if i + chunk_size < len(words):\n",
    "                chunk_content += ' '\n",
    "            \n",
    "            yield {'content': chunk_content}\n",
    "            \n",
    "            # Simulate network latency\n",
    "            await asyncio.sleep(0.1)\n",
    "    \n",
    "    @traceable(name=\"partial_quality_assessment\")\n",
    "    async def _assess_partial_quality(self, prompt: str, partial_response: str) -> float:\n",
    "        \"\"\"Assess quality of partial response\"\"\"\n",
    "        if len(partial_response) < 50:\n",
    "            return 0.5  # Not enough content to assess\n",
    "        \n",
    "        eval_prompt = f\"\"\"\n",
    "        Assess the quality of this partial response to the given prompt on a scale of 0-1.\n",
    "        Consider coherence, relevance, and how well it's developing so far.\n",
    "        \n",
    "        Prompt: {prompt}\n",
    "        \n",
    "        Partial Response: {partial_response}\n",
    "        \n",
    "        Respond with only a number between 0 and 1:\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            result = await self.quality_evaluator.ainvoke([HumanMessage(content=eval_prompt)])\n",
    "            score = float(result.content.strip())\n",
    "            return max(0, min(1, score))\n",
    "        except:\n",
    "            return 0.7  # Default if assessment fails\n",
    "    \n",
    "    async def _assess_final_quality(self, prompt: str, response: str) -> float:\n",
    "        \"\"\"Assess quality of complete response\"\"\"\n",
    "        eval_prompt = f\"\"\"\n",
    "        Evaluate the overall quality of this complete response on a scale of 0-1.\n",
    "        \n",
    "        Prompt: {prompt}\n",
    "        \n",
    "        Response: {response}\n",
    "        \n",
    "        Consider completeness, accuracy, clarity, and relevance.\n",
    "        Respond with only a number between 0 and 1:\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            result = await self.quality_evaluator.ainvoke([HumanMessage(content=eval_prompt)])\n",
    "            score = float(result.content.strip())\n",
    "            return max(0, min(1, score))\n",
    "        except:\n",
    "            return 0.75\n",
    "    \n",
    "    def _calculate_coherence_score(self, text: str) -> float:\n",
    "        \"\"\"Calculate coherence score based on text analysis\"\"\"\n",
    "        if len(text) < 100:\n",
    "            return 0.5\n",
    "        \n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        if len(sentences) < 2:\n",
    "            return 0.6\n",
    "        \n",
    "        # Simple coherence metrics\n",
    "        avg_sentence_length = np.mean([len(s.split()) for s in sentences])\n",
    "        sentence_length_std = np.std([len(s.split()) for s in sentences])\n",
    "        \n",
    "        # Reasonable sentence length indicates coherence\n",
    "        length_score = 1.0 - min(abs(avg_sentence_length - 15) / 15, 1.0)\n",
    "        \n",
    "        # Lower variance in sentence length indicates better flow\n",
    "        variance_score = 1.0 - min(sentence_length_std / avg_sentence_length, 1.0)\n",
    "        \n",
    "        return (length_score + variance_score) / 2\n",
    "    \n",
    "    def get_streaming_analytics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive streaming analytics\"\"\"\n",
    "        if not self.streaming_history:\n",
    "            return {}\n",
    "        \n",
    "        history = list(self.streaming_history)\n",
    "        \n",
    "        # Overall metrics\n",
    "        avg_first_chunk_latency = np.mean([h['metrics'].first_chunk_latency for h in history])\n",
    "        avg_streaming_rate = np.mean([h['metrics'].streaming_rate for h in history])\n",
    "        avg_total_time = np.mean([h['metrics'].total_streaming_time for h in history])\n",
    "        avg_chunk_count = np.mean([h['metrics'].chunk_count for h in history])\n",
    "        avg_final_quality = np.mean([h['final_quality'] for h in history])\n",
    "        avg_coherence = np.mean([h['metrics'].coherence_score for h in history])\n",
    "        \n",
    "        # Quality progression analysis\n",
    "        quality_progressions = [h['metrics'].partial_quality_scores for h in history if h['metrics'].partial_quality_scores]\n",
    "        avg_quality_improvement = 0\n",
    "        if quality_progressions:\n",
    "            improvements = []\n",
    "            for progression in quality_progressions:\n",
    "                if len(progression) > 1:\n",
    "                    improvement = progression[-1] - progression[0]\n",
    "                    improvements.append(improvement)\n",
    "            avg_quality_improvement = np.mean(improvements) if improvements else 0\n",
    "        \n",
    "        return {\n",
    "            'total_streaming_sessions': len(history),\n",
    "            'avg_first_chunk_latency': avg_first_chunk_latency,\n",
    "            'avg_streaming_rate': avg_streaming_rate,\n",
    "            'avg_total_time': avg_total_time,\n",
    "            'avg_chunk_count': avg_chunk_count,\n",
    "            'avg_final_quality': avg_final_quality,\n",
    "            'avg_coherence_score': avg_coherence,\n",
    "            'avg_quality_improvement': avg_quality_improvement,\n",
    "            'streaming_efficiency': avg_streaming_rate / avg_total_time if avg_total_time > 0 else 0\n",
    "        }\n",
    "\n",
    "# Initialize streaming monitor\n",
    "streaming_monitor = StreamingMonitor(llm)\n",
    "print(\"ðŸŒŠ Streaming response monitor initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Streaming Response Demo\n",
    "\n",
    "Let's test our streaming monitoring system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test streaming with monitoring\n",
    "test_prompts = [\n",
    "    \"Explain the concept of renewable energy and its importance for climate change mitigation.\",\n",
    "    \"Describe the process of machine learning model training and the key considerations.\",\n",
    "    \"What are the main challenges in developing autonomous vehicles and how are they being addressed?\"\n",
    "]\n",
    "\n",
    "print(\"ðŸŒŠ Testing Streaming Response Monitoring...\")\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\nðŸŽ¯ Streaming Test {i}:\")\n",
    "    print(f\"ðŸ“ Prompt: {prompt}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    chunk_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process streaming response\n",
    "    async for event in streaming_monitor.stream_with_monitoring(prompt, quality_check_interval=25):\n",
    "        event_type = event['type']\n",
    "        \n",
    "        if event_type == 'chunk':\n",
    "            chunk_count += 1\n",
    "            metrics = event['metrics']\n",
    "            \n",
    "            # Show progress every 5 chunks\n",
    "            if chunk_count % 5 == 0:\n",
    "                print(f\"ðŸ“¦ Chunk {metrics['chunk_count']}: \"\n",
    "                      f\"{metrics['total_tokens']} tokens, \"\n",
    "                      f\"rate: {metrics['streaming_rate']:.1f} tok/s, \"\n",
    "                      f\"coherence: {metrics['coherence_score']:.2f}\")\n",
    "                \n",
    "                if metrics['latest_quality_score']:\n",
    "                    print(f\"   â­ Partial quality: {metrics['latest_quality_score']:.2f}\")\n",
    "        \n",
    "        elif event_type == 'complete':\n",
    "            final_metrics = event['final_metrics']\n",
    "            \n",
    "            print(f\"\\nâœ… Streaming completed in {time.time() - start_time:.2f}s\")\n",
    "            print(f\"ðŸ“Š Final Metrics:\")\n",
    "            print(f\"   â€¢ Chunks: {final_metrics['chunk_count']}\")\n",
    "            print(f\"   â€¢ Total tokens: {final_metrics['total_tokens']}\")\n",
    "            print(f\"   â€¢ First chunk latency: {final_metrics['first_chunk_latency']:.3f}s\")\n",
    "            print(f\"   â€¢ Streaming rate: {final_metrics['streaming_rate']:.1f} tok/s\")\n",
    "            print(f\"   â€¢ Coherence score: {final_metrics['coherence_score']:.2f}\")\n",
    "            print(f\"   â€¢ Final quality: {final_metrics['final_quality']:.2f}\")\n",
    "            \n",
    "            if final_metrics['partial_quality_scores']:\n",
    "                quality_trend = \"improving\" if (final_metrics['partial_quality_scores'][-1] > \n",
    "                                               final_metrics['partial_quality_scores'][0]) else \"declining\"\n",
    "                print(f\"   â€¢ Quality trend: {quality_trend}\")\n",
    "            \n",
    "            # Show response preview\n",
    "            response_preview = event['final_response'][:200] + \"...\" if len(event['final_response']) > 200 else event['final_response']\n",
    "            print(f\"\\nðŸ’¬ Response preview: {response_preview}\")\n",
    "        \n",
    "        elif event_type == 'error':\n",
    "            print(f\"âŒ Streaming error: {event['error']}\")\n",
    "            break\n",
    "\n",
    "# Display streaming analytics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ˆ STREAMING ANALYTICS DASHBOARD\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "analytics = streaming_monitor.get_streaming_analytics()\n",
    "\n",
    "if analytics:\n",
    "    print(f\"\\nðŸŒŠ Overall Streaming Performance:\")\n",
    "    print(f\"   Total sessions: {analytics['total_streaming_sessions']}\")\n",
    "    print(f\"   Avg first chunk latency: {analytics['avg_first_chunk_latency']:.3f}s\")\n",
    "    print(f\"   Avg streaming rate: {analytics['avg_streaming_rate']:.1f} tokens/sec\")\n",
    "    print(f\"   Avg total time: {analytics['avg_total_time']:.2f}s\")\n",
    "    print(f\"   Avg chunks per session: {analytics['avg_chunk_count']:.1f}\")\n",
    "    \n",
    "    print(f\"\\nâ­ Quality Metrics:\")\n",
    "    print(f\"   Avg final quality: {analytics['avg_final_quality']:.2f}\")\n",
    "    print(f\"   Avg coherence score: {analytics['avg_coherence_score']:.2f}\")\n",
    "    print(f\"   Avg quality improvement: {analytics['avg_quality_improvement']:.2f}\")\n",
    "    print(f\"   Streaming efficiency: {analytics['streaming_efficiency']:.2f}\")\n",
    "    \n",
    "    # Performance insights\n",
    "    print(f\"\\nðŸŽ¯ Performance Insights:\")\n",
    "    if analytics['avg_first_chunk_latency'] < 0.5:\n",
    "        print(\"   âœ… Excellent first chunk latency\")\n",
    "    elif analytics['avg_first_chunk_latency'] < 1.0:\n",
    "        print(\"   âš ï¸  Acceptable first chunk latency\")\n",
    "    else:\n",
    "        print(\"   âŒ High first chunk latency - consider optimization\")\n",
    "    \n",
    "    if analytics['avg_streaming_rate'] > 50:\n",
    "        print(\"   âœ… Excellent streaming rate\")\n",
    "    elif analytics['avg_streaming_rate'] > 30:\n",
    "        print(\"   âš ï¸  Good streaming rate\")\n",
    "    else:\n",
    "        print(\"   âŒ Low streaming rate - investigate bottlenecks\")\n",
    "    \n",
    "    if analytics['avg_quality_improvement'] > 0.1:\n",
    "        print(\"   âœ… Quality improves during streaming\")\n",
    "    elif analytics['avg_quality_improvement'] > -0.1:\n",
    "        print(\"   âœ… Quality remains stable during streaming\")\n",
    "    else:\n",
    "        print(\"   âš ï¸  Quality declines during streaming - review prompts\")\n",
    "\n",
    "else:\n",
    "    print(\"No streaming data available yet.\")\n",
    "\n",
    "print(\"\\nâœ… Streaming monitoring demonstration completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Custom LangSmith Integrations\n",
    "\n",
    "Let's build custom LangSmith integrations and extensions for specialized use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Protocol\n",
    "from abc import ABC, abstractmethod\n",
    "import hashlib\n",
    "import pickle\n",
    "\n",
    "class CustomEvaluator(ABC):\n",
    "    \"\"\"Abstract base class for custom evaluators\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def name(self) -> str:\n",
    "        \"\"\"Return evaluator name\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    async def evaluate(self, prediction: str, reference: str = None, input: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate prediction and return score with metadata\"\"\"\n",
    "        pass\n",
    "\n",
    "class SemanticSimilarityEvaluator(CustomEvaluator):\n",
    "    \"\"\"Custom semantic similarity evaluator\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings: OpenAIEmbeddings):\n",
    "        self.embeddings = embeddings\n",
    "        self.cache = {}\n",
    "    \n",
    "    def name(self) -> str:\n",
    "        return \"semantic_similarity\"\n",
    "    \n",
    "    @traceable(name=\"semantic_similarity_evaluation\")\n",
    "    async def evaluate(self, prediction: str, reference: str = None, input: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate semantic similarity between prediction and reference\"\"\"\n",
    "        if not reference:\n",
    "            return {\"score\": 0.0, \"reason\": \"No reference provided\"}\n",
    "        \n",
    "        # Check cache\n",
    "        cache_key = hashlib.md5(f\"{prediction}:{reference}\".encode()).hexdigest()\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        try:\n",
    "            # Get embeddings\n",
    "            pred_embedding = await self.embeddings.aembed_query(prediction)\n",
    "            ref_embedding = await self.embeddings.aembed_query(reference)\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            pred_array = np.array(pred_embedding)\n",
    "            ref_array = np.array(ref_embedding)\n",
    "            \n",
    "            similarity = np.dot(pred_array, ref_array) / (np.linalg.norm(pred_array) * np.linalg.norm(ref_array))\n",
    "            \n",
    "            result = {\n",
    "                \"score\": float(similarity),\n",
    "                \"reason\": f\"Cosine similarity between embeddings: {similarity:.3f}\",\n",
    "                \"metadata\": {\n",
    "                    \"prediction_length\": len(prediction),\n",
    "                    \"reference_length\": len(reference),\n",
    "                    \"embedding_model\": \"text-embedding-3-small\"\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Cache result\n",
    "            self.cache[cache_key] = result\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"score\": 0.0,\n",
    "                \"reason\": f\"Evaluation failed: {str(e)}\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "class FactualAccuracyEvaluator(CustomEvaluator):\n",
    "    \"\"\"Custom factual accuracy evaluator using LLM-as-judge\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: ChatOpenAI):\n",
    "        self.llm = llm\n",
    "        self.evaluation_cache = {}\n",
    "    \n",
    "    def name(self) -> str:\n",
    "        return \"factual_accuracy\"\n",
    "    \n",
    "    @traceable(name=\"factual_accuracy_evaluation\")\n",
    "    async def evaluate(self, prediction: str, reference: str = None, input: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate factual accuracy of prediction\"\"\"\n",
    "        # Check cache\n",
    "        cache_key = hashlib.md5(f\"{prediction}:{reference or ''}\".encode()).hexdigest()\n",
    "        if cache_key in self.evaluation_cache:\n",
    "            return self.evaluation_cache[cache_key]\n",
    "        \n",
    "        eval_prompt = f\"\"\"\n",
    "        Evaluate the factual accuracy of the given prediction. \n",
    "        {f'Compare it against this reference: {reference}' if reference else ''}\n",
    "        \n",
    "        Prediction to evaluate: {prediction}\n",
    "        \n",
    "        Provide a score from 0 to 1 where:\n",
    "        - 1.0 = Completely accurate, no factual errors\n",
    "        - 0.8 = Mostly accurate with minor inaccuracies\n",
    "        - 0.6 = Some accurate information with notable errors\n",
    "        - 0.4 = Mixed accuracy, significant errors present\n",
    "        - 0.2 = Mostly inaccurate information\n",
    "        - 0.0 = Completely inaccurate or misleading\n",
    "        \n",
    "        Respond with JSON in this format:\n",
    "        {{\n",
    "            \"score\": <float between 0 and 1>,\n",
    "            \"reasoning\": \"<brief explanation of the score>\",\n",
    "            \"errors_found\": [\"<list of specific errors if any>\"]\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = await self.llm.ainvoke([HumanMessage(content=eval_prompt)])\n",
    "            \n",
    "            # Parse JSON response\n",
    "            result_json = json.loads(response.content.strip())\n",
    "            \n",
    "            result = {\n",
    "                \"score\": max(0.0, min(1.0, float(result_json.get(\"score\", 0.5)))),\n",
    "                \"reason\": result_json.get(\"reasoning\", \"No reasoning provided\"),\n",
    "                \"metadata\": {\n",
    "                    \"errors_found\": result_json.get(\"errors_found\", []),\n",
    "                    \"evaluation_model\": self.llm.model_name,\n",
    "                    \"has_reference\": reference is not None\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Cache result\n",
    "            self.evaluation_cache[cache_key] = result\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"score\": 0.5,\n",
    "                \"reason\": f\"Evaluation failed: {str(e)}\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "class CustomLangSmithIntegration:\n",
    "    \"\"\"Custom LangSmith integration with specialized features\"\"\"\n",
    "    \n",
    "    def __init__(self, client: Client):\n",
    "        self.client = client\n",
    "        self.custom_evaluators = {}\n",
    "        self.integration_metrics = {\n",
    "            'custom_evaluations': 0,\n",
    "            'cached_evaluations': 0,\n",
    "            'failed_evaluations': 0\n",
    "        }\n",
    "    \n",
    "    def register_evaluator(self, evaluator: CustomEvaluator):\n",
    "        \"\"\"Register a custom evaluator\"\"\"\n",
    "        self.custom_evaluators[evaluator.name()] = evaluator\n",
    "        print(f\"âœ… Registered custom evaluator: {evaluator.name()}\")\n",
    "    \n",
    "    @traceable(name=\"custom_evaluation_suite\")\n",
    "    async def run_custom_evaluation_suite(self, predictions: List[str], \n",
    "                                        references: List[str] = None,\n",
    "                                        inputs: List[str] = None,\n",
    "                                        evaluator_names: List[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Run custom evaluation suite on predictions\"\"\"\n",
    "        if not predictions:\n",
    "            return {\"error\": \"No predictions provided\"}\n",
    "        \n",
    "        # Use all evaluators if none specified\n",
    "        if evaluator_names is None:\n",
    "            evaluator_names = list(self.custom_evaluators.keys())\n",
    "        \n",
    "        results = {\n",
    "            \"total_predictions\": len(predictions),\n",
    "            \"evaluators_used\": evaluator_names,\n",
    "            \"detailed_results\": [],\n",
    "            \"summary_scores\": {}\n",
    "        }\n",
    "        \n",
    "        # Initialize summary scores\n",
    "        for name in evaluator_names:\n",
    "            results[\"summary_scores\"][name] = []\n",
    "        \n",
    "        # Evaluate each prediction\n",
    "        for i, prediction in enumerate(predictions):\n",
    "            reference = references[i] if references and i < len(references) else None\n",
    "            input_text = inputs[i] if inputs and i < len(inputs) else None\n",
    "            \n",
    "            prediction_results = {\n",
    "                \"index\": i,\n",
    "                \"prediction\": prediction,\n",
    "                \"reference\": reference,\n",
    "                \"input\": input_text,\n",
    "                \"evaluations\": {}\n",
    "            }\n",
    "            \n",
    "            # Run each evaluator\n",
    "            for evaluator_name in evaluator_names:\n",
    "                evaluator = self.custom_evaluators.get(evaluator_name)\n",
    "                if not evaluator:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    eval_result = await evaluator.evaluate(\n",
    "                        prediction=prediction,\n",
    "                        reference=reference,\n",
    "                        input=input_text\n",
    "                    )\n",
    "                    \n",
    "                    prediction_results[\"evaluations\"][evaluator_name] = eval_result\n",
    "                    results[\"summary_scores\"][evaluator_name].append(eval_result[\"score\"])\n",
    "                    \n",
    "                    self.integration_metrics['custom_evaluations'] += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    prediction_results[\"evaluations\"][evaluator_name] = {\n",
    "                        \"score\": 0.0,\n",
    "                        \"error\": str(e)\n",
    "                    }\n",
    "                    self.integration_metrics['failed_evaluations'] += 1\n",
    "            \n",
    "            results[\"detailed_results\"].append(prediction_results)\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        summary_stats = {}\n",
    "        for evaluator_name, scores in results[\"summary_scores\"].items():\n",
    "            if scores:\n",
    "                summary_stats[evaluator_name] = {\n",
    "                    \"mean\": np.mean(scores),\n",
    "                    \"std\": np.std(scores),\n",
    "                    \"min\": np.min(scores),\n",
    "                    \"max\": np.max(scores),\n",
    "                    \"count\": len(scores)\n",
    "                }\n",
    "        \n",
    "        results[\"summary_statistics\"] = summary_stats\n",
    "        \n",
    "        # Add metadata to trace\n",
    "        current_run = RunTree.get_current_run()\n",
    "        if current_run:\n",
    "            current_run.add_metadata({\n",
    "                \"custom_evaluation_suite\": True,\n",
    "                \"evaluators_used\": evaluator_names,\n",
    "                \"total_predictions\": len(predictions),\n",
    "                \"summary_statistics\": summary_stats\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def create_custom_dataset(self, name: str, examples: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"Create a custom dataset in LangSmith\"\"\"\n",
    "        try:\n",
    "            # In real implementation, use LangSmith client\n",
    "            # dataset = self.client.create_dataset(name, description=\"Custom dataset\")\n",
    "            # for example in examples:\n",
    "            #     self.client.create_example(\n",
    "            #         inputs=example.get(\"inputs\", {}),\n",
    "            #         outputs=example.get(\"outputs\", {}),\n",
    "            #         dataset_id=dataset.id\n",
    "            #     )\n",
    "            \n",
    "            # Simulate dataset creation\n",
    "            dataset_id = f\"custom_dataset_{uuid.uuid4().hex[:8]}\"\n",
    "            \n",
    "            print(f\"âœ… Created custom dataset '{name}' with {len(examples)} examples\")\n",
    "            print(f\"ðŸ“Š Dataset ID: {dataset_id}\")\n",
    "            \n",
    "            return dataset_id\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to create dataset: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_integration_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get custom integration metrics\"\"\"\n",
    "        total_evaluations = self.integration_metrics['custom_evaluations'] + self.integration_metrics['failed_evaluations']\n",
    "        \n",
    "        return {\n",
    "            \"registered_evaluators\": len(self.custom_evaluators),\n",
    "            \"total_evaluations\": total_evaluations,\n",
    "            \"successful_evaluations\": self.integration_metrics['custom_evaluations'],\n",
    "            \"failed_evaluations\": self.integration_metrics['failed_evaluations'],\n",
    "            \"success_rate\": (self.integration_metrics['custom_evaluations'] / total_evaluations * 100) if total_evaluations > 0 else 0,\n",
    "            \"cached_evaluations\": self.integration_metrics['cached_evaluations'],\n",
    "            \"evaluator_names\": list(self.custom_evaluators.keys())\n",
    "        }\n",
    "\n",
    "# Initialize custom integration\n",
    "custom_integration = CustomLangSmithIntegration(client)\n",
    "\n",
    "# Register custom evaluators\n",
    "semantic_evaluator = SemanticSimilarityEvaluator(embeddings)\n",
    "factual_evaluator = FactualAccuracyEvaluator(llm)\n",
    "\n",
    "custom_integration.register_evaluator(semantic_evaluator)\n",
    "custom_integration.register_evaluator(factual_evaluator)\n",
    "\n",
    "print(\"ðŸ”§ Custom LangSmith integration initialized with specialized evaluators\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Custom Integration Demo\n",
    "\n",
    "Let's test our custom LangSmith integration with specialized evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test custom evaluators\n",
    "test_predictions = [\n",
    "    \"The Earth orbits around the Sun in approximately 365.25 days.\",\n",
    "    \"Paris is the capital of France and home to the famous Eiffel Tower.\",\n",
    "    \"Water boils at 100 degrees Celsius at sea level atmospheric pressure.\",\n",
    "    \"The human brain contains approximately 86 billion neurons.\",\n",
    "    \"Photosynthesis converts sunlight into chemical energy in plants.\"\n",
    "]\n",
    "\n",
    "test_references = [\n",
    "    \"Earth completes one orbit around the Sun in about 365.25 days, which is why we have leap years.\",\n",
    "    \"Paris, the capital city of France, is famous for landmarks like the Eiffel Tower.\",\n",
    "    \"At standard atmospheric pressure (1 atm), water reaches its boiling point at 100Â°C.\",\n",
    "    \"Scientists estimate that the human brain contains roughly 86 billion nerve cells.\",\n",
    "    \"Through photosynthesis, plants convert light energy from the sun into stored chemical energy.\"\n",
    "]\n",
    "\n",
    "test_inputs = [\n",
    "    \"How long does it take Earth to orbit the Sun?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"At what temperature does water boil?\",\n",
    "    \"How many neurons are in the human brain?\",\n",
    "    \"What is photosynthesis?\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ§ª Testing Custom Evaluation Suite...\")\n",
    "print(f\"ðŸ“Š Evaluating {len(test_predictions)} predictions with custom evaluators\\n\")\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "evaluation_results = await custom_integration.run_custom_evaluation_suite(\n",
    "    predictions=test_predictions,\n",
    "    references=test_references,\n",
    "    inputs=test_inputs\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"ðŸ“ˆ CUSTOM EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nðŸ“Š Summary Statistics:\")\n",
    "for evaluator_name, stats in evaluation_results[\"summary_statistics\"].items():\n",
    "    print(f\"\\n{evaluator_name.title().replace('_', ' ')}:\")\n",
    "    print(f\"   Mean Score: {stats['mean']:.3f} Â± {stats['std']:.3f}\")\n",
    "    print(f\"   Range: {stats['min']:.3f} - {stats['max']:.3f}\")\n",
    "    print(f\"   Evaluations: {stats['count']}\")\n",
    "\n",
    "print(f\"\\nðŸ” Detailed Results:\")\n",
    "for result in evaluation_results[\"detailed_results\"]:\n",
    "    print(f\"\\nðŸ“ Prediction {result['index'] + 1}:\")\n",
    "    print(f\"   Input: {result['input']}\")\n",
    "    print(f\"   Prediction: {result['prediction'][:100]}...\")\n",
    "    \n",
    "    for eval_name, eval_result in result[\"evaluations\"].items():\n",
    "        if \"error\" not in eval_result:\n",
    "            print(f\"   {eval_name}: {eval_result['score']:.3f} - {eval_result['reason'][:80]}...\")\n",
    "        else:\n",
    "            print(f\"   {eval_name}: ERROR - {eval_result['error']}\")\n",
    "\n",
    "# Test custom dataset creation\n",
    "print(f\"\\nðŸ—ƒï¸  Creating Custom Dataset...\")\n",
    "\n",
    "# Create sample dataset\n",
    "dataset_examples = []\n",
    "for i in range(len(test_predictions)):\n",
    "    example = {\n",
    "        \"inputs\": {\"question\": test_inputs[i]},\n",
    "        \"outputs\": {\"answer\": test_references[i]},\n",
    "        \"metadata\": {\n",
    "            \"domain\": \"general_knowledge\",\n",
    "            \"difficulty\": \"easy\",\n",
    "            \"source\": \"custom_evaluation_demo\"\n",
    "        }\n",
    "    }\n",
    "    dataset_examples.append(example)\n",
    "\n",
    "dataset_id = custom_integration.create_custom_dataset(\n",
    "    name=\"advanced_patterns_evaluation_dataset\",\n",
    "    examples=dataset_examples\n",
    ")\n",
    "\n",
    "# Display integration metrics\n",
    "print(f\"\\nðŸ“Š Custom Integration Metrics:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "metrics = custom_integration.get_integration_metrics()\n",
    "print(f\"Registered Evaluators: {metrics['registered_evaluators']}\")\n",
    "print(f\"Total Evaluations: {metrics['total_evaluations']}\")\n",
    "print(f\"Success Rate: {metrics['success_rate']:.1f}%\")\n",
    "print(f\"Failed Evaluations: {metrics['failed_evaluations']}\")\n",
    "print(f\"Available Evaluators: {', '.join(metrics['evaluator_names'])}\")\n",
    "\n",
    "print(\"\\nâœ… Custom integration demonstration completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Comprehensive Performance Dashboard\n",
    "\n",
    "Let's create a unified dashboard showing metrics from all our advanced patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_comprehensive_dashboard():\n",
    "    \"\"\"Display comprehensive dashboard for all advanced patterns\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"ðŸš€ ADVANCED PATTERNS - COMPREHENSIVE PERFORMANCE DASHBOARD\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Multi-Agent System Metrics\n",
    "    print(\"\\nðŸ¤– MULTI-AGENT SYSTEM PERFORMANCE\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    agent_metrics = orchestrator.get_orchestrator_metrics()\n",
    "    if agent_metrics:\n",
    "        print(f\"Active Agents: {agent_metrics['active_agents']}\")\n",
    "        print(f\"Total Tasks Processed: {agent_metrics['total_tasks']}\")\n",
    "        print(f\"Overall Success Rate: {agent_metrics['overall_success_rate']:.1%}\")\n",
    "        print(f\"Average Execution Time: {agent_metrics['average_execution_time']:.2f}s\")\n",
    "        print(f\"Completed Workflows: {agent_metrics['completed_workflows']}\")\n",
    "        \n",
    "        print(\"\\nAgent-Specific Performance:\")\n",
    "        for agent_role, metrics in agent_metrics['agent_metrics'].items():\n",
    "            if metrics['total_tasks'] > 0:\n",
    "                print(f\"  {agent_role.title()}:\")\n",
    "                print(f\"    Tasks: {metrics['total_tasks']} (Success: {metrics['success_rate']:.1%})\")\n",
    "                print(f\"    Avg Time: {metrics['average_execution_time']:.2f}s\")\n",
    "                print(f\"    Avg Retries: {metrics['average_retries']:.1f}\")\n",
    "    else:\n",
    "        print(\"No multi-agent metrics available\")\n",
    "    \n",
    "    # RAG Pipeline Metrics\n",
    "    print(\"\\nðŸ” ADVANCED RAG PIPELINE PERFORMANCE\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    rag_analytics = rag_pipeline.get_pipeline_analytics()\n",
    "    if rag_analytics:\n",
    "        print(f\"Total Queries: {rag_analytics['total_queries']}\")\n",
    "        print(f\"Average Total Time: {rag_analytics['avg_total_time']:.2f}s\")\n",
    "        print(f\"Average Generation Time: {rag_analytics['avg_generation_time']:.2f}s\")\n",
    "        print(f\"Average Context Length: {rag_analytics['avg_context_length']:.0f} chars\")\n",
    "        print(f\"Average Response Quality: {rag_analytics['avg_response_quality']:.2f}\")\n",
    "        \n",
    "        print(\"\\nRetrieval Stage Performance:\")\n",
    "        for stage, metrics in rag_analytics['retrieval_stages'].items():\n",
    "            print(f\"  {stage.replace('_', ' ').title()}:\")\n",
    "            print(f\"    Avg Time: {metrics['avg_retrieval_time']:.3f}s\")\n",
    "            print(f\"    Avg Docs: {metrics['avg_retrieved_count']:.1f}\")\n",
    "            print(f\"    Avg Relevance: {metrics['avg_relevance']:.2f}\")\n",
    "        \n",
    "        trends = rag_analytics.get('performance_trend', {})\n",
    "        if trends and 'quality_trend' in trends:\n",
    "            print(f\"\\nPerformance Trends:\")\n",
    "            print(f\"  Quality: {trends['quality_trend']} ({trends['quality_change']})\")\n",
    "            print(f\"  Speed: {trends['speed_trend']} ({trends['speed_change']})\")\n",
    "    else:\n",
    "        print(\"No RAG pipeline metrics available\")\n",
    "    \n",
    "    # Streaming Metrics\n",
    "    print(\"\\nðŸŒŠ STREAMING RESPONSE PERFORMANCE\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    streaming_analytics = streaming_monitor.get_streaming_analytics()\n",
    "    if streaming_analytics:\n",
    "        print(f\"Total Streaming Sessions: {streaming_analytics['total_streaming_sessions']}\")\n",
    "        print(f\"Avg First Chunk Latency: {streaming_analytics['avg_first_chunk_latency']:.3f}s\")\n",
    "        print(f\"Avg Streaming Rate: {streaming_analytics['avg_streaming_rate']:.1f} tokens/sec\")\n",
    "        print(f\"Avg Session Duration: {streaming_analytics['avg_total_time']:.2f}s\")\n",
    "        print(f\"Avg Chunks per Session: {streaming_analytics['avg_chunk_count']:.1f}\")\n",
    "        print(f\"Avg Final Quality: {streaming_analytics['avg_final_quality']:.2f}\")\n",
    "        print(f\"Avg Coherence Score: {streaming_analytics['avg_coherence_score']:.2f}\")\n",
    "        print(f\"Avg Quality Improvement: {streaming_analytics['avg_quality_improvement']:.2f}\")\n",
    "        print(f\"Streaming Efficiency: {streaming_analytics['streaming_efficiency']:.2f}\")\n",
    "    else:\n",
    "        print(\"No streaming metrics available\")\n",
    "    \n",
    "    # Custom Integration Metrics\n",
    "    print(\"\\nðŸ”§ CUSTOM INTEGRATION PERFORMANCE\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    integration_metrics = custom_integration.get_integration_metrics()\n",
    "    print(f\"Registered Custom Evaluators: {integration_metrics['registered_evaluators']}\")\n",
    "    print(f\"Total Evaluations Run: {integration_metrics['total_evaluations']}\")\n",
    "    print(f\"Evaluation Success Rate: {integration_metrics['success_rate']:.1f}%\")\n",
    "    print(f\"Failed Evaluations: {integration_metrics['failed_evaluations']}\")\n",
    "    print(f\"Available Evaluators: {', '.join(integration_metrics['evaluator_names'])}\")\n",
    "    \n",
    "    # Overall System Health\n",
    "    print(\"\\nðŸ¥ OVERALL SYSTEM HEALTH\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Calculate overall health score\n",
    "    health_components = []\n",
    "    \n",
    "    # Multi-agent health\n",
    "    if agent_metrics and agent_metrics['overall_success_rate'] > 0:\n",
    "        agent_health = agent_metrics['overall_success_rate']\n",
    "        health_components.append(('Multi-Agent', agent_health))\n",
    "    \n",
    "    # RAG health\n",
    "    if rag_analytics and rag_analytics['avg_response_quality'] > 0:\n",
    "        rag_health = rag_analytics['avg_response_quality']\n",
    "        health_components.append(('RAG Pipeline', rag_health))\n",
    "    \n",
    "    # Streaming health\n",
    "    if streaming_analytics and streaming_analytics['avg_final_quality'] > 0:\n",
    "        streaming_health = streaming_analytics['avg_final_quality']\n",
    "        health_components.append(('Streaming', streaming_health))\n",
    "    \n",
    "    # Integration health\n",
    "    if integration_metrics['total_evaluations'] > 0:\n",
    "        integration_health = integration_metrics['success_rate'] / 100\n",
    "        health_components.append(('Custom Integration', integration_health))\n",
    "    \n",
    "    if health_components:\n",
    "        overall_health = np.mean([score for _, score in health_components])\n",
    "        health_icon = \"ðŸŸ¢\" if overall_health > 0.8 else \"ðŸŸ¡\" if overall_health > 0.6 else \"ðŸ”´\"\n",
    "        \n",
    "        print(f\"Overall System Health: {health_icon} {overall_health:.1%}\")\n",
    "        print(\"\\nComponent Health Scores:\")\n",
    "        for component, score in health_components:\n",
    "            component_icon = \"ðŸŸ¢\" if score > 0.8 else \"ðŸŸ¡\" if score > 0.6 else \"ðŸ”´\"\n",
    "            print(f\"  {component_icon} {component}: {score:.1%}\")\n",
    "    else:\n",
    "        print(\"Insufficient data for health assessment\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\nðŸŽ¯ OPTIMIZATION RECOMMENDATIONS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # Agent recommendations\n",
    "    if (agent_metrics and agent_metrics['overall_success_rate'] < 0.9):\n",
    "        recommendations.append(\"ðŸ¤– Consider optimizing multi-agent task coordination and retry logic\")\n",
    "    \n",
    "    # RAG recommendations\n",
    "    if (rag_analytics and rag_analytics['avg_total_time'] > 5):\n",
    "        recommendations.append(\"ðŸ” RAG pipeline latency is high - consider optimizing retrieval stages\")\n",
    "    \n",
    "    if (rag_analytics and rag_analytics['avg_response_quality'] < 0.8):\n",
    "        recommendations.append(\"ðŸ” RAG response quality could be improved - review retrieval and generation prompts\")\n",
    "    \n",
    "    # Streaming recommendations\n",
    "    if (streaming_analytics and streaming_analytics['avg_first_chunk_latency'] > 1.0):\n",
    "        recommendations.append(\"ðŸŒŠ High first chunk latency - investigate model loading and initialization\")\n",
    "    \n",
    "    if (streaming_analytics and streaming_analytics['avg_quality_improvement'] < -0.1):\n",
    "        recommendations.append(\"ðŸŒŠ Quality degrades during streaming - review prompt engineering\")\n",
    "    \n",
    "    # Integration recommendations\n",
    "    if integration_metrics['success_rate'] < 95:\n",
    "        recommendations.append(\"ðŸ”§ Custom evaluation failures detected - review evaluator error handling\")\n",
    "    \n",
    "    if recommendations:\n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            print(f\"{i}. {rec}\")\n",
    "    else:\n",
    "        print(\"âœ… All systems operating optimally - no immediate recommendations\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"âœ… Advanced patterns dashboard analysis completed\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "# Display comprehensive dashboard\n",
    "display_comprehensive_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've successfully completed the Advanced Patterns notebook! Here's what you've mastered:\n",
    "\n",
    "### âœ… Advanced Capabilities Mastered\n",
    "- **Multi-Agent Orchestration**: Built sophisticated agent coordination with dependency management\n",
    "- **Advanced RAG Architectures**: Implemented multi-stage retrieval with quality monitoring\n",
    "- **Streaming Response Monitoring**: Created real-time quality assessment for streaming responses\n",
    "- **Custom LangSmith Integrations**: Built specialized evaluators and custom extensions\n",
    "- **Enterprise Integration Patterns**: Learned how to integrate with existing infrastructure\n",
    "\n",
    "### ðŸ”§ Technical Achievements\n",
    "- **Complex System Observability**: Full visibility into multi-component LLM systems\n",
    "- **Real-time Performance Monitoring**: Live tracking of quality, latency, and coherence\n",
    "- **Custom Evaluation Frameworks**: Built semantic similarity and factual accuracy evaluators\n",
    "- **Advanced Metrics and Analytics**: Comprehensive dashboards and health monitoring\n",
    "- **Production-Ready Patterns**: Scalable architectures for enterprise deployment\n",
    "\n",
    "### ðŸš€ Next Steps\n",
    "\n",
    "1. **Apply to Your Use Cases**:\n",
    "   - Adapt these patterns to your specific domain\n",
    "   - Build custom evaluators for your quality metrics\n",
    "   - Implement streaming for user-facing applications\n",
    "\n",
    "2. **Scale and Optimize**:\n",
    "   - Deploy multi-agent systems for complex workflows\n",
    "   - Implement advanced RAG for knowledge-intensive applications\n",
    "   - Add custom monitoring for business-specific metrics\n",
    "\n",
    "3. **Continue Learning**:\n",
    "   - **LSM-008**: Tips and FAQs - Pro tips and troubleshooting guide\n",
    "   - Explore LangSmith's latest features and updates\n",
    "   - Join the community and share your patterns\n",
    "\n",
    "### ðŸ’¡ Key Takeaways for Advanced Patterns\n",
    "\n",
    "- **Observability is Critical**: Complex systems need comprehensive monitoring\n",
    "- **Quality Assessment**: Real-time quality monitoring enables proactive optimization\n",
    "- **Modular Design**: Build reusable components for scalability\n",
    "- **Custom Extensions**: Tailor LangSmith to your specific needs\n",
    "- **Performance Optimization**: Use metrics to drive continuous improvement\n",
    "\n",
    "---\n",
    "\n",
    "**Ready for expert tips and troubleshooting?** Continue to LSM-008 for pro tips, common pitfalls, and advanced troubleshooting techniques! ðŸš€\n",
    "\n",
    "You're now equipped to build and monitor the most sophisticated LLM applications with LangSmith. The patterns you've learned here represent the cutting edge of LLM application development and observability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}