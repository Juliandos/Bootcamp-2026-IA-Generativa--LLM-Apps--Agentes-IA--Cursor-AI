{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSM-004: Building Robust Evaluation Pipelines\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Master advanced dataset creation and management techniques\n",
    "- Build custom evaluators for different types of LLM applications\n",
    "- Implement LLM-as-Judge evaluation patterns\n",
    "- Set up regression testing and CI/CD integration\n",
    "- Use comparison views and A/B testing for optimization\n",
    "- Create comprehensive evaluation workflows for production systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Setup and Dependencies\n",
    "\n",
    "Let's start by setting up our comprehensive evaluation environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for evaluation\n",
    "!pip install langsmith langchain langchain-openai langchain-community\n",
    "!pip install datasets scikit-learn rouge-score bert-score\n",
    "!pip install python-dotenv pandas numpy matplotlib seaborn\n",
    "!pip install pytest asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Callable\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import Client, traceable\n",
    "from langsmith.evaluation import evaluate, EvaluationResult\n",
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize LangSmith client\n",
    "client = Client()\n",
    "\n",
    "print(f\"âœ… Environment setup complete\")\n",
    "print(f\"ðŸ“Š Project: {os.getenv('LANGSMITH_PROJECT', 'Not set')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Advanced Dataset Creation and Management\n",
    "\n",
    "Let's start by creating comprehensive datasets for different types of evaluation scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different types of evaluation datasets\n",
    "\n",
    "class DatasetBuilder:\n",
    "    \"\"\"Advanced dataset builder for LLM evaluation\"\"\"\n",
    "    \n",
    "    def __init__(self, client: Client):\n",
    "        self.client = client\n",
    "    \n",
    "    def create_qa_dataset(self, name: str, examples: List[Dict]) -> str:\n",
    "        \"\"\"Create a Q&A evaluation dataset\"\"\"\n",
    "        try:\n",
    "            dataset = self.client.create_dataset(\n",
    "                dataset_name=name,\n",
    "                description=\"Question-Answering evaluation dataset with ground truth answers\"\n",
    "            )\n",
    "            \n",
    "            self.client.create_examples(\n",
    "                inputs=[ex[\"inputs\"] for ex in examples],\n",
    "                outputs=[ex[\"outputs\"] for ex in examples],\n",
    "                dataset_id=dataset.id\n",
    "            )\n",
    "            \n",
    "            return dataset.id\n",
    "            \n",
    "        except Exception as e:\n",
    "            if \"already exists\" in str(e):\n",
    "                datasets = list(self.client.list_datasets(dataset_name=name))\n",
    "                return datasets[0].id if datasets else None\n",
    "            raise e\n",
    "    \n",
    "    def create_classification_dataset(self, name: str, examples: List[Dict]) -> str:\n",
    "        \"\"\"Create a classification evaluation dataset\"\"\"\n",
    "        try:\n",
    "            dataset = self.client.create_dataset(\n",
    "                dataset_name=name,\n",
    "                description=\"Text classification dataset with labels and confidence scores\"\n",
    "            )\n",
    "            \n",
    "            self.client.create_examples(\n",
    "                inputs=[ex[\"inputs\"] for ex in examples],\n",
    "                outputs=[ex[\"outputs\"] for ex in examples],\n",
    "                dataset_id=dataset.id\n",
    "            )\n",
    "            \n",
    "            return dataset.id\n",
    "            \n",
    "        except Exception as e:\n",
    "            if \"already exists\" in str(e):\n",
    "                datasets = list(self.client.list_datasets(dataset_name=name))\n",
    "                return datasets[0].id if datasets else None\n",
    "            raise e\n",
    "    \n",
    "    def create_summarization_dataset(self, name: str, examples: List[Dict]) -> str:\n",
    "        \"\"\"Create a summarization evaluation dataset\"\"\"\n",
    "        try:\n",
    "            dataset = self.client.create_dataset(\n",
    "                dataset_name=name,\n",
    "                description=\"Text summarization dataset with reference summaries\"\n",
    "            )\n",
    "            \n",
    "            self.client.create_examples(\n",
    "                inputs=[ex[\"inputs\"] for ex in examples],\n",
    "                outputs=[ex[\"outputs\"] for ex in examples],\n",
    "                dataset_id=dataset.id\n",
    "            )\n",
    "            \n",
    "            return dataset.id\n",
    "            \n",
    "        except Exception as e:\n",
    "            if \"already exists\" in str(e):\n",
    "                datasets = list(self.client.list_datasets(dataset_name=name))\n",
    "                return datasets[0].id if datasets else None\n",
    "            raise e\n",
    "\n",
    "# Initialize dataset builder\n",
    "dataset_builder = DatasetBuilder(client)\n",
    "\n",
    "# Create sample datasets\n",
    "print(\"ðŸ“Š Creating evaluation datasets...\")\n",
    "\n",
    "# Q&A Dataset\n",
    "qa_examples = [\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is the capital of France?\"},\n",
    "        \"outputs\": {\"answer\": \"Paris\", \"confidence\": 1.0, \"category\": \"geography\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Explain the concept of machine learning in one sentence.\"},\n",
    "        \"outputs\": {\"answer\": \"Machine learning is a subset of AI that enables computers to learn and make decisions from data without being explicitly programmed.\", \"confidence\": 0.9, \"category\": \"technology\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What are the main causes of climate change?\"},\n",
    "        \"outputs\": {\"answer\": \"The main causes of climate change are greenhouse gas emissions from burning fossil fuels, deforestation, and industrial processes.\", \"confidence\": 0.95, \"category\": \"science\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"How do you calculate compound interest?\"},\n",
    "        \"outputs\": {\"answer\": \"Compound interest is calculated using the formula A = P(1 + r/n)^(nt), where P is principal, r is annual interest rate, n is compounding frequency, and t is time in years.\", \"confidence\": 1.0, \"category\": \"mathematics\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is the difference between HTML and CSS?\"},\n",
    "        \"outputs\": {\"answer\": \"HTML structures web content while CSS styles and formats that content, controlling layout, colors, fonts, and visual presentation.\", \"confidence\": 0.9, \"category\": \"technology\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "qa_dataset_id = dataset_builder.create_qa_dataset(\"qa-evaluation-advanced\", qa_examples)\n",
    "print(f\"âœ… Q&A Dataset created: {qa_dataset_id}\")\n",
    "\n",
    "# Classification Dataset\n",
    "classification_examples = [\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"I absolutely love this product! It exceeded all my expectations and works perfectly.\"},\n",
    "        \"outputs\": {\"label\": \"positive\", \"confidence\": 0.95, \"reasoning\": \"Strong positive language with words like 'love', 'exceeded expectations', and 'perfectly'\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"This item is completely useless and broke after one day. Waste of money!\"},\n",
    "        \"outputs\": {\"label\": \"negative\", \"confidence\": 0.98, \"reasoning\": \"Clear negative sentiment with 'useless', 'broke', and 'waste of money'\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"The product is okay, nothing special but it does what it's supposed to do.\"},\n",
    "        \"outputs\": {\"label\": \"neutral\", \"confidence\": 0.85, \"reasoning\": \"Balanced sentiment with neutral expressions like 'okay' and 'nothing special'\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"Amazing quality and fast delivery! Highly recommend to everyone.\"},\n",
    "        \"outputs\": {\"label\": \"positive\", \"confidence\": 0.92, \"reasoning\": \"Positive words like 'amazing', 'fast', and 'highly recommend'\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"Poor customer service and the product arrived damaged. Very disappointed.\"},\n",
    "        \"outputs\": {\"label\": \"negative\", \"confidence\": 0.90, \"reasoning\": \"Negative experience described with 'poor', 'damaged', and 'disappointed'\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "classification_dataset_id = dataset_builder.create_classification_dataset(\"sentiment-classification-advanced\", classification_examples)\n",
    "print(f\"âœ… Classification Dataset created: {classification_dataset_id}\")\n",
    "\n",
    "# Summarization Dataset\n",
    "summarization_examples = [\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"Artificial intelligence (AI) is transforming industries worldwide. From healthcare to finance, AI applications are becoming more sophisticated and widespread. Machine learning algorithms can now process vast amounts of data to identify patterns that would be impossible for humans to detect manually. However, this rapid advancement also raises important questions about job displacement, privacy, and ethical considerations that society must address.\"},\n",
    "        \"outputs\": {\"summary\": \"AI is transforming industries globally with sophisticated applications, but raises concerns about jobs, privacy, and ethics.\", \"key_points\": [\"Industry transformation\", \"Advanced applications\", \"Pattern recognition capabilities\", \"Societal concerns\"], \"length_category\": \"medium\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"Climate change represents one of the most pressing challenges of our time. Rising global temperatures, melting ice caps, and extreme weather events are clear indicators of environmental change. Scientists agree that human activities, particularly the burning of fossil fuels, are the primary drivers of these changes. Immediate action is required to reduce greenhouse gas emissions and transition to renewable energy sources.\"},\n",
    "        \"outputs\": {\"summary\": \"Climate change, driven primarily by human fossil fuel use, requires immediate action to reduce emissions and adopt renewable energy.\", \"key_points\": [\"Pressing global challenge\", \"Clear environmental indicators\", \"Human-caused\", \"Need for immediate action\"], \"length_category\": \"medium\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "summarization_dataset_id = dataset_builder.create_summarization_dataset(\"summarization-evaluation\", summarization_examples)\n",
    "print(f\"âœ… Summarization Dataset created: {summarization_dataset_id}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ All evaluation datasets created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Custom Evaluator Development\n",
    "\n",
    "Now let's build sophisticated custom evaluators for different types of applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation result models\n",
    "class EvaluationScore(BaseModel):\n",
    "    \"\"\"Structured evaluation result\"\"\"\n",
    "    score: float = Field(description=\"Numeric score between 0 and 1\")\n",
    "    reasoning: str = Field(description=\"Explanation of the score\")\n",
    "    confidence: float = Field(description=\"Confidence in the evaluation\")\n",
    "    category: Optional[str] = Field(description=\"Category or aspect being evaluated\")\n",
    "\n",
    "class CustomEvaluators:\n",
    "    \"\"\"Collection of custom evaluators for different tasks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(temperature=0.1, model=\"gpt-4\")\n",
    "        self.parser = PydanticOutputParser(pydantic_object=EvaluationScore)\n",
    "    \n",
    "    def accuracy_evaluator(self, run: Run, example: Example) -> Dict[str, Any]:\n",
    "        \"\"\"Exact match accuracy evaluator\"\"\"\n",
    "        prediction = run.outputs.get(\"answer\", \"\").strip().lower()\n",
    "        ground_truth = example.outputs.get(\"answer\", \"\").strip().lower()\n",
    "        \n",
    "        is_correct = prediction == ground_truth\n",
    "        \n",
    "        return {\n",
    "            \"key\": \"accuracy\",\n",
    "            \"score\": 1.0 if is_correct else 0.0,\n",
    "            \"comment\": f\"Expected: '{ground_truth}', Got: '{prediction}'\"\n",
    "        }\n",
    "    \n",
    "    def semantic_similarity_evaluator(self, run: Run, example: Example) -> Dict[str, Any]:\n",
    "        \"\"\"LLM-as-Judge semantic similarity evaluator\"\"\"\n",
    "        prediction = run.outputs.get(\"answer\", \"\")\n",
    "        ground_truth = example.outputs.get(\"answer\", \"\")\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are an expert evaluator. Compare the semantic similarity between a predicted answer and the ground truth answer.\n",
    "            \n",
    "Rate the similarity on a scale from 0.0 to 1.0 where:\n",
    "- 1.0: Semantically identical or equivalent meaning\n",
    "- 0.8-0.9: Very similar meaning with minor differences\n",
    "- 0.6-0.7: Similar core meaning but some important differences\n",
    "- 0.4-0.5: Some overlap but significant differences\n",
    "- 0.0-0.3: Very different or contradictory meanings\n",
    "\n",
    "Consider factual accuracy, completeness, and semantic meaning.\n",
    "\n",
    "{format_instructions}\"\"\"),\n",
    "            (\"human\", \"\"\"Ground Truth Answer: {ground_truth}\n",
    "\n",
    "Predicted Answer: {prediction}\n",
    "\n",
    "Evaluate the semantic similarity:\"\"\")\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            formatted_prompt = prompt.format_messages(\n",
    "                ground_truth=ground_truth,\n",
    "                prediction=prediction,\n",
    "                format_instructions=self.parser.get_format_instructions()\n",
    "            )\n",
    "            \n",
    "            response = self.llm.invoke(formatted_prompt)\n",
    "            result = self.parser.parse(response.content)\n",
    "            \n",
    "            return {\n",
    "                \"key\": \"semantic_similarity\",\n",
    "                \"score\": result.score,\n",
    "                \"comment\": result.reasoning\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"key\": \"semantic_similarity\",\n",
    "                \"score\": 0.0,\n",
    "                \"comment\": f\"Evaluation failed: {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    def classification_evaluator(self, run: Run, example: Example) -> Dict[str, Any]:\n",
    "        \"\"\"Classification accuracy evaluator with confidence weighting\"\"\"\n",
    "        predicted_label = run.outputs.get(\"label\", \"\").strip().lower()\n",
    "        true_label = example.outputs.get(\"label\", \"\").strip().lower()\n",
    "        predicted_confidence = run.outputs.get(\"confidence\", 0.5)\n",
    "        \n",
    "        is_correct = predicted_label == true_label\n",
    "        \n",
    "        # Weight score by confidence (penalize overconfident wrong predictions)\n",
    "        if is_correct:\n",
    "            score = min(1.0, predicted_confidence * 1.2)  # Bonus for confident correct predictions\n",
    "        else:\n",
    "            score = max(0.0, 1.0 - predicted_confidence)  # Penalty for confident wrong predictions\n",
    "        \n",
    "        return {\n",
    "            \"key\": \"classification_accuracy\",\n",
    "            \"score\": score,\n",
    "            \"comment\": f\"Predicted: {predicted_label} (conf: {predicted_confidence:.2f}), True: {true_label}, Correct: {is_correct}\"\n",
    "        }\n",
    "    \n",
    "    def helpfulness_evaluator(self, run: Run, example: Example) -> Dict[str, Any]:\n",
    "        \"\"\"LLM-as-Judge helpfulness evaluator\"\"\"\n",
    "        question = example.inputs.get(\"question\", \"\")\n",
    "        answer = run.outputs.get(\"answer\", \"\")\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are an expert evaluator assessing the helpfulness of answers to questions.\n",
    "            \n",
    "Rate the helpfulness on a scale from 0.0 to 1.0 based on:\n",
    "- Relevance: How well does the answer address the question?\n",
    "- Completeness: Is the answer comprehensive enough?\n",
    "- Clarity: Is the answer clear and understandable?\n",
    "- Accuracy: Is the information provided correct?\n",
    "- Usefulness: Would this answer help someone who asked the question?\n",
    "\n",
    "{format_instructions}\"\"\"),\n",
    "            (\"human\", \"\"\"Question: {question}\n",
    "\n",
    "Answer: {answer}\n",
    "\n",
    "Evaluate the helpfulness:\"\"\")\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            formatted_prompt = prompt.format_messages(\n",
    "                question=question,\n",
    "                answer=answer,\n",
    "                format_instructions=self.parser.get_format_instructions()\n",
    "            )\n",
    "            \n",
    "            response = self.llm.invoke(formatted_prompt)\n",
    "            result = self.parser.parse(response.content)\n",
    "            \n",
    "            return {\n",
    "                \"key\": \"helpfulness\",\n",
    "                \"score\": result.score,\n",
    "                \"comment\": result.reasoning\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"key\": \"helpfulness\",\n",
    "                \"score\": 0.5,\n",
    "                \"comment\": f\"Evaluation failed: {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    def conciseness_evaluator(self, run: Run, example: Example) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate response conciseness\"\"\"\n",
    "        answer = run.outputs.get(\"answer\", \"\")\n",
    "        question = example.inputs.get(\"question\", \"\")\n",
    "        \n",
    "        word_count = len(answer.split())\n",
    "        char_count = len(answer)\n",
    "        \n",
    "        # Simple heuristic: penalize overly long answers for simple questions\n",
    "        question_words = len(question.split())\n",
    "        \n",
    "        if question_words <= 10:  # Simple question\n",
    "            ideal_length = 50  # words\n",
    "        else:  # Complex question\n",
    "            ideal_length = 100  # words\n",
    "        \n",
    "        # Calculate conciseness score\n",
    "        if word_count <= ideal_length:\n",
    "            score = 1.0\n",
    "        else:\n",
    "            # Gradual penalty for length\n",
    "            excess_ratio = (word_count - ideal_length) / ideal_length\n",
    "            score = max(0.0, 1.0 - (excess_ratio * 0.5))\n",
    "        \n",
    "        return {\n",
    "            \"key\": \"conciseness\",\n",
    "            \"score\": score,\n",
    "            \"comment\": f\"Answer length: {word_count} words, {char_count} chars. Ideal: ~{ideal_length} words.\"\n",
    "        }\n",
    "\n",
    "# Initialize evaluators\n",
    "evaluators = CustomEvaluators()\n",
    "print(\"âœ… Custom evaluators initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Application Under Test\n",
    "\n",
    "Let's create some example applications to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define applications to evaluate\n",
    "\n",
    "class ApplicationsUnderTest:\n",
    "    \"\"\"Collection of applications to evaluate\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(temperature=0.3, model=\"gpt-3.5-turbo\")\n",
    "        self.creative_llm = ChatOpenAI(temperature=0.7, model=\"gpt-3.5-turbo\")\n",
    "        self.precise_llm = ChatOpenAI(temperature=0.0, model=\"gpt-4\")\n",
    "    \n",
    "    @traceable(run_type=\"chain\", tags=[\"qa-system\", \"baseline\"])\n",
    "    def baseline_qa_system(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Baseline Q&A system using standard LLM\"\"\"\n",
    "        question = inputs[\"question\"]\n",
    "        \n",
    "        messages = [\n",
    "            SystemMessage(content=\"Answer the question concisely and accurately.\"),\n",
    "            HumanMessage(content=question)\n",
    "        ]\n",
    "        \n",
    "        response = self.llm.invoke(messages)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response.content,\n",
    "            \"model\": \"gpt-3.5-turbo\",\n",
    "            \"temperature\": 0.3\n",
    "        }\n",
    "    \n",
    "    @traceable(run_type=\"chain\", tags=[\"qa-system\", \"enhanced\"])\n",
    "    def enhanced_qa_system(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Enhanced Q&A system with better prompting\"\"\"\n",
    "        question = inputs[\"question\"]\n",
    "        \n",
    "        messages = [\n",
    "            SystemMessage(content=\"\"\"You are a knowledgeable assistant. Provide accurate, helpful, and concise answers.\n",
    "            \n",
    "Guidelines:\n",
    "- Be factually accurate\n",
    "- Keep answers concise but complete\n",
    "- If you're unsure, acknowledge uncertainty\n",
    "- Provide context when helpful\"\"\"),\n",
    "            HumanMessage(content=f\"Question: {question}\")\n",
    "        ]\n",
    "        \n",
    "        response = self.precise_llm.invoke(messages)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response.content,\n",
    "            \"model\": \"gpt-4\",\n",
    "            \"temperature\": 0.0\n",
    "        }\n",
    "    \n",
    "    @traceable(run_type=\"chain\", tags=[\"sentiment-classifier\", \"simple\"])\n",
    "    def simple_sentiment_classifier(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Simple sentiment classification system\"\"\"\n",
    "        text = inputs[\"text\"]\n",
    "        \n",
    "        messages = [\n",
    "            SystemMessage(content=\"Classify the sentiment as positive, negative, or neutral. Respond with just the label.\"),\n",
    "            HumanMessage(content=text)\n",
    "        ]\n",
    "        \n",
    "        response = self.llm.invoke(messages)\n",
    "        label = response.content.strip().lower()\n",
    "        \n",
    "        # Simple confidence estimation based on text length and obvious sentiment words\n",
    "        confidence = 0.7  # Default\n",
    "        if any(word in text.lower() for word in ['love', 'amazing', 'perfect', 'excellent']):\n",
    "            confidence = 0.9\n",
    "        elif any(word in text.lower() for word in ['hate', 'terrible', 'awful', 'useless']):\n",
    "            confidence = 0.9\n",
    "        \n",
    "        return {\n",
    "            \"label\": label,\n",
    "            \"confidence\": confidence,\n",
    "            \"model\": \"gpt-3.5-turbo\"\n",
    "        }\n",
    "    \n",
    "    @traceable(run_type=\"chain\", tags=[\"sentiment-classifier\", \"advanced\"])\n",
    "    def advanced_sentiment_classifier(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Advanced sentiment classification with reasoning\"\"\"\n",
    "        text = inputs[\"text\"]\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are an expert sentiment analyzer. Classify the sentiment and provide reasoning.\n",
    "            \n",
    "Respond in JSON format:\n",
    "{\n",
    "    \"label\": \"positive\" | \"negative\" | \"neutral\",\n",
    "    \"confidence\": 0.0-1.0,\n",
    "    \"reasoning\": \"explanation of classification\"\n",
    "}\"\"\"),\n",
    "            (\"human\", \"Text to analyze: {text}\")\n",
    "        ])\n",
    "        \n",
    "        response = self.llm.invoke(prompt.format_messages(text=text))\n",
    "        \n",
    "        try:\n",
    "            result = json.loads(response.content)\n",
    "            return {\n",
    "                \"label\": result.get(\"label\", \"neutral\"),\n",
    "                \"confidence\": result.get(\"confidence\", 0.5),\n",
    "                \"reasoning\": result.get(\"reasoning\", \"No reasoning provided\"),\n",
    "                \"model\": \"gpt-3.5-turbo\"\n",
    "            }\n",
    "        except:\n",
    "            # Fallback if JSON parsing fails\n",
    "            return {\n",
    "                \"label\": \"neutral\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"reasoning\": \"Failed to parse response\",\n",
    "                \"model\": \"gpt-3.5-turbo\"\n",
    "            }\n",
    "\n",
    "# Initialize applications\n",
    "apps = ApplicationsUnderTest()\n",
    "print(\"âœ… Test applications initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Running Comprehensive Evaluations\n",
    "\n",
    "Now let's run comprehensive evaluations comparing different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive Q&A evaluation\n",
    "\n",
    "print(\"ðŸ§ª Running Q&A System Evaluations...\\n\")\n",
    "\n",
    "# Evaluate baseline Q&A system\n",
    "print(\"ðŸ“Š Evaluating Baseline Q&A System\")\n",
    "try:\n",
    "    baseline_results = evaluate(\n",
    "        apps.baseline_qa_system,\n",
    "        data=\"qa-evaluation-advanced\",\n",
    "        evaluators=[\n",
    "            evaluators.accuracy_evaluator,\n",
    "            evaluators.semantic_similarity_evaluator,\n",
    "            evaluators.helpfulness_evaluator,\n",
    "            evaluators.conciseness_evaluator\n",
    "        ],\n",
    "        experiment_prefix=\"qa-baseline\",\n",
    "        description=\"Baseline Q&A system evaluation\",\n",
    "        max_concurrency=2\n",
    "    )\n",
    "    print(f\"âœ… Baseline evaluation completed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Baseline evaluation failed: {e}\")\n",
    "\n",
    "# Evaluate enhanced Q&A system\n",
    "print(\"\\nðŸ“Š Evaluating Enhanced Q&A System\")\n",
    "try:\n",
    "    enhanced_results = evaluate(\n",
    "        apps.enhanced_qa_system,\n",
    "        data=\"qa-evaluation-advanced\",\n",
    "        evaluators=[\n",
    "            evaluators.accuracy_evaluator,\n",
    "            evaluators.semantic_similarity_evaluator,\n",
    "            evaluators.helpfulness_evaluator,\n",
    "            evaluators.conciseness_evaluator\n",
    "        ],\n",
    "        experiment_prefix=\"qa-enhanced\",\n",
    "        description=\"Enhanced Q&A system evaluation\",\n",
    "        max_concurrency=2\n",
    "    )\n",
    "    print(f\"âœ… Enhanced evaluation completed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Enhanced evaluation failed: {e}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Q&A Evaluations completed! Check your LangSmith dashboard for detailed results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sentiment classification evaluation\n",
    "\n",
    "print(\"ðŸ§ª Running Sentiment Classification Evaluations...\\n\")\n",
    "\n",
    "# Evaluate simple sentiment classifier\n",
    "print(\"ðŸ“Š Evaluating Simple Sentiment Classifier\")\n",
    "try:\n",
    "    simple_sentiment_results = evaluate(\n",
    "        apps.simple_sentiment_classifier,\n",
    "        data=\"sentiment-classification-advanced\",\n",
    "        evaluators=[evaluators.classification_evaluator],\n",
    "        experiment_prefix=\"sentiment-simple\",\n",
    "        description=\"Simple sentiment classification evaluation\",\n",
    "        max_concurrency=2\n",
    "    )\n",
    "    print(f\"âœ… Simple classifier evaluation completed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Simple classifier evaluation failed: {e}\")\n",
    "\n",
    "# Evaluate advanced sentiment classifier\n",
    "print(\"\\nðŸ“Š Evaluating Advanced Sentiment Classifier\")\n",
    "try:\n",
    "    advanced_sentiment_results = evaluate(\n",
    "        apps.advanced_sentiment_classifier,\n",
    "        data=\"sentiment-classification-advanced\",\n",
    "        evaluators=[evaluators.classification_evaluator],\n",
    "        experiment_prefix=\"sentiment-advanced\",\n",
    "        description=\"Advanced sentiment classification evaluation\",\n",
    "        max_concurrency=2\n",
    "    )\n",
    "    print(f\"âœ… Advanced classifier evaluation completed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Advanced classifier evaluation failed: {e}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Sentiment Classification Evaluations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Evaluation Results Analysis\n",
    "\n",
    "Let's create tools to analyze and visualize our evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation results analysis tools\n",
    "\n",
    "class EvaluationAnalyzer:\n",
    "    \"\"\"Analyze and visualize evaluation results\"\"\"\n",
    "    \n",
    "    def __init__(self, client: Client):\n",
    "        self.client = client\n",
    "    \n",
    "    def get_experiment_results(self, experiment_prefix: str) -> List[Dict]:\n",
    "        \"\"\"Get results from experiments with given prefix\"\"\"\n",
    "        try:\n",
    "            experiments = list(self.client.list_experiments(\n",
    "                project_name=os.getenv(\"LANGSMITH_PROJECT\")\n",
    "            ))\n",
    "            \n",
    "            matching_experiments = [\n",
    "                exp for exp in experiments \n",
    "                if exp.name and exp.name.startswith(experiment_prefix)\n",
    "            ]\n",
    "            \n",
    "            if not matching_experiments:\n",
    "                print(f\"No experiments found with prefix: {experiment_prefix}\")\n",
    "                return []\n",
    "            \n",
    "            # Get the most recent experiment\n",
    "            latest_experiment = max(matching_experiments, key=lambda x: x.created_at)\n",
    "            \n",
    "            # Get experiment results\n",
    "            results = []\n",
    "            for run in self.client.list_runs(experiment_name=latest_experiment.name):\n",
    "                if hasattr(run, 'feedback_stats') and run.feedback_stats:\n",
    "                    results.append({\n",
    "                        'run_id': run.id,\n",
    "                        'feedback_stats': run.feedback_stats\n",
    "                    })\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving experiment results: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def calculate_summary_metrics(self, results: List[Dict]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate summary metrics from evaluation results\"\"\"\n",
    "        if not results:\n",
    "            return {}\n",
    "        \n",
    "        metrics = {}\n",
    "        metric_values = {}\n",
    "        \n",
    "        # Collect all metric values\n",
    "        for result in results:\n",
    "            feedback_stats = result.get('feedback_stats', {})\n",
    "            for metric_name, metric_data in feedback_stats.items():\n",
    "                if metric_name not in metric_values:\n",
    "                    metric_values[metric_name] = []\n",
    "                \n",
    "                # Extract score based on metric data structure\n",
    "                if isinstance(metric_data, dict):\n",
    "                    score = metric_data.get('avg', metric_data.get('mean', 0))\n",
    "                else:\n",
    "                    score = metric_data\n",
    "                \n",
    "                metric_values[metric_name].append(score)\n",
    "        \n",
    "        # Calculate averages\n",
    "        for metric_name, values in metric_values.items():\n",
    "            if values:\n",
    "                metrics[f\"{metric_name}_avg\"] = np.mean(values)\n",
    "                metrics[f\"{metric_name}_std\"] = np.std(values)\n",
    "                metrics[f\"{metric_name}_min\"] = np.min(values)\n",
    "                metrics[f\"{metric_name}_max\"] = np.max(values)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def create_comparison_report(self, experiment_prefixes: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Create a comparison report between different experiments\"\"\"\n",
    "        comparison_data = []\n",
    "        \n",
    "        for prefix in experiment_prefixes:\n",
    "            results = self.get_experiment_results(prefix)\n",
    "            metrics = self.calculate_summary_metrics(results)\n",
    "            \n",
    "            if metrics:\n",
    "                row = {'experiment': prefix, **metrics}\n",
    "                comparison_data.append(row)\n",
    "        \n",
    "        if comparison_data:\n",
    "            df = pd.DataFrame(comparison_data)\n",
    "            return df\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def visualize_metrics(self, df: pd.DataFrame, metrics_to_plot: List[str]):\n",
    "        \"\"\"Create visualizations of evaluation metrics\"\"\"\n",
    "        if df.empty:\n",
    "            print(\"No data to visualize\")\n",
    "            return\n",
    "        \n",
    "        # Set up the plot style\n",
    "        plt.style.use('default')\n",
    "        fig, axes = plt.subplots(1, len(metrics_to_plot), figsize=(5 * len(metrics_to_plot), 6))\n",
    "        \n",
    "        if len(metrics_to_plot) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, metric in enumerate(metrics_to_plot):\n",
    "            metric_col = f\"{metric}_avg\"\n",
    "            error_col = f\"{metric}_std\"\n",
    "            \n",
    "            if metric_col in df.columns:\n",
    "                x_pos = range(len(df))\n",
    "                values = df[metric_col]\n",
    "                errors = df.get(error_col, [0] * len(df))\n",
    "                \n",
    "                bars = axes[i].bar(x_pos, values, yerr=errors, capsize=5, alpha=0.8)\n",
    "                axes[i].set_xlabel('Experiment')\n",
    "                axes[i].set_ylabel(f'{metric.title()} Score')\n",
    "                axes[i].set_title(f'{metric.title()} Comparison')\n",
    "                axes[i].set_xticks(x_pos)\n",
    "                axes[i].set_xticklabels(df['experiment'], rotation=45, ha='right')\n",
    "                axes[i].set_ylim(0, 1.1)\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Add value labels on bars\n",
    "                for bar, value in zip(bars, values):\n",
    "                    axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                               f'{value:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = EvaluationAnalyzer(client)\n",
    "\n",
    "# Create comparison report\n",
    "print(\"ðŸ“Š Analyzing Evaluation Results...\\n\")\n",
    "\n",
    "# Attempt to create comparison reports\n",
    "qa_experiments = [\"qa-baseline\", \"qa-enhanced\"]\n",
    "sentiment_experiments = [\"sentiment-simple\", \"sentiment-advanced\"]\n",
    "\n",
    "print(\"Q&A Systems Comparison:\")\n",
    "qa_comparison = analyzer.create_comparison_report(qa_experiments)\n",
    "if not qa_comparison.empty:\n",
    "    print(qa_comparison.to_string(index=False, float_format='%.3f'))\n",
    "    \n",
    "    # Visualize key metrics\n",
    "    qa_metrics_to_plot = []\n",
    "    for col in qa_comparison.columns:\n",
    "        if col.endswith('_avg') and any(metric in col for metric in ['accuracy', 'semantic_similarity', 'helpfulness']):\n",
    "            qa_metrics_to_plot.append(col.replace('_avg', ''))\n",
    "    \n",
    "    if qa_metrics_to_plot:\n",
    "        print(f\"\\nðŸ“ˆ Visualizing metrics: {qa_metrics_to_plot}\")\n",
    "        analyzer.visualize_metrics(qa_comparison, qa_metrics_to_plot)\n",
    "else:\n",
    "    print(\"No Q&A comparison data available yet. Run the evaluations first.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Sentiment Classification Systems Comparison:\")\n",
    "sentiment_comparison = analyzer.create_comparison_report(sentiment_experiments)\n",
    "if not sentiment_comparison.empty:\n",
    "    print(sentiment_comparison.to_string(index=False, float_format='%.3f'))\n",
    "    \n",
    "    # Visualize key metrics\n",
    "    sentiment_metrics_to_plot = []\n",
    "    for col in sentiment_comparison.columns:\n",
    "        if col.endswith('_avg') and 'classification' in col:\n",
    "            sentiment_metrics_to_plot.append(col.replace('_avg', ''))\n",
    "    \n",
    "    if sentiment_metrics_to_plot:\n",
    "        print(f\"\\nðŸ“ˆ Visualizing metrics: {sentiment_metrics_to_plot}\")\n",
    "        analyzer.visualize_metrics(sentiment_comparison, sentiment_metrics_to_plot)\nelse:\n",
    "    print(\"No sentiment comparison data available yet. Run the evaluations first.\")\n",
    "\n",
    "print(\"\\nâœ… Analysis complete! Check your LangSmith dashboard for detailed evaluation results and comparisons.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ Regression Testing Setup\n",
    "\n",
    "Let's set up automated regression testing to ensure new changes don't break existing functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression testing framework\n",
    "\n",
    "import pytest\n",
    "from typing import Tuple\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "class RegressionTester:\n",
    "    \"\"\"Automated regression testing for LLM applications\"\"\"\n",
    "    \n",
    "    def __init__(self, client: Client):\n",
    "        self.client = client\n",
    "        self.baseline_thresholds = {\n",
    "            \"accuracy\": 0.7,\n",
    "            \"semantic_similarity\": 0.8,\n",
    "            \"helpfulness\": 0.75,\n",
    "            \"classification_accuracy\": 0.8\n",
    "        }\n",
    "    \n",
    "    def run_regression_test(self, \n",
    "                          application: Callable,\n",
    "                          dataset_name: str,\n",
    "                          evaluators: List[Callable],\n",
    "                          test_name: str,\n",
    "                          baseline_thresholds: Dict[str, float] = None) -> Tuple[bool, Dict]:\n",
    "        \"\"\"Run a regression test and check against thresholds\"\"\"\n",
    "        \n",
    "        thresholds = baseline_thresholds or self.baseline_thresholds\n",
    "        \n",
    "        try:\n",
    "            # Run evaluation\n",
    "            results = evaluate(\n",
    "                application,\n",
    "                data=dataset_name,\n",
    "                evaluators=evaluators,\n",
    "                experiment_prefix=f\"regression-{test_name}\",\n",
    "                description=f\"Regression test for {test_name}\",\n",
    "                max_concurrency=1\n",
    "            )\n",
    "            \n",
    "            # Extract metrics\n",
    "            experiment_results = analyzer.get_experiment_results(f\"regression-{test_name}\")\n",
    "            metrics = analyzer.calculate_summary_metrics(experiment_results)\n",
    "            \n",
    "            # Check against thresholds\n",
    "            passed_checks = {}\n",
    "            overall_pass = True\n",
    "            \n",
    "            for metric_name, threshold in thresholds.items():\n",
    "                metric_key = f\"{metric_name}_avg\"\n",
    "                if metric_key in metrics:\n",
    "                    value = metrics[metric_key]\n",
    "                    passed = value >= threshold\n",
    "                    passed_checks[metric_name] = {\n",
    "                        \"value\": value,\n",
    "                        \"threshold\": threshold,\n",
    "                        \"passed\": passed\n",
    "                    }\n",
    "                    if not passed:\n",
    "                        overall_pass = False\n",
    "            \n",
    "            return overall_pass, {\n",
    "                \"metrics\": metrics,\n",
    "                \"checks\": passed_checks,\n",
    "                \"test_name\": test_name\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return False, {\n",
    "                \"error\": str(e),\n",
    "                \"test_name\": test_name\n",
    "            }\n",
    "    \n",
    "    def create_regression_test_suite(self) -> str:\n",
    "        \"\"\"Create a pytest test suite for regression testing\"\"\"\n",
    "        \n",
    "        test_suite = '''\n",
    "import pytest\n",
    "import os\n",
    "from langsmith import Client\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# Import your applications and evaluators here\n",
    "# from your_module import apps, evaluators\n",
    "\n",
    "class TestRegressions:\n",
    "    \"\"\"Regression tests for LLM applications\"\"\"\n",
    "    \n",
    "    @pytest.fixture(autouse=True)\n",
    "    def setup(self):\n",
    "        self.client = Client()\n",
    "        # Initialize your apps and evaluators here\n",
    "    \n",
    "    def test_qa_baseline_regression(self):\n",
    "        \"\"\"Test Q&A baseline system doesn't regress\"\"\"\n",
    "        results = evaluate(\n",
    "            # apps.baseline_qa_system,\n",
    "            lambda x: {\"answer\": \"test\"},  # Placeholder\n",
    "            data=\"qa-evaluation-advanced\",\n",
    "            evaluators=[],  # Your evaluators here\n",
    "            experiment_prefix=\"regression-qa-baseline\"\n",
    "        )\n",
    "        \n",
    "        # Add assertions based on your thresholds\n",
    "        # assert results[\"accuracy\"] >= 0.7\n",
    "        pass\n",
    "    \n",
    "    def test_sentiment_classification_regression(self):\n",
    "        \"\"\"Test sentiment classifier doesn't regress\"\"\"\n",
    "        results = evaluate(\n",
    "            # apps.simple_sentiment_classifier,\n",
    "            lambda x: {\"label\": \"neutral\", \"confidence\": 0.5},  # Placeholder\n",
    "            data=\"sentiment-classification-advanced\",\n",
    "            evaluators=[],  # Your evaluators here\n",
    "            experiment_prefix=\"regression-sentiment-simple\"\n",
    "        )\n",
    "        \n",
    "        # Add assertions based on your thresholds\n",
    "        # assert results[\"classification_accuracy\"] >= 0.8\n",
    "        pass\n",
    "    \n",
    "    def test_response_time_regression(self):\n",
    "        \"\"\"Test that response times don't regress significantly\"\"\"\n",
    "        # Implement latency regression tests\n",
    "        pass\n",
    "    \n",
    "    def test_cost_regression(self):\n",
    "        \"\"\"Test that costs don't increase unexpectedly\"\"\"\n",
    "        # Implement cost regression tests\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pytest.main([__file__])\n",
    "'''\n",
    "        \n",
    "        # Write test suite to file\n",
    "        test_file_path = \"test_regressions.py\"\n",
    "        with open(test_file_path, \"w\") as f:\n",
    "            f.write(test_suite)\n",
    "        \n",
    "        return test_file_path\n",
    "    \n",
    "    def create_github_actions_workflow(self) -> str:\n",
    "        \"\"\"Create a GitHub Actions workflow for CI/CD integration\"\"\"\n",
    "        \n",
    "        workflow = '''\n",
    "name: LLM Application Regression Tests\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main, develop ]\n",
    "  pull_request:\n",
    "    branches: [ main ]\n",
    "  schedule:\n",
    "    # Run regression tests daily at 2 AM UTC\n",
    "    - cron: '0 2 * * *'\n",
    "\n",
    "jobs:\n",
    "  regression-tests:\n",
    "    runs-on: ubuntu-latest\n",
    "    \n",
    "    steps:\n",
    "    - uses: actions/checkout@v3\n",
    "    \n",
    "    - name: Set up Python\n",
    "      uses: actions/setup-python@v4\n",
    "      with:\n",
    "        python-version: '3.11'\n",
    "    \n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        python -m pip install --upgrade pip\n",
    "        pip install pytest langsmith langchain langchain-openai\n",
    "        # Add other dependencies as needed\n",
    "    \n",
    "    - name: Run regression tests\n",
    "      env:\n",
    "        LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}\n",
    "        LANGSMITH_PROJECT: ${{ secrets.LANGSMITH_PROJECT }}\n",
    "        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n",
    "      run: |\n",
    "        pytest test_regressions.py -v --tb=short\n",
    "    \n",
    "    - name: Upload test results\n",
    "      uses: actions/upload-artifact@v3\n",
    "      if: always()\n",
    "      with:\n",
    "        name: regression-test-results\n",
    "        path: test-results/\n",
    "'''\n",
    "        \n",
    "        # Create .github/workflows directory if it doesn't exist\n",
    "        os.makedirs(\".github/workflows\", exist_ok=True)\n",
    "        \n",
    "        workflow_file_path = \".github/workflows/regression-tests.yml\"\n",
    "        with open(workflow_file_path, \"w\") as f:\n",
    "            f.write(workflow)\n",
    "        \n",
    "        return workflow_file_path\n",
    "\n",
    "# Initialize regression tester\n",
    "regression_tester = RegressionTester(client)\n",
    "\n",
    "print(\"ðŸ”„ Setting up Regression Testing Framework...\")\n",
    "\n",
    "# Create test suite\n",
    "test_file = regression_tester.create_regression_test_suite()\n",
    "print(f\"âœ… Created regression test suite: {test_file}\")\n",
    "\n",
    "# Create GitHub Actions workflow\n",
    "workflow_file = regression_tester.create_github_actions_workflow()\n",
    "print(f\"âœ… Created GitHub Actions workflow: {workflow_file}\")\n",
    "\n",
    "# Run a sample regression test\n",
    "print(\"\\nðŸ§ª Running Sample Regression Test...\")\n",
    "\n",
    "try:\n",
    "    passed, results = regression_tester.run_regression_test(\n",
    "        application=apps.baseline_qa_system,\n",
    "        dataset_name=\"qa-evaluation-advanced\",\n",
    "        evaluators=[evaluators.accuracy_evaluator, evaluators.semantic_similarity_evaluator],\n",
    "        test_name=\"qa-baseline-sample\",\n",
    "        baseline_thresholds={\"accuracy\": 0.5, \"semantic_similarity\": 0.6}  # Lower thresholds for demo\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Regression Test Results:\")\n",
    "    print(f\"Overall Status: {'âœ… PASSED' if passed else 'âŒ FAILED'}\")\n",
    "    \n",
    "    if \"checks\" in results:\n",
    "        print(\"\\nDetailed Results:\")\n",
    "        for metric, check in results[\"checks\"].items():\n",
    "            status = \"âœ…\" if check[\"passed\"] else \"âŒ\"\n",
    "            print(f\"  {status} {metric}: {check['value']:.3f} (threshold: {check['threshold']})\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Regression test failed: {e}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Regression Testing Framework setup complete!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Customize the test_regressions.py file with your specific applications and thresholds\")\n",
    "print(\"2. Set up the required secrets in your GitHub repository\")\n",
    "print(\"3. Commit the workflow file to enable automatic regression testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¡ Advanced Evaluation Patterns\n",
    "\n",
    "Let's explore some advanced evaluation patterns and best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced evaluation patterns\n",
    "\n",
    "class AdvancedEvaluationPatterns:\n",
    "    \"\"\"Advanced patterns for LLM evaluation\"\"\"\n",
    "    \n",
    "    def __init__(self, client: Client):\n",
    "        self.client = client\n",
    "        self.llm = ChatOpenAI(temperature=0.0, model=\"gpt-4\")\n",
    "    \n",
    "    def multi_dimensional_evaluator(self, run: Run, example: Example) -> Dict[str, Any]:\n",
    "        \"\"\"Multi-dimensional evaluation with weighted scoring\"\"\"\n",
    "        question = example.inputs.get(\"question\", \"\")\n",
    "        answer = run.outputs.get(\"answer\", \"\")\n",
    "        \n",
    "        # Define evaluation dimensions with weights\n",
    "        dimensions = {\n",
    "            \"accuracy\": 0.3,\n",
    "            \"completeness\": 0.25,\n",
    "            \"clarity\": 0.2,\n",
    "            \"relevance\": 0.15,\n",
    "            \"conciseness\": 0.1\n",
    "        }\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are an expert evaluator. Assess the answer across multiple dimensions.\n",
    "            \n",
    "Rate each dimension on a scale from 0.0 to 1.0:\n",
    "- Accuracy: How factually correct is the answer?\n",
    "- Completeness: Does the answer fully address the question?\n",
    "- Clarity: How clear and understandable is the answer?\n",
    "- Relevance: How relevant is the answer to the question?\n",
    "- Conciseness: Is the answer appropriately concise?\n",
    "\n",
    "Respond in JSON format:\n",
    "{\n",
    "    \"accuracy\": 0.0-1.0,\n",
    "    \"completeness\": 0.0-1.0,\n",
    "    \"clarity\": 0.0-1.0,\n",
    "    \"relevance\": 0.0-1.0,\n",
    "    \"conciseness\": 0.0-1.0,\n",
    "    \"reasoning\": \"detailed explanation\"\n",
    "}\"\"\"),\n",
    "            (\"human\", \"\"\"Question: {question}\n",
    "\n",
    "Answer: {answer}\n",
    "\n",
    "Evaluate across all dimensions:\"\"\")\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke(prompt.format_messages(question=question, answer=answer))\n",
    "            result = json.loads(response.content)\n",
    "            \n",
    "            # Calculate weighted score\n",
    "            weighted_score = sum(\n",
    "                result.get(dim, 0) * weight \n",
    "                for dim, weight in dimensions.items()\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"key\": \"multi_dimensional\",\n",
    "                \"score\": weighted_score,\n",
    "                \"comment\": f\"Weighted score: {weighted_score:.3f}. Reasoning: {result.get('reasoning', 'No reasoning provided')}\"\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"key\": \"multi_dimensional\",\n",
    "                \"score\": 0.5,\n",
    "                \"comment\": f\"Evaluation failed: {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    def consistency_evaluator(self, run: Run, example: Example) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate consistency by asking the same question multiple times\"\"\"\n",
    "        question = example.inputs.get(\"question\", \"\")\n",
    "        original_answer = run.outputs.get(\"answer\", \"\")\n",
    "        \n",
    "        # This would typically be run with multiple samples\n",
    "        # For demo purposes, we'll simulate consistency checking\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"Assess how consistent this answer would likely be if the question were asked multiple times.\n",
    "            \n",
    "Consider:\n",
    "- Is the answer based on factual information?\n",
    "- Does it contain subjective elements that might vary?\n",
    "- How deterministic is the reasoning?\n",
    "\n",
    "Rate consistency from 0.0 to 1.0 where:\n",
    "- 1.0: Highly consistent, factual answer\n",
    "- 0.5: Moderately consistent, some variability expected\n",
    "- 0.0: Highly inconsistent, subjective or random\n",
    "\n",
    "Respond with just a number between 0.0 and 1.0\"\"\"),\n",
    "            (\"human\", \"\"\"Question: {question}\n",
    "Answer: {answer}\"\"\")\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke(prompt.format_messages(question=question, answer=original_answer))\n",
    "            score = float(response.content.strip())\n",
    "            score = max(0.0, min(1.0, score))  # Clamp to valid range\n",
    "            \n",
    "            return {\n",
    "                \"key\": \"consistency\",\n",
    "                \"score\": score,\n",
    "                \"comment\": f\"Estimated consistency score: {score:.3f}\"\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"key\": \"consistency\",\n",
    "                \"score\": 0.5,\n",
    "                \"comment\": f\"Consistency evaluation failed: {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    def robustness_test_generator(self, original_examples: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Generate adversarial examples for robustness testing\"\"\"\n",
    "        adversarial_examples = []\n",
    "        \n",
    "        for example in original_examples:\n",
    "            question = example[\"inputs\"][\"question\"]\n",
    "            \n",
    "            # Generate variations\n",
    "            variations = [\n",
    "                # Paraphrasing\n",
    "                f\"Can you tell me: {question.lower()}\",\n",
    "                f\"I'd like to know {question.lower()}\",\n",
    "                # Adding context\n",
    "                f\"Context: This is for educational purposes. Question: {question}\",\n",
    "                # Minor typos (simulate real user input)\n",
    "                question.replace(\"the\", \"teh\").replace(\"you\", \"yu\"),\n",
    "            ]\n",
    "            \n",
    "            for variation in variations:\n",
    "                if variation != question:  # Avoid duplicates\n",
    "                    adversarial_examples.append({\n",
    "                        \"inputs\": {\"question\": variation},\n",
    "                        \"outputs\": example[\"outputs\"],\n",
    "                        \"metadata\": {\n",
    "                            \"original_question\": question,\n",
    "                            \"variation_type\": \"adversarial\"\n",
    "                        }\n",
    "                    })\n",
    "        \n",
    "        return adversarial_examples\n",
    "    \n",
    "    def create_robustness_dataset(self, base_dataset_name: str, robustness_dataset_name: str):\n",
    "        \"\"\"Create a robustness testing dataset\"\"\"\n",
    "        # Get original examples\n",
    "        original_examples = []\n",
    "        # This would typically fetch from the original dataset\n",
    "        # For demo, we'll use our qa_examples\n",
    "        original_examples = qa_examples\n",
    "        \n",
    "        # Generate adversarial examples\n",
    "        adversarial_examples = self.robustness_test_generator(original_examples)\n",
    "        \n",
    "        # Create new dataset\n",
    "        try:\n",
    "            dataset = self.client.create_dataset(\n",
    "                dataset_name=robustness_dataset_name,\n",
    "                description=\"Robustness testing dataset with adversarial examples\"\n",
    "            )\n",
    "            \n",
    "            self.client.create_examples(\n",
    "                inputs=[ex[\"inputs\"] for ex in adversarial_examples],\n",
    "                outputs=[ex[\"outputs\"] for ex in adversarial_examples],\n",
    "                dataset_id=dataset.id\n",
    "            )\n",
    "            \n",
    "            return dataset.id\n",
    "            \n",
    "        except Exception as e:\n",
    "            if \"already exists\" in str(e):\n",
    "                datasets = list(self.client.list_datasets(dataset_name=robustness_dataset_name))\n",
    "                return datasets[0].id if datasets else None\n",
    "            raise e\n",
    "\n",
    "# Initialize advanced evaluation patterns\n",
    "advanced_eval = AdvancedEvaluationPatterns(client)\n",
    "\n",
    "print(\"ðŸš€ Advanced Evaluation Patterns Demo\\n\")\n",
    "\n",
    "# Create robustness dataset\n",
    "print(\"ðŸ“Š Creating Robustness Testing Dataset...\")\n",
    "robustness_dataset_id = advanced_eval.create_robustness_dataset(\n",
    "    \"qa-evaluation-advanced\", \n",
    "    \"qa-robustness-testing\"\n",
    ")\n",
    "print(f\"âœ… Robustness dataset created: {robustness_dataset_id}\")\n",
    "\n",
    "# Run advanced evaluation\n",
    "print(\"\\nðŸ§ª Running Advanced Multi-Dimensional Evaluation...\")\n",
    "try:\n",
    "    advanced_results = evaluate(\n",
    "        apps.enhanced_qa_system,\n",
    "        data=\"qa-evaluation-advanced\",\n",
    "        evaluators=[\n",
    "            advanced_eval.multi_dimensional_evaluator,\n",
    "            advanced_eval.consistency_evaluator\n",
    "        ],\n",
    "        experiment_prefix=\"advanced-eval\",\n",
    "        description=\"Advanced multi-dimensional evaluation\",\n",
    "        max_concurrency=1\n",
    "    )\n",
    "    print(\"âœ… Advanced evaluation completed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Advanced evaluation failed: {e}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Advanced Evaluation Patterns demonstration complete!\")\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"1. Multi-dimensional evaluation provides richer insights\")\n",
    "print(\"2. Consistency testing helps identify reliability issues\")\n",
    "print(\"3. Robustness testing with adversarial examples reveals edge cases\")\n",
    "print(\"4. Weighted scoring allows prioritizing different aspects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¡ Key Takeaways and Best Practices\n",
    "\n",
    "### âœ… What You've Mastered\n",
    "\n",
    "1. **Advanced Dataset Management**:\n",
    "   - Multi-purpose dataset creation (Q&A, classification, summarization)\n",
    "   - Structured data organization with metadata\n",
    "   - Adversarial example generation for robustness testing\n",
    "\n",
    "2. **Custom Evaluator Development**:\n",
    "   - Exact match and semantic similarity evaluators\n",
    "   - LLM-as-Judge patterns for complex assessments\n",
    "   - Multi-dimensional weighted scoring systems\n",
    "   - Confidence-aware evaluation metrics\n",
    "\n",
    "3. **Comprehensive Evaluation Workflows**:\n",
    "   - A/B testing and comparison frameworks\n",
    "   - Regression testing and CI/CD integration\n",
    "   - Automated threshold-based quality gates\n",
    "   - Results visualization and analysis\n",
    "\n",
    "4. **Production-Ready Patterns**:\n",
    "   - GitHub Actions integration for continuous testing\n",
    "   - Consistency and robustness evaluation\n",
    "   - Automated reporting and alerting\n",
    "\n",
    "### ðŸŽ¯ Best Practices for Production\n",
    "\n",
    "1. **Dataset Quality**:\n",
    "   - Start small but representative (20-50 examples)\n",
    "   - Continuously expand with production examples\n",
    "   - Include edge cases and failure scenarios\n",
    "   - Version control your datasets\n",
    "\n",
    "2. **Evaluator Design**:\n",
    "   - Use multiple complementary evaluators\n",
    "   - Combine automated and human evaluation\n",
    "   - Design domain-specific metrics\n",
    "   - Validate evaluator quality with ground truth\n",
    "\n",
    "3. **Testing Strategy**:\n",
    "   - Run evaluations on every significant change\n",
    "   - Set up automated regression testing\n",
    "   - Monitor evaluation trends over time\n",
    "   - Use staged rollouts with evaluation gates\n",
    "\n",
    "4. **Continuous Improvement**:\n",
    "   - Analyze failed cases to improve systems\n",
    "   - Update datasets based on production feedback\n",
    "   - Refine evaluators based on business needs\n",
    "   - Share evaluation insights across teams\n",
    "\n",
    "### ðŸ”§ Advanced Tips\n",
    "\n",
    "- **Sampling**: Use stratified sampling for balanced evaluation\n",
    "- **Caching**: Cache expensive evaluations to speed up iterations\n",
    "- **Parallelization**: Run evaluations in parallel for faster results\n",
    "- **Versioning**: Track evaluator versions alongside model versions\n",
    "- **Documentation**: Document evaluation metrics and their business meaning\n",
    "\n",
    "## ðŸš€ What's Next?\n",
    "\n",
    "You're now equipped to build robust evaluation pipelines! Continue to:\n",
    "\n",
    "- **LSM-005: Prompt Engineering** - Master collaborative prompt development and version control\n",
    "- **LSM-006: Production Monitoring** - Set up enterprise-grade monitoring and alerting\n",
    "- **LSM-007: Advanced Patterns** - Explore complex use cases and integration patterns\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to master collaborative prompt development?** Continue to **LSM-005: Prompt Engineering** to learn advanced prompt management and collaboration techniques! ðŸŽ¨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}